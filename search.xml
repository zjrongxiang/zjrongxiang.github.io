<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux系统中的su命令]]></title>
    <url>%2F2021%2F06%2F14%2F2021-06-14-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84su%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景Linux操作系统是多用户多任务操作系统。多用户多任务就是可以在操作系统上建立多个用户，多个用户可以在同一时间内登录并执行各自不同的任务，互不影响。不同用户具有不同的权限，每个用户是在权限允许的范围内完成不同的任务，Linux正是通过这种权限的划分与管理，实现了多用户多任务的运行机制。 在日常运维中，su命令是最简单的用户切换命令，通过该命令可以实现任何用户身份的切换（包括从普通用户切换为 root 用户、从 root 用户切换为普通用户以及普通用户之间的切换）。普通用户之间切换以及普通用户切换至 root 用户，需要目标用户密钥，只有正确输入密钥，才能实现切换；从 root 用户切换至其他用户，无需知晓对方密钥，直接可切换成功。 第一部分 su命令su 命令的基本格式如下： 1root@VM-0-5-ubuntu:~# su [选项] 用户名 参数选项： -：当前用户不仅切换为指定用户的身份，同时所用的工作环境也切换为此用户的环境（包括 PATH 变量、MAIL 变量等）。使用 - 选项可省略用户名，默认会切换为 root 用户。 -l：同-的使用类似，也就是在切换用户身份的同时，完整切换工作环境，但后面需要添加欲切换的使用者账号。 -p：表示切换为指定用户的身份，但不改变当前的工作环境（不使用切换用户的配置文件）。 -m：和 -p 一样； -c 命令：仅切换用户执行一次命令，执行后自动切换回来，该选项后通常会带有要执行的命令。 第二部分 su 和 su -区别在实际运维使用中，经常踩的坑就是 su 和 su -的区别了。运维人员通常认为两者是相同，或者不知道 su -。 事实上，有-和没有 -是完全不同的，-选项表示在切换用户身份的同时，连当前使用的环境变量也切换成指定用户的。环境变量是用来定义操作系统环境的，因此如果系统环境没有随用户身份切换，很多命令无法正确执行。 初学者可以这样理解它们之间的区别，即有 - 选项，切换用户身份更彻底；反之，只切换了一部分。在不使用 su -的情况下，虽然用户身份成功切换，但环境变量依旧用的是原用户的，切换并不完整。 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RestFul接口规范总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-06-24-RestFul%E6%8E%A5%E5%8F%A3%E8%A7%84%E8%8C%83%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景一、协议API与用户的通信协议，总是使用HTTPs协议。 二、域名应该尽量将API部署在专用域名之下。 12&gt; https://api.example.com&gt; 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。 12&gt; https://example.org/api/&gt; 三、版本（Versioning）应该将API的版本号放入URL。 12&gt; https://api.example.com/v1/&gt; 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观。Github采用这种做法。 四、路径（Endpoint）路径又称”终点”（endpoint），表示API的具体网址。 在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的”集合”（collection），所以API中的名词也应该使用复数。 举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。 https://api.example.com/v1/zoos https://api.example.com/v1/animals https://api.example.com/v1/employees 五、HTTP动词对于资源的具体操作类型，由HTTP动词表示。 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。 GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。 HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。 GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息（Filtering）如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。 下面是一些常见的参数。 ?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。 七、状态码（Status Codes）服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。 200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。 202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务） 204 NO CONTENT - [DELETE]：用户删除数据成功。 400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。 状态码的完全列表参见这里。 八、错误处理（Error handling）如果状态码是4xx，就应该向用户返回出错信息。一般来说，返回的信息中将error作为键名，出错信息作为键值即可。 1234&gt; &#123;&gt; error: "Invalid API key"&gt; &#125;&gt; 九、返回结果针对不同操作，服务器向用户返回的结果应该符合以下规范。 GET /collection：返回资源对象的列表（数组） GET /collection/resource：返回单个资源对象 POST /collection：返回新生成的资源对象 PUT /collection/resource：返回完整的资源对象 PATCH /collection/resource：返回完整的资源对象 DELETE /collection/resource：返回一个空文档 十、Hypermedia APIRESTful API最好做到Hypermedia，即返回结果中提供链接，连向其他API方法，使得用户不查文档，也知道下一步应该做什么。 比如，当用户向api.example.com的根目录发出请求，会得到这样一个文档。 1234567&gt; &#123;"link": &#123;&gt; "rel": "collection https://www.example.com/zoos",&gt; "href": "https://api.example.com/zoos",&gt; "title": "List of zoos",&gt; "type": "application/vnd.yourformat+json"&gt; &#125;&#125;&gt; 上面代码表示，文档中有一个link属性，用户读取这个属性就知道下一步该调用什么API了。rel表示这个API与当前网址的关系（collection关系，并给出该collection的网址），href表示API的路径，title表示API的标题，type表示返回类型。 Hypermedia API的设计被称为HATEOAS。Github的API就是这种设计，访问api.github.com会得到一个所有可用API的网址列表。 123456&gt; &#123;&gt; "current_user_url": "https://api.github.com/user",&gt; "authorizations_url": "https://api.github.com/authorizations",&gt; // ...&gt; &#125;&gt; 从上面可以看到，如果想获取当前用户的信息，应该去访问api.github.com/user，然后就得到了下面结果。 12345&gt; &#123;&gt; "message": "Requires authentication",&gt; "documentation_url": "https://developer.github.com/v3"&gt; &#125;&gt; 上面代码表示，服务器给出了提示信息，以及文档的网址。 十一、其他（1）API的身份认证应该使用OAuth 2.0框架。 （2）服务器返回的数据格式，应该尽量使用JSON，避免使用XML。 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink Table API & SQL编程总结]]></title>
    <url>%2F2021%2F06%2F14%2F2021-06-23-Flink%20Table%20API%20%26%20SQL%E7%BC%96%E7%A8%8B%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 su命令 第二部分 su 和 su -区别 参考文献及资料 背景Apache Flink提供了两种顶层的关系型API，分别为Table API和SQL，Flink通过Table API&amp;SQL实现了批流统一。其中Table API是用于Scala和Java的语言集成查询API，它允许以非常直观的方式组合关系运算符（例如select，where和join）的查询。Flink SQL基于Apache Calcite 实现了标准的SQL，用户可以使用标准的SQL处理数据集。Table API和SQL与Flink的DataStream和DataSet API紧密集成在一起，用户可以实现相互转化，比如可以将DataStream或者DataSet注册为table进行操作数据。值得注意的是，Table API and SQL目前尚未完全完善，还在积极的开发中，所以并不是所有的算子操作都可以通过其实现。 依赖从Flink1.9开始，Flink为Table &amp; SQL API提供了两种planner,分别为Blink planner和old planner，其中old planner是在Flink1.9之前的版本使用。主要区别如下： 尖叫提示：对于生产环境，目前推荐使用old planner. flink-table-common: 通用模块，包含 Flink Planner 和 Blink Planner 一些共用的代码 flink-table-api-java: java语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) flink-table-api-scala: scala语言的Table &amp; SQL API，仅针对table(处于早期的开发阶段，不推荐使用) flink-table-api-java-bridge: java语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) flink-table-api-scala-bridge: scala语言的Table &amp; SQL API，支持DataStream/DataSet API(推荐使用) flink-table-planner:planner 和runtime. planner为Flink1,9之前的old planner(推荐使用) flink-table-planner-blink: 新的Blink planner. flink-table-runtime-blink: 新的Blink runtime. flink-table-uber: 将上述的API模块及old planner打成一个jar包，形如flink-table-*.jar，位与/lib目录下 flink-table-uber-blink:将上述的API模块及Blink 模块打成一个jar包，形如fflink-table-blink-*.jar，位与/lib目录下 Blink planner &amp; old plannerBlink planner和old planner有许多不同的特点，具体列举如下： Blink planner将批处理作业看做是流处理作业的特例。所以，不支持Table 与DataSet之间的转换，批处理的作业也不会被转成DataSet程序，而是被转为DataStream程序。 Blink planner不支持 BatchTableSource，使用的是有界的StreamTableSource。 Blink planner仅支持新的 Catalog，不支持ExternalCatalog (已过时)。 对于FilterableTableSource的实现，两种Planner是不同的。old planner会谓词下推到PlannerExpression(未来会被移除)，而Blink planner 会谓词下推到 Expression(表示一个产生计算结果的逻辑树)。 仅仅Blink planner支持key-value形式的配置，即通过Configuration进行参数设置。 关于PlannerConfig的实现，两种planner有所不同。 Blink planner 会将多个sink优化成一个DAG(仅支持TableEnvironment，StreamTableEnvironment不支持)，old planner总是将每一个sink优化成一个新的DAG，每一个DAG都是相互独立的。 old planner不支持catalog统计，Blink planner支持catalog统计。 第一部分 Flink Table &amp; SQL程序的pom依赖根据使用的语言不同，可以选择下面的依赖，包括scala版和java版，如下： 1234567891011121314&lt;!-- java版 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-java-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- scala版 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-api-scala-bridge_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 除此之外，如果需要在本地的IDE中运行Table API &amp; SQL的程序，则需要添加下面的pom依赖： 1234567891011121314&lt;!-- Flink 1.9之前的old planner --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- 新的Blink planner --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.11&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 另外，如果需要实现自定义的格式(比如和kafka交互)或者用户自定义函数，需要添加如下依赖： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-common&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Table API &amp; SQL的编程模板所有的Table API&amp;SQL的程序(无论是批处理还是流处理)都有着相同的形式，下面将给出通用的编程结构形式： 1234567891011121314// 创建一个TableEnvironment对象，指定planner、处理模式(batch、streaming)TableEnvironment tableEnv = ...; // 创建一个表tableEnv.connect(...).createTemporaryTable("table1");// 注册一个外部的表tableEnv.connect(...).createTemporaryTable("outputTable");// 通过Table API的查询创建一个Table 对象Table tapiResult = tableEnv.from("table1").select(...);// 通过SQL查询的查询创建一个Table 对象Table sqlResult = tableEnv.sqlQuery("SELECT ... FROM table1 ... ");// 将结果写入TableSinktapiResult.insertInto("outputTable");// 执行tableEnv.execute("java_job"); 注意：Table API &amp; SQL的查询可以相互集成，另外还可以在DataStream或者DataSet中使用Table API &amp; SQL的API，实现DataStreams、 DataSet与Table之间的相互转换。 创建TableEnvironmentTableEnvironment是Table API &amp; SQL程序的一个入口，主要包括如下的功能： 在内部的catalog中注册Table 注册catalog 加载可插拔模块 执行SQL查询 注册用户定义函数 DataStream 、DataSet与Table之间的相互转换 持有对ExecutionEnvironment 、StreamExecutionEnvironment的引用 一个Table必定属于一个具体的TableEnvironment，不可以将不同TableEnvironment的表放在一起使用(比如join，union等操作)。 TableEnvironment是通过调用 BatchTableEnvironment.create() 或者StreamTableEnvironment.create()的静态方法进行创建的。另外，默认两个planner的jar包都存在与classpath下，所有需要明确指定使用的planner。 1234567891011121314151617181920212223242526272829303132333435363738394041// **********************// FLINK 流处理查询// **********************import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);//或者TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);// ******************// FLINK 批处理查询// ******************import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.table.api.java.BatchTableEnvironment;ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);// **********************// BLINK 流处理查询// **********************import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.java.StreamTableEnvironment;StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);// 或者 TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);// ******************// BLINK 批处理查询// ******************import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings); 在catalog中创建表临时表与永久表表可以分为临时表和永久表两种，其中永久表需要一个catalog(比如Hive的Metastore)俩维护表的元数据信息，一旦永久表被创建，只要连接到该catalog就可以访问该表，只有显示删除永久表，该表才可以被删除。临时表的生命周期是Flink Session，这些表不能够被其他的Flink Session访问，这些表不属于任何的catalog或者数据库，如果与临时表相对应的数据库被删除了，该临时表也不会被删除。 创建表虚表(Virtual Tables)一个Table对象相当于SQL中的视图(虚表)，它封装了一个逻辑执行计划，可以通过一个catalog创建，具体如下： 123456// 获取一个TableEnvironmentTableEnvironment tableEnv = ...; // table对象，查询的结果集Table projTable = tableEnv.from("X").select(...);// 注册一个表，名称为 "projectedTable"tableEnv.createTemporaryView("projectedTable", projTable); 外部数据源表(Connector Tables)可以把外部的数据源注册成表，比如可以读取MySQL数据库数据、Kafka数据等 123456tableEnvironment .connect(...) .withFormat(...) .withSchema(...) .inAppendMode() .createTemporaryTable("MyTable") 扩展创建表的标识属性表的注册总是包含三部分标识属性：catalog、数据库、表名。用户可以在内部设置一个catalog和一个数据库作为当前的catalog和数据库，所以对于catalog和数据库这两个标识属性是可选的，即如果不指定，默认使用的是“current catalog”和 “current database”。 1234567891011121314151617181920212223TableEnvironment tEnv = ...;tEnv.useCatalog("custom_catalog");//设置catalogtEnv.useDatabase("custom_database");//设置数据库Table table = ...;// 注册一个名为exampleView的视图，catalog名为custom_catalog// 数据库的名为custom_databasetableEnv.createTemporaryView("exampleView", table);// 注册一个名为exampleView的视图，catalog的名为custom_catalog// 数据库的名为other_databasetableEnv.createTemporaryView("other_database.exampleView", table); // 注册一个名为'View'的视图，catalog的名称为custom_catalog// 数据库的名为custom_database，'View'是保留关键字，需要使用``(反引号)tableEnv.createTemporaryView("`View`", table);// 注册一个名为example.View的视图，catalog的名为custom_catalog，// 数据库名为custom_databasetableEnv.createTemporaryView("`example.View`", table);// 注册一个名为'exampleView'的视图， catalog的名为'other_catalog'// 数据库名为other_database' tableEnv.createTemporaryView("other_catalog.other_database.exampleView", table); 查询表Table APITable API是一个集成Scala与Java语言的查询API，与SQL相比，它的查询不是一个标准的SQL语句，而是由一步一步的操作组成的。如下展示了一个使用Table API实现一个简单的聚合查询。 1234567891011// 获取TableEnvironmentTableEnvironment tableEnv = ...;//注册Orders表// 查询注册的表Table orders = tableEnv.from("Orders");// 计算操作Table revenue = orders .filter("cCountry === 'FRANCE'") .groupBy("cID, cName") .select("cID, cName, revenue.sum AS revSum"); SQLFlink SQL依赖于Apache Calcite，其实现了标准的SQL语法，如下案例： 12345678910111213141516171819202122// 获取TableEnvironmentTableEnvironment tableEnv = ...;//注册Orders表// 计算逻辑同上面的Table APITable revenue = tableEnv.sqlQuery( "SELECT cID, cName, SUM(revenue) AS revSum " + "FROM Orders " + "WHERE cCountry = 'FRANCE' " + "GROUP BY cID, cName" );// 注册"RevenueFrance"外部输出表// 计算结果插入"RevenueFrance"表tableEnv.sqlUpdate( "INSERT INTO RevenueFrance " + "SELECT cID, cName, SUM(revenue) AS revSum " + "FROM Orders " + "WHERE cCountry = 'FRANCE' " + "GROUP BY cID, cName" ); 输出表一个表通过将其写入到TableSink，然后进行输出。TableSink是一个通用的支持多种文件格式(CSV、Parquet, Avro)和多种外部存储系统(JDBC, Apache HBase, Apache Cassandra, Elasticsearch)以及多种消息对列(Apache Kafka, RabbitMQ)的接口。 批处理的表只能被写入到 BatchTableSink,流处理的表需要指明AppendStreamTableSink、RetractStreamTableSink或者 UpsertStreamTableSink 123456789101112131415161718// 获取TableEnvironmentTableEnvironment tableEnv = ...;// 创建输出表final Schema schema = new Schema() .field("a", DataTypes.INT()) .field("b", DataTypes.STRING()) .field("c", DataTypes.LONG());tableEnv.connect(new FileSystem("/path/to/file")) .withFormat(new Csv().fieldDelimiter('|').deriveSchema()) .withSchema(schema) .createTemporaryTable("CsvSinkTable");// 计算结果表Table result = ...// 输出结果表到注册的TableSinkresult.insertInto("CsvSinkTable"); Table API &amp; SQL底层的转换与执行上文提到了Flink提供了两种planner，分别为old planner和Blink planner，对于不同的planner而言，Table API &amp; SQL底层的执行与转换是有所不同的。 Old planner根据是流处理作业还是批处理作业，Table API &amp;SQL会被转换成DataStream或者DataSet程序。一个查询在内部表示为一个逻辑查询计划，会被转换为两个阶段: 1.逻辑查询计划优化 2.转换成DataStream或者DataSet程序 上面的两个阶段只有下面的操作被执行时才会被执行： 当一个表被输出到TableSink时，比如调用了Table.insertInto()方法 当执行更新查询时，比如调用TableEnvironment.sqlUpdate()方法 当一个表被转换为DataStream或者DataSet时 一旦执行上述两个阶段，Table API &amp; SQL的操作会被看做是普通的DataStream或者DataSet程序，所以当StreamExecutionEnvironment.execute()或者ExecutionEnvironment.execute() 被调用时，会执行转换后的程序。 Blink planner无论是批处理作业还是流处理作业，如果使用的是Blink planner，底层都会被转换为DataStream程序。在一个查询在内部表示为一个逻辑查询计划，会被转换成两个阶段： 1.逻辑查询计划优化 2.转换成DataStream程序 对于TableEnvironment and StreamTableEnvironment而言，一个查询的转换是不同的 首先对于TableEnvironment，当TableEnvironment.execute()方法执行时，Table API &amp; SQL的查询才会被转换，因为TableEnvironment会将多个sink优化为一个DAG。 对于StreamTableEnvironment，转换发生的时间与old planner相同。 与DataStream &amp; DataSet API集成对于Old planner与Blink planner而言，只要是流处理的操作，都可以与DataStream API集成，仅仅只有Old planner才可以与DataSet API集成，由于Blink planner的批处理作业会被转换成DataStream程序，所以不能够与DataSet API集成。值得注意的是，下面提到的table与DataSet之间的转换仅适用于Old planner。 Table API &amp; SQL的查询很容易与DataStream或者DataSet程序集成，并可以将Table API &amp; SQL的查询嵌入DataStream或者DataSet程序中。DataStream或者DataSet可以转换成表，反之，表也可以被转换成DataStream或者DataSet。 从DataStream或者DataSet中注册临时表(视图)尖叫提示：只能将DataStream或者DataSet转换为临时表(视图) 下面演示DataStream的转换，对于DataSet的转换类似。 1234567// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...// 将DataStream注册为一个名为myTable的视图，其中字段分别为"f0", "f1"tableEnv.createTemporaryView("myTable", stream);// 将DataStream注册为一个名为myTable2的视图,其中字段分别为"myLong", "myString"tableEnv.createTemporaryView("myTable2", stream, "myLong, myString"); 将DataStream或者DataSet转化为Table对象可以直接将DataStream或者DataSet转换为Table对象，之后可以使用Table API进行查询操作。下面演示DataStream的转换，对于DataSet的转换类似。 1234567// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...// 将DataStream转换为Table对象，默认的字段为"f0", "f1"Table table1 = tableEnv.fromDataStream(stream);// 将DataStream转换为Table对象，默认的字段为"myLong", "myString"Table table2 = tableEnv.fromDataStream(stream, "myLong, myString"); 将表转换为DataStream或者DataSet当将Table转为DataStream或者DataSet时，需要指定DataStream或者DataSet的数据类型。通常最方便的数据类型是row类型，Flink提供了很多的数据类型供用户选择，具体包括Row、POJO、样例类、Tuple和原子类型。 将表转换为DataStream一个流处理查询的结果是动态变化的，所以将表转为DataStream时需要指定一个更新模式，共有两种模式：Append Mode和Retract Mode。 Append Mode 如果动态表仅只有Insert操作，即之前输出的结果不会被更新，则使用该模式。如果更新或删除操作使用追加模式会失败报错 Retract Mode 始终可以使用此模式。返回值是boolean类型。它用true或false来标记数据的插入和撤回，返回true代表数据插入，false代表数据的撤回。 1234567891011121314151617// 获取StreamTableEnvironment. StreamTableEnvironment tableEnv = ...; // 包含两个字段的表(String name, Integer age)Table table = ...// 将表转为DataStream，使用Append Mode追加模式，数据类型为RowDataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);// 将表转为DataStream，使用Append Mode追加模式，数据类型为定义好的TypeInformationTupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT());DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toAppendStream(table, tupleType);// 将表转为DataStream，使用的模式为Retract Mode撤回模式，类型为Row// 对于转换后的DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;，X表示流的数据类型，// boolean值表示数据改变的类型，其中INSERT返回true，DELETE返回的是falseDataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = tableEnv.toRetractStream(table, Row.class); 将表转换为DataSet123456789101112// 获取BatchTableEnvironmentBatchTableEnvironment tableEnv = BatchTableEnvironment.create(env);// 包含两个字段的表(String name, Integer age)Table table = ...// 将表转为DataSet数据类型为RowDataSet&lt;Row&gt; dsRow = tableEnv.toDataSet(table, Row.class);// 将表转为DataSet，通过TypeInformation定义Tuple2&lt;String, Integer&gt;数据类型TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;( Types.STRING(), Types.INT());DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = tableEnv.toDataSet(table, tupleType); 表的Schema与数据类型之间的映射表的Schema与数据类型之间的映射有两种方式：分别是基于字段下标位置的映射和基于字段名称的映射。 基于字段下标位置的映射该方式是按照字段的顺序进行一一映射，使用方式如下： 123456789// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...// 将DataStream转为表，默认的字段名为"f0"和"f1"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，选取tuple的第一个元素，指定一个名为"myLong"的字段名Table table = tableEnv.fromDataStream(stream, "myLong");// 将DataStream转为表，为tuple的第一个元素指定名为"myLong"，为第二个元素指定myInt的字段名Table table = tableEnv.fromDataStream(stream, "myLong, myInt"); 基于字段名称的映射基于字段名称的映射方式支持任意的数据类型包括POJO类型，可以很灵活地定义表Schema映射，所有的字段被映射成一个具体的字段名称，同时也可以使用”as”为字段起一个别名。其中Tuple元素的第一个元素为f0,第二个元素为f1，以此类推。 1234567891011// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...// 将DataStream转为表，默认的字段名为"f0"和"f1"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，选择tuple的第二个元素，指定一个名为"f1"的字段名Table table = tableEnv.fromDataStream(stream, "f1");// 将DataStream转为表，交换字段的顺序Table table = tableEnv.fromDataStream(stream, "f1, f0");// 将DataStream转为表，交换字段的顺序，并为f1起别名为"myInt"，为f0起别名为"myLongTable table = tableEnv.fromDataStream(stream, "f1 as myInt, f0 as myLong"); 原子类型Flink将Integer, Double, String或者普通的类型称之为原子类型，一个数据类型为原子类型的DataStream或者DataSet可以被转成单个字段属性的表，这个字段的类型与DataStream或者DataSet的数据类型一致，这个字段的名称可以进行指定。 12345678//获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; // 数据类型为原子类型LongDataStream&lt;Long&gt; stream = ...// 将DataStream转为表，默认的字段名为"f0"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，指定字段名为myLong"Table table = tableEnv.fromDataStream(stream, "myLong"); Tuple类型Tuple类型的DataStream或者DataSet都可以转为表，可以重新设定表的字段名(即根据tuple元素的位置进行一一映射，转为表之后，每个元素都有一个别名)，如果不为字段指定名称，则使用默认的名称(java语言默认的是f0,f1,scala默认的是_1),用户也可以重新排列字段的顺序，并为每个字段起一个别名。 1234567891011121314// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; //Tuple2&lt;Long, String&gt;类型的DataStreamDataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...// 将DataStream转为表，默认的字段名为 "f0", "f1"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，指定字段名为 "myLong", "myString"(按照Tuple元素的顺序位置)Table table = tableEnv.fromDataStream(stream, "myLong, myString");// 将DataStream转为表，指定字段名为 "f0", "f1"，并且交换顺序Table table = tableEnv.fromDataStream(stream, "f1, f0");// 将DataStream转为表，只选择Tuple的第二个元素，指定字段名为"f1"Table table = tableEnv.fromDataStream(stream, "f1");// 将DataStream转为表，为Tuple的第二个元素指定别名为myString，为第一个元素指定字段名为myLongTable table = tableEnv.fromDataStream(stream, "f1 as 'myString', f0 as 'myLong'"); POJO类型当将POJO类型的DataStream或者DataSet转为表时，如果不指定表名，则默认使用的是POJO字段本身的名称，原始字段名称的映射需要指定原始字段的名称，可以为其起一个别名，也可以调换字段的顺序，也可以只选择部分的字段。 123456789101112// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; //数据类型为Person的POJO类型，字段包括"name"和"age"DataStream&lt;Person&gt; stream = ...// 将DataStream转为表，默认的字段名称为"age", "name"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，为"age"字段指定别名myAge, 为"name"字段指定别名myNameTable table = tableEnv.fromDataStream(stream, "age as myAge, name as myName");// 将DataStream转为表，只选择一个name字段Table table = tableEnv.fromDataStream(stream, "name");// 将DataStream转为表，只选择一个name字段，并起一个别名myNameTable table = tableEnv.fromDataStream(stream, "name as myName"); Row类型Row类型的DataStream或者DataSet转为表的过程中，可以根据字段的位置或者字段名称进行映射，同时也可以为字段起一个别名，或者只选择部分字段。 1234567891011121314// 获取StreamTableEnvironmentStreamTableEnvironment tableEnv = ...; // Row类型的DataStream，通过RowTypeInfo指定两个字段"name"和"age"DataStream&lt;Row&gt; stream = ...// 将DataStream转为表，默认的字段名为原始字段名"name"和"age"Table table = tableEnv.fromDataStream(stream);// 将DataStream转为表，根据位置映射，为第一个字段指定myName别名，为第二个字段指定myAge别名Table table = tableEnv.fromDataStream(stream, "myName, myAge");// 将DataStream转为表，根据字段名映射，为name字段起别名myName，为age字段起别名myAgeTable table = tableEnv.fromDataStream(stream, "name as myName, age as myAge");// 将DataStream转为表，根据字段名映射，只选择name字段Table table = tableEnv.fromDataStream(stream, "name");// 将DataStream转为表，根据字段名映射，只选择name字段，并起一个别名"myName"Table table = tableEnv.fromDataStream(stream, "name as myName"); 查询优化Old plannerApache Flink利用Apache Calcite来优化和转换查询。当前执行的优化包括投影和过滤器下推，去相关子查询以及其他类型的查询重写。Old Planner目前不支持优化JOIN的顺序，而是按照查询中定义的顺序执行它们。 通过提供一个CalciteConfig对象，可以调整在不同阶段应用的优化规则集。这可通过调用CalciteConfig.createBuilder()方法来进行创建，并通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)方法将该对象传递给TableEnvironment。 Blink plannerApache Flink利用并扩展了Apache Calcite来执行复杂的查询优化。这包括一系列基于规则和基于成本的优化(cost_based)，例如： 基于Apache Calcite的去相关子查询 投影裁剪 分区裁剪 过滤器谓词下推 过滤器下推 子计划重复数据删除以避免重复计算 特殊的子查询重写，包括两个部分： 将IN和EXISTS转换为左半联接( left semi-join) 将NOT IN和NOT EXISTS转换为left anti-join 调整join的顺序，需要启用 table.optimizer.join-reorder-enabled 注意： IN / EXISTS / NOT IN / NOT EXISTS当前仅在子查询重写的结合条件下受支持。 查询优化器不仅基于计划，而且还可以基于数据源的统计信息以及每个操作的细粒度开销(例如io，cpu，网络和内存）,从而做出更加明智且合理的优化决策。 高级用户可以通过CalciteConfig对象提供自定义优化规则，通过调用tableEnv.getConfig.setPlannerConfig(calciteConfig)，将参数传递给TableEnvironment。 查看执行计划SQL语言支持通过explain来查看某条SQL的执行计划，Flink Table API也可以通过调用explain()方法来查看具体的执行计划。该方法返回一个字符串用来描述三个部分计划，分别为： 关系查询的抽象语法树，即未优化的逻辑查询计划， 优化的逻辑查询计划 实际执行计划 123456789101112StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream1 = env.fromElements(new Tuple2&lt;&gt;(1, "hello"));DataStream&lt;Tuple2&lt;Integer, String&gt;&gt; stream2 = env.fromElements(new Tuple2&lt;&gt;(1, "hello"));Table table1 = tEnv.fromDataStream(stream1, "count, word");Table table2 = tEnv.fromDataStream(stream2, "count, word");Table table = table1 .where("LIKE(word, 'F%')") .unionAll(table2);// 查看执行计划String explanation = tEnv.explain(table);System.out.println(explanation); 执行计划的结果为： 123456789101112131415161718192021222324252627282930== 抽象语法树 ==LogicalUnion(all=[true]) LogicalFilter(condition=[LIKE($1, _UTF-16LE'F%')]) FlinkLogicalDataStreamScan(id=[1], fields=[count, word]) FlinkLogicalDataStreamScan(id=[2], fields=[count, word])== 优化的逻辑执行计划 ==DataStreamUnion(all=[true], union all=[count, word]) DataStreamCalc(select=[count, word], where=[LIKE(word, _UTF-16LE'F%')]) DataStreamScan(id=[1], fields=[count, word]) DataStreamScan(id=[2], fields=[count, word])== 物理执行计划 ==Stage 1 : Data Source content : collect elements with CollectionInputFormatStage 2 : Data Source content : collect elements with CollectionInputFormat Stage 3 : Operator content : from: (count, word) ship_strategy : REBALANCE Stage 4 : Operator content : where: (LIKE(word, _UTF-16LE'F%')), select: (count, word) ship_strategy : FORWARD Stage 5 : Operator content : from: (count, word) ship_strategy : REBALANCE 小结本文主要介绍了Flink TableAPI &amp;SQL，首先介绍了Flink Table API &amp;SQL的基本概念 ，然后介绍了构建Flink Table API &amp; SQL程序所需要的依赖，接着介绍了Flink的两种planner，还介绍了如何注册表以及DataStream、DataSet与表的相互转换，最后介绍了Flink的两种planner对应的查询优化并给出了一个查看执行计划的案例。 参考文献及资料1、Su Command in Linux (Switch User)，链接：https://linuxize.com/post/su-command-in-linux/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Nginx使用说明]]></title>
    <url>%2F2021%2F05%2F23%2F2021-05-23-Nginx%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Lua语言系列--语言基础]]></title>
    <url>%2F2021%2F05%2F23%2F2021-05-27-Lua%E8%AF%AD%E8%A8%80%E7%B3%BB%E5%88%97--%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Lua语言入门 第二部分 数值 第三部分 字符串 第四部分 表 第五部分 函数 第六部分 输入和输出 第七部分 知识补充 参考文献及资料 背景第一部分 Lua语言入门Lua是解释型语言。 1.1 程序段1.2 语法规范Lua语言中标识符（名称）由任意字母、数值和下划线组成的字符串，但是不能以数值开头。 Lua中关键字（保留字）： 逻辑运算关键字：and、 or、not 基本类型：function、table、nil 控制类：for、 while、do 、break、in、return、until、goto、repeat 逻辑变量：true、false if控制类：if、then 、else、elseif 变量作用域：local Lua语言对于大小写敏感。 Lua语言使用连字符--作为单行注释。多行注释为： 123--[[print("多行注释")--]] 1.3 全局变量Lua语言中，全局变量无需声明，可以直接使用。没有初始化的全局变量初始值为nil。 1.4 类型和值Lua语言属于动态语言。 lua语言中有8种基本类型。 nil（空） boolean（布尔） number（数值） string（字符串） userdata（用户数据） function（函数） thread（线程） table（表） 可以使用函数type来返回变量数据类型。注意type函数返回的是一个字符串。 1.6 练习第二部分 数值第三部分 字符串第四部分 表第五部分 函数第六部分 输入和输出第七部分 知识补充参考文献及资料[1]]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[揭开HDFS存储的面纱]]></title>
    <url>%2F2021%2F05%2F15%2F2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景https://blog.csdn.net/m0_37613244/article/details/109920466?utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromMachineLearnPai2~default-1.control HDFS Namenode本地目录的存储结构和Datanode数据块存储目录结构，也就是hdfs-site.xml中配置的dfs.namenode.name.dir和dfs.namenode.data.dir 第一部分 NameNode元数据第二部分 DataNode数据You need to look in your hdfs-default.xml configuration file for the dfs.data.dir setting. The default setting is: ${hadoop.tmp.dir}/dfs/data and note that the ${hadoop.tmp.dir} is actually in core-default.xml described here. 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn资源调度的]]></title>
    <url>%2F2021%2F05%2F15%2F2021-05-15-%E6%8F%AD%E5%BC%80HDFS%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A2%E7%BA%B1%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景yarn.resourcemanager.store.class : 有三种StateStore，分别是基于zookeeper, HDFS, leveldb, HA高可用集群必须用ZKRMStateStore 存储 yarn.resourcemanager.store.class ZooKeeper org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore FileSystem org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore LevelDB org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore By default the number of completed applications stored in state store is 10000. https://maprdocs.mapr.com/51/ReferenceGuide/Default-YARN-Parameters.html Try to move/delete some completed applications hadoop fs -mv /var/mapr/cluster/yarn/rm/system/FSRMStateRoot/RMAppRoot/* /path_to_local_dir hadoop conf | grep yarn.resourcemanager.max-completed-applications https://www.programmersought.com/article/36321434084/ https://issues.apache.org/jira/browse/YARN-7150 https://my.oschina.net/dabird/blog/3089265 https://cloud.tencent.com/developer/article/1491079 https://my.oschina.net/dabird/blog/4273830 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Pyspark实现原理和源码分析]]></title>
    <url>%2F2021%2F05%2F06%2F2020-10-06-Pyspark%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景https://blog.csdn.net/oTengYue/article/details/105379628 https://www.readfog.com/a/1631040025628086272 spark为了保证核心架构的统一性，在核心架构外围封装了一层python，spark的核心架构功能包括计算资源的申请，task的管理和分配， driver与executor之间的通信，executor之间的通信，rdd的载体等都是在基于JVM的 spark的这种设计可以说是非常方便的去进行多种开发语言的扩展。但是也可以明显看出与在jvm内部运行的udf相比，在python worker中执行udf时，额外增加了数据在executor jvm和pythin worker之间序列化、反序列化、及通信IO等损耗，并且在程序运行上python相比java的具有一定的性能劣势。在计算逻辑比重比较大的spark任务中，使用自定义udf的pyspark程序会明显有更多的性能损耗。当然在spark sql 中使用内置udf会降低或除去上述描述中产生的性能差异。 程序模型提交命令： 1234567[root@quickstart pysparkExample]# cat run.sh /usr/lib/spark/bin/spark-submit \--master yarn \--deploy-mode cluster \--archives hdfs:///user/admin/python/python3.5.2.zip \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./python3.5.2.zip/conda/bin/python \test.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@quickstart pysparkExample]# ./run.sh21/05/16 00:02:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:803221/05/16 00:02:32 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers21/05/16 00:02:32 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (2816 MB per container)21/05/16 00:02:32 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead21/05/16 00:02:32 INFO yarn.Client: Setting up container launch context for our AM21/05/16 00:02:32 INFO yarn.Client: Setting up the launch environment for our AM container21/05/16 00:02:32 INFO yarn.Client: Preparing resources for our AM container21/05/16 00:02:33 INFO yarn.YarnSparkHadoopUtil: getting token for namenode: hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_000221/05/16 00:02:33 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 9 for admin on 172.17.0.2:802021/05/16 00:02:34 INFO hive.metastore: Trying to connect to metastore with URI thrift://quickstart.cloudera:908321/05/16 00:02:34 INFO hive.metastore: Opened a connection to metastore, current connections: 121/05/16 00:02:34 INFO hive.metastore: Connected to metastore.21/05/16 00:02:34 INFO hive.metastore: Closed a connection to metastore, current connections: 021/05/16 00:02:34 INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs:/user/admin/python/python3.5.2.zip21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/home/pyspark/pysparkExample/test.py -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/test.py21/05/16 00:02:34 INFO yarn.Client: Uploading resource file:/tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18/__spark_conf__7787627711692930444.zip -&gt; hdfs://quickstart.cloudera:8020/user/admin/.sparkStaging/application_1621088965108_0002/__spark_conf__7787627711692930444.zip21/05/16 00:02:35 INFO spark.SecurityManager: Changing view acls to: root,admin21/05/16 00:02:35 INFO spark.SecurityManager: Changing modify acls to: root,admin21/05/16 00:02:35 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, admin); users with modify permissions: Set(root, admin)21/05/16 00:02:35 INFO yarn.Client: Submitting application 2 to ResourceManager21/05/16 00:02:35 INFO impl.YarnClientImpl: Submitted application application_1621088965108_000221/05/16 00:02:36 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:36 INFO yarn.Client: client token: Token &#123; kind: YARN_CLIENT_TOKEN, service: &#125; diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.admin start time: 1621094555087 final status: UNDEFINED tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/ user: admin21/05/16 00:02:37 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:38 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:39 INFO yarn.Client: Application report for application_1621088965108_0002 (state: ACCEPTED)21/05/16 00:02:40 INFO yarn.Client: Application report for application_1621088965108_0002 (state: FINISHED)21/05/16 00:02:40 INFO yarn.Client: client token: Token &#123; kind: YARN_CLIENT_TOKEN, service: &#125; diagnostics: N/A ApplicationMaster host: 172.17.0.2 ApplicationMaster RPC port: 0 queue: root.admin start time: 1621094555087 final status: SUCCEEDED tracking URL: http://quickstart.cloudera:8088/proxy/application_1621088965108_0002/history/application_1621088965108_0002/1 user: admin21/05/16 00:02:40 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1621088965108_000221/05/16 00:02:40 INFO util.ShutdownHookManager: Shutdown hook called21/05/16 00:02:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f65e84d5-0438-473c-9dff-03aeb95d4f18 yarn日志： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606121/05/15 16:19:29 INFO yarn.ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]21/05/15 16:19:30 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1621095402395_0001_00000121/05/15 16:19:30 INFO spark.SecurityManager: Changing view acls to: admin21/05/15 16:19:30 INFO spark.SecurityManager: Changing modify acls to: admin21/05/15 16:19:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)21/05/15 16:19:30 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization21/05/15 16:19:30 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 21/05/15 16:19:31 INFO spark.SparkContext: Running Spark version 1.6.021/05/15 16:19:31 INFO spark.SecurityManager: Changing view acls to: admin21/05/15 16:19:31 INFO spark.SecurityManager: Changing modify acls to: admin21/05/15 16:19:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriver' on port 46545.21/05/15 16:19:31 INFO slf4j.Slf4jLogger: Slf4jLogger started21/05/15 16:19:31 INFO Remoting: Starting remoting21/05/15 16:19:31 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]21/05/15 16:19:31 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriverActorSystem@172.17.0.2:35921]21/05/15 16:19:31 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 35921.21/05/15 16:19:31 INFO spark.SparkEnv: Registering MapOutputTracker21/05/15 16:19:31 INFO spark.SparkEnv: Registering BlockManagerMaster21/05/15 16:19:31 INFO storage.DiskBlockManager: Created local directory at /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/blockmgr-edbfee4f-522d-4c06-81a0-5b83b750e88a21/05/15 16:19:31 INFO storage.MemoryStore: MemoryStore started with capacity 491.7 MB21/05/15 16:19:31 INFO spark.SparkEnv: Registering OutputCommitCoordinator21/05/15 16:19:31 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter21/05/15 16:19:31 INFO util.Utils: Successfully started service 'SparkUI' on port 43419.21/05/15 16:19:31 INFO ui.SparkUI: Started SparkUI at http://172.17.0.2:4341921/05/15 16:19:31 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler21/05/15 16:19:31 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40989.21/05/15 16:19:31 INFO netty.NettyBlockTransferService: Server created on 4098921/05/15 16:19:31 INFO storage.BlockManager: external shuffle service port = 733721/05/15 16:19:31 INFO storage.BlockManagerMaster: Trying to register BlockManager21/05/15 16:19:31 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.17.0.2:40989 with 491.7 MB RAM, BlockManagerId(driver, 172.17.0.2, 40989)21/05/15 16:19:31 INFO storage.BlockManagerMaster: Registered BlockManager21/05/15 16:19:32 INFO scheduler.EventLoggingListener: Logging events to hdfs://quickstart.cloudera:8020/user/spark/applicationHistory/application_1621095402395_0001_121/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.821/05/15 16:19:32 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done21/05/15 16:19:32 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@172.17.0.2:46545)21/05/15 16:19:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/172.17.0.2:803021/05/15 16:19:32 INFO yarn.YarnRMClient: Registering the ApplicationMaster21/05/15 16:19:32 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals21/05/15 16:19:32 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 021/05/15 16:19:32 INFO spark.SparkContext: Invoking stop() from shutdown hook21/05/15 16:19:32 INFO ui.SparkUI: Stopped Spark web UI at http://172.17.0.2:4341921/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors21/05/15 16:19:32 INFO cluster.YarnClusterSchedulerBackend: Asking each executor to shut down21/05/15 16:19:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!21/05/15 16:19:32 INFO storage.MemoryStore: MemoryStore cleared21/05/15 16:19:32 INFO storage.BlockManager: BlockManager stopped21/05/15 16:19:32 INFO storage.BlockManagerMaster: BlockManagerMaster stopped21/05/15 16:19:32 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.21/05/15 16:19:32 INFO spark.SparkContext: Successfully stopped SparkContext21/05/15 16:19:32 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED21/05/15 16:19:32 INFO Remoting: Remoting shut down21/05/15 16:19:32 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.21/05/15 16:19:32 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.21/05/15 16:19:32 INFO yarn.ApplicationMaster: Deleting staging directory .sparkStaging/application_1621095402395_000121/05/15 16:19:32 INFO util.ShutdownHookManager: Shutdown hook called21/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21/pyspark-b56e6390-71a2-4475-8ffe-4164798ab6c421/05/15 16:19:32 INFO util.ShutdownHookManager: Deleting directory /yarn/nm/usercache/admin/appcache/application_1621095402395_0001/spark-76c43a00-f562-4e4e-be1a-be3a2fcefc21 http://sharkdtu.com/posts/pyspark-internal.html https://cloud.tencent.com/developer/article/1589011 https://cloud.tencent.com/developer/article/1558621 https://www.nativex.com/cn/blog/2019-12-27-2/ 但是在大数据场景下，JVM和Python进程间频繁的数据通信导致其性能损耗较多，恶劣时还可能会直接卡死，所以建议对于大规模机器学习或者Streaming应用场景还是慎用PySpark，尽量使用原生的Scala/Java编写应用程序，对于中小规模数据量下的简单离线任务，可以使用PySpark快速部署提交。 参考文献及资料1、Job Scheduling，链接：https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中的动态伸缩和反压机制]]></title>
    <url>%2F2021%2F05%2F02%2F2021-05-02-Spark%E4%B8%AD%E7%9A%84%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E5%92%8C%E5%8F%8D%E5%8E%8B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景https://fares.codes/posts/dynamic-scaling-and-backpressure/ 第一部分 建议采用以下做法以实现更好的自动缩放比例： 最好从相当大的集群和数量的执行程序开始，并在必要时进行缩减。（执行程序映射到YARN容器。） 执行者的数量应至少等于接收者的数量。 设置每个执行器的核心数，以使执行器具有一些多余的容量，这些容量超出了运行接收器所需的容量。 内核总数必须大于接收器数量；否则，应用程序将无法处理收到的数据。 设置spark.streaming.backpressure.enabled为true，则Spark Streaming可以控制接收速率（基于当前的批处理调度延迟和处理时间），以便系统仅以其可以处理的速度接收数据。 为了获得最佳性能，请考虑使用Kryo序列化程序在Spark数据的序列化表示和反序列化表示之间进行转换。这不是Spark的默认设置，但是您可以显式更改它：将spark.serializer属性设置 为org.apache.spark.serializer.KryoSerializer。 开发人员可以通过在不再需要DStream时取消缓存它们来减少内存消耗。 参考文献及资料1、Job Scheduling，链接：https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark on Yarn任务动态伸缩机制介绍]]></title>
    <url>%2F2021%2F05%2F02%2F2021-05-02-Spark%20on%20Yarn%E4%BB%BB%E5%8A%A1%E5%8A%A8%E6%80%81%E4%BC%B8%E7%BC%A9%E6%9C%BA%E5%88%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 配置实现 第二部分 动态配置原理和源码分析 第三部分 总结 参考文献及资料 背景Spark默认使用的是资源预分配的模式。即在任务运行之前，需要提前指定任务运行需要的资源量。但是在实际线上生产环境使用过程就存在资源浪费和不足的问题，特别是Spark Streaming类型的任务。例如很多日志数据在一天中量并不是均匀分布的，而是一个“双驼峰”。对于预分配模式，就存在日志峰值期间，运算资源不足导致数据处理的延迟，而在日志低峰时期存在资源闲置却无法释放（特别是资源管理器粗粒度模式）。使得生产线上环境资源未能高效使用。 Spark在Spark 1.2版本后，对于Spark On Yarn模式，开始支持动态资源分配（Dynamic Resource Allocation，后文我们也简称DRA）。该机制下Spark Core和Spark Streaming任务就可以根据Application的负载情况，动态的增加和减少Executors。 第一部分 配置实现对于Spark on Yarn模式需要提前配置Yarn服务，主要是配置External shuffle service（Spark 1.2开始引入）。Spark计算需要shuffle时候，每个Executor 需要把上一个 stage 的 mapper 输出写入磁盘，然后作为 server 等待下一个stage 的reducer 来获取 map 的输出。因此如果 Executor 在 map 阶段完成后被回收，reducer 将无法找到 block的位置。所以开启 Dynamic Resource Allocation 时，必须开启 External shuffle service。这样，mapper 的输出位置（元数据信息）将会由 External shuffle service（长期运行的守护进程） 来登记保存，Executor 不需要再保留状态信息，可以安全回收。 1.1 Yarn服务配置首先需要对Yarn的NodeManager服务进行配置，使其支持Spark的Shuffle Service。 修改每台NodeManager上的配置文件yarn-site.xml： 12345678910111213&lt;!--修改和增加--&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;spark.shuffle.service.port&lt;/name&gt; &lt;value&gt;7337&lt;/value&gt;&lt;/property&gt; 配置服务依赖包。将$SPARK_HOME/lib/spark-1.6.0-yarn-shuffle.jar（注意实际版本号）复制到每台NodeManager的${HADOOP_HOME}/share/hadoop/yarn/lib/下。 重启所有NodeManager生效配置调整。 1.2 Spark core 任务配置1.2.1 配置方法通常配置Saprk应用任务的参数有三种方式： 修改配置文件spark-defaults.conf，全局生效； 配置文件位置：$SPARK_HOME/conf/spark-defaults.conf，具体参数如下： 123456789101112//启用External shuffle Service服务spark.shuffle.service.enabled true//Shuffle Service服务端口，必须和yarn-site中的一致spark.shuffle.service.port 7337//开启动态资源分配spark.dynamicAllocation.enabled true//每个Application最小分配的executor数spark.dynamicAllocation.minExecutors 1//每个Application最大并发分配的executor数spark.dynamicAllocation.maxExecutors 30spark.dynamicAllocation.schedulerBacklogTimeout 1sspark.dynamicAllocation.sustainedSchedulerBacklogTimeout 5s spark-submit 命令配置，个性化生效； 参考下面的案例： 12345678910111213spark-submit --master yarn-cluster \ --driver-cores 2 \ --driver-memory 2G \ --num-executors 10 \ --executor-cores 5 \ --executor-memory 2G \ --conf spark.dynamicAllocation.enabled=true \ --conf spark.shuffle.service.enabled=true \ --conf spark.dynamicAllocation.minExecutors=5 \ --conf spark.dynamicAllocation.maxExecutors=30 \ --conf spark.dynamicAllocation.initialExecutors=10 --class com.spark.sql.jdbc.SparkDFtoOracle2 \ Spark-hive-sql-Dataframe-0.0.1-SNAPSHOT-jar-with-dependencies.jar 代码中配置，个性化生效； 参考下面的scala代码案例： 123456val conf: SparkConf = new SparkConf()conf.set("spark.dynamicAllocation.enabled", true);conf.set("spark.shuffle.service.enabled", true);conf.set("spark.dynamicAllocation.minExecutors", "5");conf.set("spark.dynamicAllocation.maxExecutors", "30");conf.set("spark.dynamicAllocation.initialExecutors", "10"); 接下来我们介绍详细的参数含义。 1.2.2 配置说明 Property Name Default Meaning Since Version spark.dynamicAllocation.enabled false Whether to use dynamic resource allocation, which scales the number of executors registered with this application up and down based on the workload. For more detail, see the description here. This requires spark.shuffle.service.enabled or spark.dynamicAllocation.shuffleTracking.enabled to be set. The following configurations are also relevant: spark.dynamicAllocation.minExecutors, spark.dynamicAllocation.maxExecutors, and spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.executorAllocationRatio 1.2.0 spark.dynamicAllocation.executorIdleTimeout 60s If dynamic allocation is enabled and an executor has been idle for more than this duration, the executor will be removed. For more detail, see this description. 1.2.0 spark.dynamicAllocation.cachedExecutorIdleTimeout infinity If dynamic allocation is enabled and an executor which has cached data blocks has been idle for more than this duration, the executor will be removed. For more details, see this description. 1.4.0 spark.dynamicAllocation.initialExecutors spark.dynamicAllocation.minExecutors Initial number of executors to run if dynamic allocation is enabled. If --num-executors (or spark.executor.instances) is set and larger than this value, it will be used as the initial number of executors. 1.3.0 spark.dynamicAllocation.maxExecutors infinity Upper bound for the number of executors if dynamic allocation is enabled. 1.2.0 spark.dynamicAllocation.minExecutors 0 Lower bound for the number of executors if dynamic allocation is enabled. 1.2.0 spark.dynamicAllocation.executorAllocationRatio 1 By default, the dynamic allocation will request enough executors to maximize the parallelism according to the number of tasks to process. While this minimizes the latency of the job, with small tasks this setting can waste a lot of resources due to executor allocation overhead, as some executor might not even do any work. This setting allows to set a ratio that will be used to reduce the number of executors w.r.t. full parallelism. Defaults to 1.0 to give maximum parallelism. 0.5 will divide the target number of executors by 2 The target number of executors computed by the dynamicAllocation can still be overridden by the spark.dynamicAllocation.minExecutors and spark.dynamicAllocation.maxExecutors settings 2.4.0 spark.dynamicAllocation.schedulerBacklogTimeout 1s If dynamic allocation is enabled and there have been pending tasks backlogged for more than this duration, new executors will be requested. For more detail, see this description. 1.2.0 spark.dynamicAllocation.sustainedSchedulerBacklogTimeout schedulerBacklogTimeout Same as spark.dynamicAllocation.schedulerBacklogTimeout, but used only for subsequent executor requests. For more detail, see this description. 1.2.0 spark.dynamicAllocation.shuffleTracking.enabled false Experimental. Enables shuffle file tracking for executors, which allows dynamic allocation without the need for an external shuffle service. This option will try to keep alive executors that are storing shuffle data for active jobs. 3.0.0 spark.dynamicAllocation.shuffleTracking.timeout infinity When shuffle tracking is enabled, controls the timeout for executors that are holding shuffle data. The default value means that Spark will rely on the shuffles being garbage collected to be able to release executors. If for some reason garbage collection is not cleaning up shuffles quickly enough, this option can be used to control when to time out executors even when they are storing shuffle data. 3.0.0 1.3 Spark Streaming 任务配置对于Spark Streaming 流处理任务，Spark官方并未在文档中给出介绍。Dynamic Resource Allocation配置指引如下： 必要配置（Spark 3.0.0） 123456# 开启Spark Streaming流处理动态资源分配参数开关（默认关闭）spark.streaming.dynamicAllocation.enabled=true# 设置最大和最小的Executor数量spark.streaming.dynamicAllocation.minExecutors=1（必须正整数）spark.streaming.dynamicAllocation.maxExecutors=50（必须正整数，默认Int.MaxValue，即无限大） 可选配置（Spark 3.0.0） 这些参数可以不用配置，都已经提供了一个较为合理的默认值。 123spark.streaming.dynamicAllocation.scalingUpRatio（必须正数，默认0.9）spark.streaming.dynamicAllocation.scalingInterval（单位秒，默认60）spark.streaming.dynamicAllocation.scalingDownRatio（必须正数，默认0.3） 第二部分 动态配置原理和源码分析介绍完使用配置后，接下来将详细介绍实现原理。以便理解各参数的含义和参数调优。 2.1 Spark Core任务为了动态伸缩Spark任务的计算资源（Executor为基本分配单位），首先需要确定的度量是任务的繁忙程度。DRA机制将Spark任务是否有挂起任务(pending task)作为判断标准，一旦有挂起任务表示当前的Executor数量不够支撑所有的task并行运行，所以会申请增加资源。 2.1.1 资源请求（Request）策略当Spark任务开启DRA机制，SparkContext会启动后台ExecutorAllocationManager，用来管理集群的Executors。 1234567891011121314151617//package org.apache.spark SparkContext.scalaval dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf) _executorAllocationManager = if (dynamicAllocationEnabled) &#123; schedulerBackend match &#123; case b: ExecutorAllocationClient =&gt; Some(new ExecutorAllocationManager( schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf, cleaner = cleaner, resourceProfileManager = resourceProfileManager)) case _ =&gt; None &#125; &#125; else &#123; None &#125; _executorAllocationManager.foreach(_.start()) Start()方法将ExecutorAllocationListener加入到listenerBus中，ExecutorAllocationListener通过监听listenerBus里的事件，动态添加，删除Executor。并且通过Thread不断添加Executor，遍历Executor，将超时的Executor杀掉并移除。 Spark会周期性（intervalMillis=100毫秒）计算实际需要的Executor的最大数量maxNeeded。公式如下。 12val maxNeeded = math.ceil(numRunningOrPendingTasks * executorAllocationRatio / tasksPerExecutor).toInt 逻辑代码： 12345678910111213141516private def updateAndSyncNumExecutorsTarget(now: Long): Int = synchronized &#123; if (initializing) &#123; 0 &#125; else &#123; val updatesNeeded = new mutable.HashMap[Int, ExecutorAllocationManager.TargetNumUpdates] numExecutorsTargetPerResourceProfileId.foreach &#123; case (rpId, targetExecs) =&gt; val maxNeeded = maxNumExecutorsNeededPerResourceProfile(rpId) if (maxNeeded &lt; targetExecs) &#123; decrementExecutorsFromTarget(maxNeeded, rpId, updatesNeeded) &#125; else if (addTime != NOT_SET &amp;&amp; now &gt;= addTime) &#123; addExecutorsToTarget(maxNeeded, rpId, updatesNeeded) &#125; &#125; doUpdateRequest(updatesNeeded.toMap, now) &#125;&#125; 当集群中有Executor出现pending task，计算判断条件maxNeeded &gt; targetExecs，并且等待时间超过schedulerBacklogTimeout(默认1s)，则会触发方法addExecutorsToTarget(maxNeeded, rpId, updatesNeeded)。对于首次增加Executor。 1spark.dynamicAllocation.schedulerBacklogTimeout = 1s（秒） 后续按照周期性时间sustainedSchedulerBacklogTimeout来检测pending task，一旦出现pending task，即触发增加Executor。 1spark.dynamicAllocation.sustainedSchedulerBacklogTimeout = 1s(秒) 每次（轮）触发增加Executor资源请求，增加的数量翻倍，即是一个指数数列（2的n次方），例如：1、2、4、8。 2.1.2 资源释放（Remove）策略对于移除策略如下： 如果Executor闲置（maxNeeded &lt; targetExecs）时间超过以下参数，并且executor中没有cache（数据缓存在内存），则spark应用将会释放该Executor。 1spark.dynamicAllocation.executorIdleTimeout（单位为秒） 默认60s 如果空闲Executor中有cache，那么这个超时参数为： 1spark.dynamicAllocation.cachedExecutorIdleTimeout 默认值：Integer.MAX_VALUE（即永不超时） 对于Executor的退出，设计上需要考虑状态的问题，主要： 需要移除的Executor存在cache。 如果需要移除的Executor含有RDD cache。这时候超时时间为整型最大值（相当于无限）。 123456private[spark] val DYN_ALLOCATION_CACHED_EXECUTOR_IDLE_TIMEOUT = ConfigBuilder("spark.dynamicAllocation.cachedExecutorIdleTimeout") .version("1.4.0") .timeConf(TimeUnit.SECONDS) .checkValue(_ &gt;= 0L, "Timeout must be &gt;= 0.") .createWithDefault(Integer.MAX_VALUE) Shuffle状态的保存问题。如果需要移除的Executor包含了Shuffle状态数据（在shuffle期间，Spark executor先要将map的输出写入到磁盘，然后该executor充当一个文件服务器，将这些文件共享给其他的executor访问）。需要提前启动External shuffle service，由专门外置服务提供存储，Executor中不再负责保存，架构上功能解耦。 另外添加和移除Executor之后，需要告知DAGSchedule进行相关信息更新。 2.1.3 配置建议Spark的动态伸缩机制的几点建议： 给Executor数量设置一个合理的伸缩区间，即[minExecutors-maxExecutors]区间值。 配置资源粒度较小的Executor，例如CPU数量为3-4个。动态伸缩的最小伸缩单位是单个Executor，如果出现资源伸缩，特别是Executor数目下降后业务量突增，新申请资源未就绪，已有的Executor就可能由于任务过载而导致集群崩溃。 如果程序中有shuffle,例如(reduce,groupBy),建议设置一个合理的并行数，避免杀掉过多的Executors。 对于每个Stage持续时间很短的应用，不适合动态伸缩机制。这样会频繁增加和移除Executors，造成系统颠簸。特别是在 Spark on Yarn模式下资源的申请处理速度并不快。 2.2 Spark Streaming 任务Spark Streaming任务可以看成连续运行的微（micro-batch）批任务，如果直接套用Spark Core的动态伸缩机制就水土不服了。一般一个微批任务较短（默认60秒），实际线上任务可能更小，动态伸缩的反应时间较长（特别是on Yarn模式），一个微批任务结束，动态伸缩策略还没生效。所以针对Spark Streaming任务，项目组设计新的动态机制（Spark 2.0.0 版本引入）。 提案：https://issues.apache.org/jira/browse/SPARK-12133 2.2.1 源码分析Spark Streaming任务会统计微批任务运行时间的延迟时间，最朴素的想法就是按照这个度量指标来作为动态伸缩的触发指标。这部分源码在org.apache.spark.streaming.scheduler中： 周期性计算微批运行完成的平均时间，然后和batch interval进行比较； 这里的周期大小由参数spark.streaming.dynamicAllocation.scalingInterval决定，大小为scalingIntervalSecs * 1000。例如默认值为：60*1000毫秒，即60秒。 通过streamingListener计算微批平均处理时间（averageBatchProcTime），然后计算微批处理率（ratio，微批平均处理时间/微批处理周期）。 然后和参数值上限（scalingUpRatio）和下限（scalingDownRatio）进行比较。详细控制函数如下： 123456789101112131415161718private def manageAllocation(): Unit = synchronized &#123; logInfo(s"Managing executor allocation with ratios = [$scalingUpRatio, $scalingDownRatio]") if (batchProcTimeCount &gt; 0) &#123; val averageBatchProcTime = batchProcTimeSum / batchProcTimeCount val ratio = averageBatchProcTime.toDouble / batchDurationMs logInfo(s"Average: $averageBatchProcTime, ratio = $ratio" ) if (ratio &gt;= scalingUpRatio) &#123; logDebug("Requesting executors") val numNewExecutors = math.max(math.round(ratio).toInt, 1) requestExecutors(numNewExecutors) &#125; else if (ratio &lt;= scalingDownRatio) &#123; logDebug("Killing executors") killExecutor() &#125; &#125; batchProcTimeSum = 0 batchProcTimeCount = 0 &#125; 增加Executor数量；如果ratio &gt;= scalingUpRatio，然后按照下面的公司增加数量： 1val numNewExecutors = math.max(math.round(ratio).toInt, 1) 例如ratio=1.6&gt;0.9(scalingUpRatio)，这时候说明有大量微批任务出现了延迟，按照公式计算numNewExecutors=2。接下来会调用requestExecutors(numNewExecutors)方法去申请2个Executor。 减少Executor数量；如果ratio &lt;= scalingDownRatio，这直接调用killExecutor()方法（方法中判断没有receiver运行的Executor）去kill Executor。 2.2.2 配置建议Spark Streaming动态资源分配起作用前，需要至少完成一个Batch处理(batchProcTimeCount &gt; 0)。 Spark Core和Spark Streaming的动态配置开关配置是分别设置的。 如果两个配置开关同时配置为true，会抛出错误。建议如下配置： 12spark.dynamicAllocation.enabled=false （默认是false，可以不配置）spark.streaming.dynamicAllocation.enabled=true 第三部分 总结3.1 对比Spark Core中动态伸缩机制是基于空闲时间来控制回收Executor。而在Spark Streaming中，一个Executor每隔很短的时间都会有一批作业被调度，所以在streaming里面是基于平均每批作业处理的时间。 3.2 Structed Streaming任务动态伸缩在spark Streaming中，最小的可能延迟受限于每批的调度间隔以及任务启动时间。所以这不能满足更低延迟的需求。如果能够连续的处理，尤其是简单的处理而没有任何的阻塞操作。这种连续处理的架构可以使得端到端延迟最低降低到1ms级别，而不是目前的10-100ms级别，这就是Spark 2.2.0版本引入新的Spark流处理框架：Structed Streaming。 https://issues.apache.org/jira/browse/SPARK-20928 当然项目组自然也会考虑该框架的资源伸缩机制（未完成） https://issues.apache.org/jira/browse/SPARK-24815 后续趋势上看，Spark项目会将更多精力放在Structed Streaming。 3.3 Spark Streaming 背压机制为了应对Spark Streaming处理数据波动，除了资源动态伸缩机制，在Spark 1.5版本项目在Spark Streaming 中引入了的背压（Backpressure）机制。 Spark Streaming任务中，当batch的处理时间大于batch interval时，意味着数据处理速度跟不上数据接收速度。这时候在数据接收端(Receiver)Executor就会开始积压数据。如果数据存储采用MEMORY_ONLY模式（内存）就会导致OOM，采用MEMORY_AND_DISK多余的数据保存到磁盘上，增加数据IO时间。 背压（Backpressure）机制，通过动态控制数据接收速率来适配集群数据处理能力。这是被动防守型的应对，将数据缓存在Kafka消息层。如果数据持续保持高量级，就需要主动启停任务来增加计算资源。 参考文献及资料1、Job Scheduling，链接：https://spark.apache.org/docs/latest/job-scheduling.html#configuration-and-setup 2、About Spark Streaming，链接：https://www.turbofei.wang/spark/2019/05/26/about-spark-streaming]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[suse操作系统rpm命令]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-30-suse%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9Frpm%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[背景RPM 安装操作 命令： rpm -i 需要安装的包文件名 举例如下： rpm -i example.rpm 安装 example.rpm 包； rpm -iv example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息； rpm -ivh example.rpm 安装 example.rpm 包并在安装过程中显示正在安装的文件信息及安装进度； RPM 查询操作 命令： rpm -q … 附加查询命令： a 查询所有已经安装的包以下两个附加命令用于查询安装包的信息； i 显示安装包的信息； l 显示安装包中的所有文件被安装到哪些目录下； s 显示安装版中的所有文件状态及被安装到哪些目录下；以下两个附加命令用于指定需要查询的是安装包还是已安装后的文件； p 查询的是安装包的信息； f 查询的是已安装的某文件信息； 举例如下： rpm -qa | grep tomcat4 查看 tomcat4 是否被安装； rpm -qip example.rpm 查看 example.rpm 安装包的信息； rpm -qif /bin/df 查看/bin/df 文件所在安装包的信息； rpm -qlf /bin/df 查看/bin/df 文件所在安装包中的各个文件分别被安装到哪个目录下； RPM 卸载操作 命令： rpm -e 需要卸载的安装包 在卸载之前，通常需要使用rpm -q …命令查出需要卸载的安装包名称。 举例如下： rpm -e tomcat4 卸载 tomcat4 软件包 RPM 升级操作 命令： rpm -U 需要升级的包 举例如下： rpm -Uvh example.rpm 升级 example.rpm 软件包 RPM 验证操作 命令： rpm -V 需要验证的包 举例如下： rpm -Vf /etc/tomcat4/tomcat4.conf 输出信息类似如下： S.5….T c /etc/tomcat4/tomcat4.conf 其中，S 表示文件大小修改过，T 表示文件日期修改过。限于篇幅，更多的验证信息请您参考rpm 帮助文件：man rpm RPM 的其他附加命令 –force 强制操作 如强制安装删除等； –requires 显示该包的依赖关系； –nodeps 忽略依赖关系并继续操作； 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sparkSql使用总结]]></title>
    <url>%2F2021%2F04%2F15%2F2021-06-06-sparkSql%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景https://blog.csdn.net/weixin_40035337/article/details/108018058 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orange网关使用手册]]></title>
    <url>%2F2021%2F04%2F15%2F2021-04-19-orange%E7%BD%91%E5%85%B3%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[背景Orange是一个基于OpenResty的API Gateway，提供API及自定义规则的监控和管理，如访问统计、流量切分、API重定向、API鉴权、WEB防火墙等功能。Orange可用来替代前置机中广泛使用的Nginx/OpenResty， 在应用服务上无痛前置一个功能丰富的网关系统。 流量网关 业务网关； API内置插件以HTTP Restful形式开放全部API，详细可查看API文档 注意 现实中由于用户的业务系统多种多样，对于复杂应用，Orange并不是一个开箱即用的组件，需要调整一些配置才能集成到现有系统中。 Orange提供的的配置文件和示例都是最简配置，用户使用时请根据具体项目或业务需要自行调整，这些调整可能包括但不限于: 使用的各个shared dict的大小， 如ngx.shared.status nginx.conf配置文件中各个server、location的配置及其权限控制，比如orange dashboard/API的server应该只对内部有权限的机器开放访问 根据不同业务而设置的不同nginx配置，如timeout、keepalive、gzip、log、connections等等 LicenceOrange采用MIT协议开源 其它 Orange的插件模式参考了Kong，Kong是一个功能比较全面的API Gateway实现，推荐关注。 Orange与Kong的不同(刨除基础设计，如存储、API)主要体现在针对”插件”和”API”的组织方式， Orange在流量筛选和变量提取方面相对来说更灵活一些。 第一部分 Nginx知识准备1.1 Nginx配置文件nginx 的配置文件结构中 HTTP 配置主要包括三个区块，结构如下： 1234567Global: nginx 运行相关Events: 与用户的网络连接相关http http Global: 代理，缓存，日志，以及第三方模块的配置 server server Global: 虚拟主机相关 location: 地址定向，数据缓存，应答控制，以及第三方模块的配置 从上面展示的 nginx 结构中可以看出 location 属于请求级别配置，这也是我们最常用的配置。 1.2 location介绍1.2.1 location 语法Location 块通过指定模式来与客户端请求的URI相匹配。Location基本语法： 匹配 URI 类型，有四种参数可选，当然也可以不带参数。 命名location，用@来标识，类似于定义goto语句块。 12location [ = | ~ | ~* | ^~ | !~ | !~* ] /uri/&#123; … &#125;location @/name/ &#123; … &#125; 各类参数含义： =表示请求字符串与其精准匹配，成功则立即处理，nginx停止搜索其他匹配； ~ 表示区分大小写正则匹配； ~* 表示不区分大小写正则匹配； ^~ 表示URI以某个常规字符串开头，并要求一旦匹配到就会立即处理，不再去匹配其他的正则 URI，一般用来匹配目录； !~ 表示区分大小写正则不匹配； !~* 表示不区分大小写正则不匹配； / 通用匹配，任何请求都会匹配到； @ 定义一个命名的 location，@ 定义的locaiton名字一般用在内部定向，例如error_page, try_files命令中。它的功能类似于编程中的goto。 1.2.2 location匹配顺序nginx有两层指令来匹配请求 URI 。第一个层次是 server 指令，它通过域名、ip 和端口来做第一层级匹配，当找到匹配的 server 后就进入此 server 的 location 匹配。 location 的匹配并不完全按照其在配置文件中出现的顺序来匹配，请求URI 会按如下规则进行匹配： 先精准匹配 = ，精准匹配成功则会立即停止其他类型匹配； 没有精准匹配成功时，进行前缀匹配。先查找带有 ^~ 的前缀匹配，带有 ^~ 的前缀匹配成功则立即停止其他类型匹配，普通前缀匹配（不带参数 ^~ ）成功则会暂存，继续查找正则匹配； = 和 ^~ 均未匹配成功前提下，查找正则匹配 ~ 和 ~\* 。当同时有多个正则匹配时，按其在配置文件中出现的先后顺序优先匹配，命中则立即停止其他类型匹配； 所有正则匹配均未成功时，返回步骤 2 中暂存的普通前缀匹配（不带参数 ^~ ）结果 以上规则简单总结就是优先级从高到低依次为（序号越小优先级越高）： 1234561. location = # 精准匹配2. location ^~ # 带参前缀匹配3. location ~ # 正则匹配（区分大小写）4. location ~* # 正则匹配（不区分大小写）5. location /a # 普通前缀匹配，优先级低于带参数前缀匹配。6. location / # 任何没有匹配成功的，都会匹配这里处理 1.2 网关中流量筛选1.2.1 orange中流量选择器orange本质是使用web的方式动态配置nginx，就需要能过滤流量。实现方式是：流量选择器，如下图： 名称，定义流量选择器名称； 类型，可选参数有：全流量、自定义流量； 全流量匹配就是对原始流量不过滤。 自定义流量，需要设置匹配方式与条件，符合条件的请求才会被进行流量管理。 规则，自定义流量开启参数。可选参数有：单一条件匹配、and匹配、or匹配、复杂匹配。 单一条件匹配，单个条件，只能配置一个条件； and匹配，多个条件以且的逻辑过滤； or匹配，多个条件以或的逻辑过滤； 复杂匹配，即自定义条件之间逻辑关系； 按照表达式对所有条件求值，表达式不能为空。表达式中每个值的格式为v[index], 比如v[1]对应的就是第一个条件的值。 例如我们编写了3个条件，表达式为：(v[1] or v[2]) and v[3]。即前两个条件至少一个为真并且第三个条件为真时，规则为真。3个条件按照顺序分别对应：v[1] 、v[2]、v[3]。 条件编写，一条完整的调优有三个要素：条件类型、匹配类型、正则表达式。 条件类型有： Random， URI 根据你请求路径中的 uri 来进行匹配，在接入网关的时候，前端几乎不用做任何更改。 在选择器中，推荐使用 uri 中的前缀来进行匹配，而在规则中，则使用具体路径来进行匹配。 Header，K/V类型 根据 http 请求头中的字段值来匹配。这个类型的name非空。 Query，K/V类型 根据 uri 中的查询参数来进行匹配，比如 /test?a=1&amp;b=2 ，那么可以选择该匹配方式。 Cookie Cookie是用于维持服务端会话状态的，通常由服务端写入，在后续请求中，供服务端读取。 Postparams，K/V类型 IP 根据 http 调用方的 ip 来进行匹配。尤其是在 waf 插件里面，如果发现一个 ip 地址有攻击，可以新增一条匹配条件，填上该 ip ，拒绝该 ip 的访问。 UserAgent User-Agent会告诉网站服务器，访问者是通过什么工具来请求的。包含了一个特征字符串，用来让网络协议的对端来识别发起请求的用户代理软件的应用类型、操作系统、软件开发商以及版本号。例如火狐浏览器发起的请求，User-Agent字段为： 1Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0 Host 根据 http 调用方的 host 来进行匹配。尤其是在 waf 插件里面，如果发现一个 host 地址有攻击，可以新增一条匹配条件，填上该 host ，拒绝该 host 的访问。 Referer HTTP 协议在请求（request）的头信息里面，设计了一个Referer字段，给出”引荐网页”的 URL。这个字段是可选的。客户端发送请求的时候，自主决定是否加上该字段。 HttpMethod HTTP 请求可以使用多种请求方法。HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD方法。HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。一共有8种请求方法。 匹配的类型有： Match，正则匹配 Not Match，正则不匹配 =，精确相等 !=，精确不相等 &gt; &gt;= &lt; &lt;= % 表达式，这里填写正则表达式或者匹配值。 处理，两个参数：继续后续选择器、略过后续选择器； 继续后续选择器，流量继续被其他选择器过滤。 略过后续选择器，流量终止后面的选择器过滤。 是否开启，是否生效选择器。 时候记录日志，过滤结果是否记录日志。 在多个插件里（比如URL重定向插件、WAF插件、自定义监控插件）都使用了选择器来将流量进行第一步划分， 在流量被一个选择器命中后才会进入它内部的规则过滤。 使用选择器的目的是减少不必要的规则判断，进一步提高性能。 1.2.2 orange中的流量规则流量规则必须归属一个流量选择器，用来进一步过滤被流量选择器过滤后的流量。 一个流量选择器，可以有多个所属规则器。 下图是一个规则器，不同的插件的规则器不相同，我们后续在插件中分别介绍。 1.3 变量提取变量提取模块是很多orange插件都使用到的一个概念， 它主要用来从请求中提取出各种信息， 比如query string, form表单里的字段， 某个header头等等。 它支持两种提取方式： 索引式提取 模板式提取 1.3.1 索引式提取顾名思义， 索引式提取的含义是将提取出的所有变量按照顺序保存在一个数组中， 后续以下标的方式来使用。 比如我们在”变量提取模块”提取了三个值： 那么之后就可以通过\${1}、​\${2}、​\${3}来使用， 其中 ${1}指的是header头里的app_version字段， 如果header没有此字段， 则赋一个默认值v0.1 ${2}指的是query中的uid字段 ${3}指的是query中age字段， 若无则给予默认值18 1.3.2 模板式提取模板时提取主要为了解决索引式提取必须要按序使用的问题， 并且当需要从uri中提取多个值时索引式提取方式并不友好。 如以下示例， 我们提取了四个值： 则之后我们可以通过以下方式来使用： 指的是从query中提取出的uid字段 指的是从query中提取出的age字段, 若无则给予默认值18 指的是从格式为1^/start/(.*)/(.*)/end 的URI中提取出的第1个分段值 比如， 如果URI为/start/abc/123/end, 则此时值为abc 如果URI为/start/momo/sharp/end, 则此时值为momo 指的是从格式为1^/start/(.*)/(.*)/end 的URI中提取出的第2个分段值 比如， 若URI为/start/abc/123/end, 则此时值为123 如果URI为/start/momo/sharp/end, 则此时值为sharp 注意， 若从URI中提取， 仍然要根据顺序来使用， 如、、. 设计原理:https://github.com/orlabs/orange/issues/15 第二部分 核心组件介绍最新稳定版本0.8.1，该版本对第三方组件进行去除。这里核心组件主要是0.6.4版本中组件。 2.1 全局统计可统计API访问情况、Nginx连接情况、流量统计、QPS、应用版本、服务器信息等。如下图： 2.2 自定义监控可根据配置的规则筛选出流量并监控，统计其各个指标。当前的监控指标有： 请求总次数： 分别统计200/300/400/500区间的请求数 QPS 请求总耗时/平均请求耗时 总流量/平均请求流量 案例： 例如筛选出指定流量，进行监控。下面是监控视图： 2.3 URL重定向（redirect）网关实现的重定向主要是：当客户端向网关请求URL资源的时候，网关通知客户端实际的资源地址，然后客户端向实际的URL请求资源。重定向是指当浏览器请求一个URL时，服务器返回一个重定向指令，告诉浏览器地址已经变了，麻烦使用新的URL再重新发送新请求。 网关实现了通过UI配置各种rewrite策略，省去手写nginx rewrite和重启。redirect是浏览器和服务器发生两次请求，也就是服务器命令客户端“去访问某个页面”。 2.3.1 案例我们使用重定向来代理网关的官网(http://orange.sumory.com/)。 首先，添加选择器： 然后在选择器中创建新的规则： 配置完成后，当我们访问192.168.52.137:8888/to_orange时候，会自动跳转为：http://orange.sumory.com/。 2.3.2 参数说明重定向有两种：一种是302响应，称为临时重定向，一种是301响应，称为永久重定向。两者的区别是，如果服务器发送301永久重定向响应，浏览器会缓存/hi到/hello这个重定向的关联，下次请求/hi的时候，浏览器就直接发送/hello请求了。 2.4 URI 重写(Rewrite)Url重写主要用于站内请求的重写。rewrite则是服务器内部的一个接管，在服务器内部告诉“某个页面请帮我处理这个用户的请求”，浏览器和服务器只发生一次交互，浏览器不知道是该页面做的响应，浏览器只是向服务器发出一个请求。 URL重写用于将页面映射到本站另一页面，若重写到另一网络主机（域名），则按重定向处理。 rewrite是把一个地址重写成另一个地址。地址栏不跳转。相当于给另一个地址加了一个别名一样。 2.4.1 案例我们使用重写（rewrite）来重写上面案例中地址。 首先添加选择器： 然后在选择器中创建新的规则： 配置完成后，当我们访问http://192.168.52.137:8888/to_orange_test时候，流量被映射到本站另一个地址http://192.168.52.137:8888/to_orange。而后面地址被重定向到http://orange.sumory.com/。 2.4.2 参数说明2.5 HTTP Basic Authorizationbasic auth是最简单权限认证方式，密钥被base64加密，但是网络传输是非加密的，一旦被截取，解密是容易的。所以通常用于安全的内部网络。 案例参数说明2.6 HTTP Key Auth案例参数说明2.7 Signature Auth案例参数说明https://www.cnblogs.com/Sinte-Beuve/p/12093307.html 2.8 Rate Limiting 访问限速案例参数说明2.9 Rate Limiting 防刷案例参数说明2.10 WAF 防火墙案例参数说明2.11 代理 &amp; 分流 &amp; ABTesting当前divide分流插件是静态的，需要提前在nginx.conf里配置upstream，但是这样不利于灵活管理，能否实现动态配置upstream。 分流插件，可分为三个使用场景： 作为proxy，如代理后端的多个HTTP应用 用于AB测试 用于动态分流，API版本控制等 https://book.aikaiyuan.com/openresty/orange-divide.html#%E8%AF%95%E9%AA%8C%E7%8E%AF%E5%A2%83 http://bbs.orchina.org/topic/160/view 案例我们使用该插件代理elasticsearch集群节点。 我们对es进行负载配置 http://www.ttlsa.com/nginx/nginx-elasticsearch/ 123upstream es_upstream &#123; server 192.168.31.3:9200; &#125; https://github.com/orlabs/orange/issues/136 2.12 KV Store第三部分 第三方插件https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html The balancer plugin migrated tov0.9.0-dev due to conflicts with existing features. The dynamic_upstream plugin migrated tov0.9.0-dev due to conflicts with existing features. The consul_balancer plugin migrated tov0.9.0-dev due to conflict with existing functions. The persist plugin migrated tov0.9.0-dev due to conflicts with existing features. 3.1 node插件该插件主要用户网关集群管理。 node plugin（容器集群节点管理插件） 新增集群节点注册命令 orange register 通过 dashboard 面板同步节点配置信息 配合 persist 插件，可以查看历史统计信息 12345678influxdb:/usr/local/orange/conf # opm install ledgetech/lua-resty-http* Fetching ledgetech/lua-resty-http Downloading https://opm.openresty.org/api/pkg/tarball/ledgetech/lua-resty-http-0.14.opm.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 20862 100 20862 0 0 7545 0 0:00:02 0:00:02 --:--:-- 7545 Package ledgetech/lua-resty-http 0.14 installed successfully under /usr/local/openresty/site/ . https://github.com/orlabs/orange/issues/353 3.2 headers 插件用于修改请求头。 3.3 balancer插件用户负载多个upstream https://zhjwpku.com/2017/11/14/orange-balancer-plugin-tutorial.html 3.4 Consul Upstream3.4 Dynamic Upstream案例参数说明3.6 HTTP Jwt Auth3.7 HTTP Hmac Auth3.8 持久日志第四部分 插件的优先级orange中所有插件都是继承基本组件的，文件plugins\base_handler.lua中定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344local Object = require("orange.lib.classic")local BasePlugin = Object:extend()function BasePlugin:new(name) self._name = nameendfunction BasePlugin:get_name() return self._nameendfunction BasePlugin:init_worker() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": init_worker")endfunction BasePlugin:redirect() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": redirect")endfunction BasePlugin:rewrite() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": rewrite")endfunction BasePlugin:access() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": access")endfunction BasePlugin:balancer() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": balancer")endfunction BasePlugin:header_filter() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": header_filter")endfunction BasePlugin:body_filter() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": body_filter")endfunction BasePlugin:log() ngx.log(ngx.DEBUG, " executing plugin \"", self._name, "\": log")endreturn BasePlugin 第五部分 Dashboard 用户管理从v0.1.1版本开始，Orange Dashboard支持授权验证（默认未开启）。当配置开启后，只有通过成功登录的账户才能登陆展现Dashboard。 配置这部分功能在配置文件conf/orange.conf中： 12345678"dashboard": &#123; "auth": false,//是否开启用户鉴权，默认为false "session_secret": "y0ji4pdj61aaf3f11c2e65cd2263d3e7e5",//用于加密cookie的盐 "whitelist": [ "^/auth/login$", "^/error/$" ]&#125; Dashboard 用户的用户信息存储在Mysql的dashboard_user表中。 默认系统管理员用户名和密钥如下，首次登陆后可以修改和添加其他用户： 12用户名：admin密码：orange_admin 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orange网关原理的源码分析]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-30-orange%E7%BD%91%E5%85%B3%E7%94%9F%E4%BA%A7%E7%BB%B4%E6%8A%A4%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[背景Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做selector, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行规则匹配， 匹配到后进行相关处理。 https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/ https://book.aikaiyuan.com/openresty/understanding-orange.html https://zhuanlan.zhihu.com/p/67481992 生产单机部署生产集群部署配置更新日志切割https://jingsam.github.io/2019/01/15/nginx-access-log.html 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx常见使用场景总结]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-30-Nginx%E5%B8%B8%E8%A7%81%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景 HTTP服务器（含动静分离） 负载均衡 反向代理 正向代理 跨域请求 第一部分 HTTP服务器（含动静分离）Nginx本身是一个静态资源的服务器，当只有静态资源的时候，就可以使用Nginx来做服务器，如下，我们使用Nginx来部署一个打包好的vue项目 12345678#vue项目server&#123; listen 8081; #监听端口 server_name 209.250.235.145; root /app/vue/dist/; # 我们的资源在服务器中的路径 index index.html; #指定资源的入口文件&#125;复制代码 完成后我们nginx -s reload一下，然后访问209.250.235.145:8081，只要路径没错静态资源就访问的到了 第二部分 正向代理第三部分 反向代理反向代理应该是Nginx做的最多的一件事了，什么是反向代理呢，以下是百度百科的说法：反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。简单来说就是真实的服务器不能直接被外部网络访问，所以需要一台代理服务器，而代理服务器能被外部网络访问的同时又跟真实服务器在同一个网络环境，当然也可能是同一台服务器，端口不同而已。 下面贴上一段简单的实现反向代理的代码 server{ listen80; server_namelocalhost; client_max_body_size1024M; location/{ proxy_passhttp://localhost:8080; proxy_set_headerHost$host:$server_port; } } 保存配置文件后启动Nginx，这样当我们访问localhost的时候，就相当于访问localhost:8080了 第四部分 负载均衡在线上生产环境，为了承载较大流量，通常需要以集群方式并发处理，这就需要有代理服务对流量进行智能负载。通过算法将流量合理的分配给集群中各个处理节点。实现方式有硬件和软件两种，硬件常见的是F5专用设备，成本较高。如果流量不大可以由软件来实现。 而Nginx就是常见的软负载组件。利用upstream定义集群服务器。负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。 Nginx目前支持自带3种负载均衡策略，还有2种常用的第三方策略。 4.1 配置首先需要在http节点中，配置upstream，例如： 1234upstream upstreamTest &#123; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 将server节点下的location节点中的proxy_pass配置为：http:// + upstream名称，即 12345location / &#123; root html; index index.html index.htm; proxy_pass http://upstreamTest; &#125; 4.2 负载模式4.2.1 轮询 （round-robin）（默认方式）轮询为负载均衡中最为朴素的算法，不需要配置额外参数。假设共有N台服务器，算法将遍历服务器节点列表，并按节点次序每轮选择一台服务器处理请求。当所有节点均被调用过一次后，算法将从第一个节点开始重新一轮遍历。如果列表中服务有下宕的，算法能主动将服务器从轮询列表中去除。 这个算法前提需要服务器列表中每台服务器的处理能力是均衡的，否则会有分配不均的问题。 配置案例： 1234upstream upstreamTest &#123; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 4.2.2 加权轮询但后端负载集群性能不均的时候，可以通过加权方式分配流量，这就是加权轮询。例如： 1234upstream upstreamTest &#123; server 192.168.88.1:9200 weight=5; server 192.168.88.2:9200 weight=10; &#125; 上面的配置给每一台服务指定了weight值，weight 的值越大意味着该服务器的性能越好，可以承载更多的请求。也可以从概率角度去理解，192.168.88.2的流量分配概率比192.168.88.1大一倍。 4.2.3 IP 哈希（IP hash）轮询和加权轮询，每次访问后端是随机不同的机器，对于一些场景就不太适应。当程序有状态的时候，例如采用了session保存数据，把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录。所以这时候需要原IP客户端固定访问同一台服务器。 ip hash函数将每个请求按访问ip的hash结果分配，同一个IP客户端访问的负载后端服务不变。配置如下： 12345upstream upstreamTest &#123; ip_hash; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 4.2.4 fair（第三方）对于上面的负载算法没有动态的考虑服务器的性能变化。fair算法根据负载后端服务器的响应时间来动态分配请求，响应时间短优先分配流量。配置参考： 12345upstream upstreamTest &#123; fair; server 192.168.88.1:9200; server 192.168.88.2:9200; &#125; 4.2.5 url_hash（第三方）按訪问url的hash结果来分配请求，使每一个url定向到同一个后端服务器。后端服务器为缓存时有效。静态资源缓存,节约存储，加快速度。配置参考： 123456upstream upstreamTest &#123; server 192.168.88.1:9200; server 192.168.88.2:9200; hash $request_uri; hash_method crc32;&#125; 其中hash_method crc32配置为指定hash算法为crc32。 4.3 补充upstream还能够为每一个设备设置状态值，这些状态值的含义分别例如以下： down 后端节点不参与负载； max_fails和fail_timeout Nginx基于连接探测，如果发现后端异常，在单位周期为fail_timeout设置的时间，中达到max_fails次数，这个周期次数内，如果后端同一个节点不可用，那么接将把节点标记为不可用，并等待下一个周期（同样时常为fail_timeout）再一次去请求，判断是否连接是否成功。如果成功，将恢复之前的轮询方式，如果不可用将在下一个周期(fail_timeout)再试一次。 backup backup 不能和ip_hash一起使用，backup 参数是指当所有非备机都宕机或者不可用的情况下，就只能使用带backup标准的备机。 max_conns 允许最大连接数。 slow_start 当节点恢复，不立即加入 例如下面的案例： 123456upstream upstreamTest &#123; server 192.168.88.1:9200 down; server 192.168.88.2:9200 backup; server 192.168.88.3:9200 max_fails=2 fail_timeout=60s;&#125; 如上配置表明如果后端节点60秒内出现2次不可用情况，判定节点不可用。判定不可用后10秒内请求不会转发到此节点，直到60秒后重新检测节点健康情况。 第五部分 跨域请求前后端分离的项目中由于前后端项目分别部署到不同的服务器上，我们首先遇到的问题就是跨域，在这个场景我们下nginx可以帮助我们很好地解决这个问题 12345678910111213#跨域请求serverserver&#123; listen 9000; server_name 209.250.235.145; root /app/crossDomain/; index index.html; location /douban/ &#123; #添加访问目录为/apis的代理配置 rewrite ^/douban/(.*)$ /$1 break; proxy_pass https://m.douban.com; &#125;&#125;复制代码 在我的服务器下我写了一个 index.html请求豆瓣接口，模拟跨域 123456789101112function nginxClick()&#123; $.ajax(&#123; url: '/douban/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3', dataType: 'json', type: 'get', data: "", success:(res)=&gt;&#123; console.log(res) &#125; &#125;)&#125;复制代码 当我们访问点击请求时，匹配到location下的/douban/ 1rewrite ^/douban/(.*)$ /$1 break;复制代码 这段配置将请求路径重写为/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，其中$1代表正则模糊匹配到的第一个参数， 1proxy_pass https://m.douban.com;复制代码 这段配置是将请求域名代理到豆瓣的域名下面，所以从本地服务器发出去的请求将被重新重写为： https://m.douban.com/rexxar/api/v2/muzzy/columns/10018/items?start=0&amp;count=3，我们就能拿到豆瓣api提供的数据。详情可以看看这篇[文章](https://www.jianshu.com/p/10ecc107b5ee) 演示地址：http://209.250.235.145:9000/ 参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用spark streaming将Kafka汇入Mysql实践]]></title>
    <url>%2F2021%2F04%2F15%2F2021-04-20-%E4%BD%BF%E7%94%A8spark%20streaming%E5%B0%86Kafka%E6%B1%87%E5%85%A5Mysql%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[背景http://www.biancheng666.com/article_147327.html 参考文献及资料]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka中序列化和反序列化总结]]></title>
    <url>%2F2021%2F04%2F15%2F2021-04-15-Kafka%E4%B8%AD%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景https://www.vijaykonnackal.com/protobuf-kafka-message/ https://blog.csdn.net/weixin_26717681/article/details/108499713#t6 https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/ https://codingharbour.com/apache-kafka/how-to-use-protobuf-with-apache-kafka-and-schema-registry/ https://data-flair.training/blogs/kafka-serialization-and-deserialization/ https://blog.csdn.net/weixin_40929150/article/details/88775559 参考文献及资料https://blog.csdn.net/shirukai/article/details/82152172 https://shirukai.github.io/blog/kafka-custom-message-serialization-and-deserialization-mode.html]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orange网关原理的源码分析]]></title>
    <url>%2F2021%2F04%2F15%2F2021-05-29-orange%E7%BD%91%E5%85%B3%E5%8E%9F%E7%90%86%E7%9A%84%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[背景Orange v0.6.0版本是一个重构版本， 着重为了解决之前版本在有大量规则配置时性能损耗的问题。 基本的设计思路是将原来的规则细分成两层， 第一层叫做selector, 用于将流量进行第一步划分， 在进入某个selector后才按照之前的设计进行规则匹配， 匹配到后进行相关处理。 https://lengrongfu.github.io/2019/05/21/orange-%E5%8E%9F%E7%90%86/ https://book.aikaiyuan.com/openresty/understanding-orange.html https://zhuanlan.zhihu.com/p/67481992 网关优化项目https://github.com/starjiang/xorange orange 中设计概念orange 缓存在nginx.conf文件中： 1234567891011lua_code_cache on;lua_shared_dict orange_data 20m; # should not removed. used for orange data, e.g. plugins configurations..lua_shared_dict status 1m; # used for global statistic, see plugin: statlua_shared_dict waf_status 1m; # used for waf statistic, see plugin: waflua_shared_dict monitor 10m; # used for url monitor statistic, see plugin: monitorlua_shared_dict rate_limit 10m; # used for rate limiting count, see plugin: rate_limitinglua_shared_dict property_rate_limiting 10m; # used for rate limiting count, see plugin: rate_limitinglua_shared_dict consul_upstream 5m; # used for consul_upstream, see plugin consul_balancerlua_shared_dict consul_upstream_watch 5m; # used for consul_upstream_watch, consul_balancer 这些配置是插件缓存数据。 如何开发一个插件参考文献及资料[1] Orange官网，链接：http://orange.sumory.com/ [2] Orange网关官网docker，链接：https://hub.docker.com/r/syhily/orange]]></content>
      <categories>
        <category>orange</category>
      </categories>
      <tags>
        <tag>orange</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch中Mapping总结]]></title>
    <url>%2F2021%2F04%2F12%2F2021-04-12-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Elasticsearch%E4%B8%ADMapping%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 版本升级指引 第二部分 升级方法和具体步骤 总结 参考文献及资料 背景第一部分 MappingMapping在Elasticsearch中作用类似于传统数据库中的表结构定义（schema），即Index的元数据信息。主要作用有： 定义索引（Index）中字段名称； 定义字段的数据类型； 定义倒排索引的配置； 在Elasticsearch 7.x版本前，单个index中支持多个type。每个index都会对应有自己的mapping。这时候index可以类比关系型数据库中库，而不同type类比库中表。 在索引中定义太多字段可能会导致索引膨胀，出现内存不足和难以恢复的情况，下面有几个设置： index.mapping.total_fields.limit：一个索引中能定义的字段的最大数量，默认是 1000 index.mapping.depth.limit：字段的最大深度，以内部对象的数量来计算，默认是20 index.mapping.nested_fields.limit：索引中嵌套字段的最大数量，默认是50 1.1 Elasticsearch中数据类型1.1.1 基本类型 text类型 该类型的字段将经过分词器分词，用于全文检索； keyword类型 不经过分词器，用于精确检索（只能检索该字段的完整值），用于过滤（filtering） 数值类型 和编程语言相同，主要有： long：有符号64-bit integer：-2^63 ~ 2^63 - 1 integer：有符号32-bit integer，-2^31 ~ 2^31 - 1 short：有符号16-bit integer，-32768 ~ 32767 byte： 有符号8-bit integer，-128 ~ 127 double：64-bit IEEE 754 浮点数 float：32-bit IEEE 754 浮点数 half_float：16-bit IEEE 754 浮点数 scaled_float 布尔类型 逻辑型，即true/false 日期类型 由于Json没有date类型，所以es通过识别字符串是否符合format定义的格式来判断是否为date类型。ormat默认为：strict_date_optional_time||epoch_millis https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html 二进制类型 该类型的字段把值当做经过 base64 编码的字符串，默认不存储，且不可搜索 范围类型 范围类型表示值是一个范围，而不是一个具体的值 譬如 age 的类型是 integer_range，那么值可以是 {“gte” : 10, “lte” : 20}；搜索 “term” : {“age”: 15} 可以搜索该值；搜索 “range”: {“age”: {“gte”:11, “lte”: 15}} 也可以搜索到 range参数 relation 设置匹配模式 INTERSECTS ：默认的匹配模式，只要搜索值与字段值有交集即可匹配到 WITHIN：字段值需要完全包含在搜索值之内，也就是字段值是搜索值的子集才能匹配 CONTAINS：与WITHIN相反，只搜索字段值包含搜索值的文档 integer_range float_range long_range double_range date_range：64-bit 无符号整数，时间戳（单位：毫秒） ip_range：IPV4 或 IPV6 格式的字符串 1.1.2 复杂类型 数组类型 字符串数组 [ “one”, “two” ] 整数数组 [ 1, 2 ] 数组的数组 [ 1, [ 2, 3 ]]，相当于 [ 1, 2, 3 ] Object对象数组 [ { “name”: “Mary”, “age”: 12 }, { “name”: “John”, “age”: 10 }] 同一个数组只能存同类型的数据，不能混存，譬如 [ 10, “some string” ] 是错误的 数组中的 null 值将被 null_value 属性设置的值代替或者被忽略 空数组 [] 被当做 missing field 处理 对象类型 对象类型可能有内部对象 被索引的形式为：manager.name.first 嵌套类型 地理位置数据类型 专用数据类型 记录IP地址 ip 实现自动补全 completion 记录分词数 token_count 记录字符串hash值 murmur3 Percolator 1.2 多字段特性（multi-fields） 允许对同一个字段采用不同的配置，比如分词，常见例子如对人名实现拼音搜索，只需要在人名中新增一个子字段为 pinyin 即可 通过参数 fields 设置 1.3 不可修改性Mapping中的字段类型一旦设定后，禁止直接修改，原因是Lucence实现的倒排索引生成后不允许修改，除非重新建立新的索引，然后做reindex操作。但是允许新增字段。通过dynamic参数来控制字段的新增： true(默认）允许自动新增字段 false不允许自动新增字段，但是文档可以正常写入，但无法对字段进行查询等操作 strict 文档不能写入，报错 第二部分 设置Mapping在创建一个索引的时候，可以对 dynamic 进行设置，可以设成 false、true 或者 strict。 第三部分 Dynamic MappingDynamic Mapping 机制使我们不需要手动定义 Mapping，ES 会自动根据文档信息来判断字段合适的类型，但是有时候也会推算的不对，比如地理位置信息有可能会判断为 Text，当类型如果设置不对时，会导致一些功能无法正常工作，比如 Range 查询。 3.1 类型自动推断3.2 更新mapping如果是新增加的字段，根据 Dynamic 的设置分为以下三种状况： 当 Dynamic 设置为 true 时，一旦有新增字段的文档写入，Mapping 也同时被更新。 当 Dynamic 设置为 false 时，索引的 Mapping 是不会被更新的，新增字段的数据无法被索引，也就是无法被搜索，但是信息会出现在 _source 中。 当 Dynamic 设置为 strict 时，文档写入会失败。 另外一种是字段已经存在，这种情况下，ES 是不允许修改字段的类型的，因为 ES 是根据 Lucene 实现的倒排索引，一旦生成后就不允许修改，如果希望改变字段类型，必须使用 Reindex API 重建索引。 不能修改的原因是如果修改了字段的数据类型，会导致已被索引的无法被搜索，但是如果是增加新的字段，就不会有这样的影响。 第四部分 Index Template### 参考材料1、Elasticsearch官网 链接：https://www.elastic.co/cn/ https://www.cnblogs.com/wupeixuan/p/12514843.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用深度学习对图片进行动作驱动]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E5%8A%A8%E4%BD%9C%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[背景https://github.com/AliaksandrSiarohin/first-order-model 参考文献及资料1、DeOldify项目地址，链接：https://github.com/jantic/DeOldify]]></content>
      <categories>
        <category>DeOldify</category>
      </categories>
      <tags>
        <tag>DeOldify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用深度学习对图片进行增强]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%9B%BE%E7%89%87%E8%BF%9B%E8%A1%8C%E5%A2%9E%E5%BC%BA%2F</url>
    <content type="text"><![CDATA[背景第一部分 环境部署https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life https://github.com/aiff22/DPED https://www.zhihu.com/question/319291048 https://github.com/CrazyVertigo/awesome-data-augmentation 参考文献及资料1、DPED项目地址，链接：http://people.ee.ethz.ch/~ihnatova/#dataset]]></content>
      <categories>
        <category>DeOldify</category>
      </categories>
      <tags>
        <tag>DeOldify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库的行式存储和列式存储]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-27-%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[背景数据库技术发展迅速，由原来的关系型数据库到越来越丰富的非关系型数据库。如果按照存储形式分类，主要有：行式存储（Row-Based）、列式存储（Column-Based）、键值（key-value）存储、文档（doc）存储、图形（graph）存储、时序数据库等。我们常用的传统关系型数据库（MySQL、Oracle、PostgreSQL 、DB2和 SQL Server）都是采用行式存储，而最新兴起的分布式数据库很多采用列式存储，例如：Druid、Kudu、Clickhouse等。 本文将详细介绍行式存储、列式存储以及业务场景的选型和对比。 第一部分 存储基础数据库按照存储介质，分为磁盘数据库和内存数据库。对于磁盘数据库，需要依赖于内置硬盘或外置集中式存储设备。首先我们介绍磁盘的存储原理。 1.1 磁盘存储首先我们先介绍几个概念： 磁盘扇区（sector） 磁盘中每个磁道等分为若干弧段，这些弧段即称为扇区。磁盘的读写以扇区为基本单位。可以使用fdisk -l命令来查看服务器中磁盘扇区的大小，通常是512 bytes=0.5K。为什么是这么大，这是一个行业标准（近期也有4K的扇区磁盘）。扇区是磁盘的最小存储单元。 扇区是一个物理层面的概念，操作系统是不直接和扇区交互的，而是和磁盘块交互。 磁盘块（IO Block） 操作系统将相邻的扇区组合在一起，形成一个块，对块进行管理，通常称为磁盘块（或磁盘簇）。可以使用stat /boot参看，一般一个磁盘块由2、4、8、16、64等个扇区组成，这是操作系统中的逻辑概念，所以可以调节。通常一个磁盘块为4096 Bytes=4K，即8个连续扇区。 inode 操作系统中文件数据存储在磁盘块中，那么还需要有地方存储文件的元数据信息（文件的创建用户、时间信息、权限等），最重要的是文件数据所在的磁盘块地址信息。这就是inode数据。 文件存储在磁盘中，操作系统有一定规范： 每个磁盘块中只能存储一个文件 例如一个文件大小为5K，操作系统中每个磁盘块大小为4K，那么这个文件实际使用2个磁盘块进行落盘存储。 磁盘块是操作系统最小存储单位 例如例子中的5K文件，存储在两个磁盘块，那么读取这个文件需要进行两次I/O操作。 文件的元数据信息 每个文件的数据存储在磁盘块中，inode进行登记。如果一个磁盘块不够会申请新的磁盘块，均在inode中登记。文件读取时候，顺序读取inode中磁盘块的数据，加载至内存中。 1.2 内存存储同样对于内存，操作系统同样定义了逻辑读取的基本单位为：页（page）。页的大小为磁盘的$2^n$倍数，可以使用命令getconf PAGE_SIZE参看。通常和磁盘块大小一致为4K。 1.3 数据库中的页类似磁盘和内存，数据库中同样页（page）的概念，显然这是一个逻辑的概念。数据库中的数据在磁盘上以文件的形式存储，数据库的存储引擎会以固定大小的page为单位组织文件，读写磁盘也以page为单位。不同数据库page大小有差异，比如：SQLite 1KB, Oracle/DB2 4KB, SQL Server 8KB, MySQL 16KB。 第二部分 行列存储原理磁盘数据库的内部组件组成是复杂的，我们只关注数据文件的存储方式（不同数据库系统会有复杂的处理机制）。 2.1 行式存储数据按照行数据为基础单元进行组织存储。例如下面的二维表数据： 表中每一行数据用|分割，整体存储在一个磁盘块中（图中每条行记录挤满一个磁盘块）； 表中其他记录连续写入文件分配的磁盘块中； 2.1.1 优势 具有随机写入优势，特别是频繁写和更新操作。对于写入的每条记录，只需要内存拼接好整行记录，一次性写入到磁盘块中，单挑记录数量年小于单个磁盘块就只需要1次IO操作。 2.1.2 短板 读取冗余。在查询操作中计算机是按照磁盘块为基本单位进行读取，同一个磁盘块中其他记录也需要读取。然后加载至内存，在内存中进行过滤处理。即使查询只涉及少数字段，也需要读取完整的行记录。 行存储数据会引入索引和分库分表技术来避免全量读取。 数据压缩。每行数据有不同类型数据，数据压缩率较低。 2.2 列式存储数据按照列为单元进行集中存储。例如下面的二维表数据： 数据列SSN的数据顺序存储在同一个磁盘块中； 其他列同样集中顺序存储。顺序写在前列后面或单独一个文件。 2.2.1 优势 大量查询操作优势。对于目标查询，列存只需要返回目标列的值。而且列值在磁盘中集中存储，会减少读取磁盘块的频次，较少IO的数量。最后再内存中高效组装各列的值，最终形成查询结果。另外列式记录每一列数据类型同质，容易解析。每列存储是独立的可以并发处理，进一步提升读取效率。 数据压缩优势。因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率。如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。整体上减轻IO的频次。 映射下推(Project PushDown)、谓词下推(Predicate PushDown)。可以减少不必要的数据扫描，尤其是表结构比较庞大的时候更加明显，由此也能够带来更好的查询性能 2.2.2 短板 写入更新短板。列式存储写入前需要将一条记录拆分成单列，分别追加写入到列所在的磁盘块中，如果列比较多，一条记录就会产生多次磁盘块的IO操作。对于实时的逐条写入性能会比行式弱。 但是对于大量的批写入，列可以在内存中拆分好各分列，然后集中写入，性能和行式数据比不一定差。 第三部分 业务场景适应性分析对于列存和行存，我们在技术选型的时候应该挑选哪个？技术没有包打一切银弹，只有适应对应场景的技术。通常将数据处理分为三大类：联机事务处理OLTP（on-line transaction processing）、联机分析处理OLAP（On-Line Analytical Processing）还有混合事务/分析处理（Hybrid transaction/analytical processing）。 3.1 联机事物处理（OLTP）联机事物处理类型通常表示事务性非常高的系统。一般都是联机在线系统，通常以小的事务和小的查询为主。评估系统性能主要看每秒执行的Transaction以及Execute SQL的数量。单个数据库每秒处理的Transaction往往超过几百个，或者是几千个，查询语句的执行量每秒几千甚至几万个。对于互联网秒杀场景要求会更改。典型的OLTP系统有银行、证券、电子商务系统等互联网在线业务系统等。 业务特点从主要有： 数据库视角： 每个事务的读、写、更改涉及的数据量非常小。 数据库的数据必须是最新的，所以对数据库的可用性要求很高。 系统使用用户较大，有海量访问。 要求业务处理快速响应，通常一个事务需要在秒级内完成。 存储视角： 每个I/O非常小，通常为2KB～8KB。 访问硬盘数据的位置非常随机，至少30％的数据是随机写操作。 REDO日志（重做日志文件）写入非常频繁。 3.2 联机分析处理（OLAP）联机分析处理 (OLAP) 的概念最早是由关系数据库之父E.F.Codd于1993年提出的。也称为决策支持系统，即数据仓库。该场景下语句的执行量不再是考核标准，通常一条语句的执行延迟非常长，读取的数据也是海量的。性能评估的指标变为查询的吞吐量，如能达到多少MB/s的流量。 业务特点主要有： 数据库视角： 数据更新操作少（以大批量写入为主）或没有数据更新和修改。 数据查询过程复杂。 系统用户较少，数据的使用频率逐渐减小，允许延迟。 查询结果以统计、聚合计算为主。 存储视角： 单个I/O数据量大（均为读），通常为MB和GB级别，甚至TB级别。 读取操作通常顺序读取。 当进行读取操作进行时，写操作的数据存放在临时表空间内。 对在线日志写入少。只有在批量加载数据时，写入操作增多。 3.3 混合事务/分析处理(HTAP)HTAP 就是 OLAP 和OLTP 两种场景的结合。在对新旧数据进行 OLAP 分析的情况下增加事务的处理来对数据进行更新。实际业务场景中，往往是 OLAP、OLTP是同时存在的（见下图）。对于HTAP系统，一种解决方案分别针对新旧数据构建两套引擎，一套负责 OLTP（热数据），一套负责 OLAP（冷数据），一个查询到达后，需要分别解析成两套查询，在两个查询引擎都得到结果后进行合并，还可能用到两阶段提交等分布式事务。 例如TiDB数据库就是一个典型的HTAP数据库。目前有两种存储节点，分别是 TiKV和 TiFlash。TiKV 采用了行式存储，更适合 TP（OLTP） 类型的业务；而TiFlash 采用列式存储，擅长 AP(OLAP) 类型的业务。TiFlash 通过 raft 协议从 TiKV 节点实时同步数据，拥有毫秒级别的延迟，以及非常优秀的数据分析性能。它支持实时同步 TiKV 的数据更新，以及支持在线 DDL。把 TiFlash 作为 Raft Learner 融合进 TiDB 的 raft 体系，将两种节点整合在一个数据库集群中，上层统一通过 TiDB 节点查询，使得 TiDB 成为一款真正的 HTAP 数据库。 3.4 选型通过上文的介绍， 联机事物处理（OLTP）、联机分析处理（OLAP）分别适合于行式存储、列式存储数据库。实际选型时，在时间允许的前提下，需要深入了解业务场景的特点，进行场景模拟测试后，根据评测结果最终选择合适的数据库。例如在联机分析处理场景中，如果大量的查询是读取的记录中的大多数或所有字段，主要由单条记录查询和范围扫描组成，则面向行的存储布局的数据库会更适合。 第四部分 总结在过去几年中，由于对不断增长的数据集和运行复杂分析查询的需求不断增长，产生许多新的面向列的文件格式，如Apache Parquet、Apache ORC、RCFile，以及面向列的存储，如Apache Kudu、ClickHouse，以及许多其他列式数据存储组件。 对于列式存储和行式存储存在的短板，在实际数据系统会有很多巧妙设计去弥补。例如行式存储数据库的索引、分库分表、读写分离等功能加入，大大提升了读写性能。而列式存储在面对数据更新短板，数据库设置缓存池（WAL机制解析），积累一定量更新操作后批量提交执行；对于删除操作，采用先标记后删除方式等设计来提升性能。 随着列式数据库的发展，很多传统的行式数据库也加入了列式存储的支持，形成具有两种存储方式的数据库系统。例如，Oracle在12c版本中推出了in memory组件，使得Oracle数据库具有了双模式数据存放方式。还有新兴数据库TiDB也同时支持行列两种存储模式，从而能够实现对混合类型应用的支持。 总之，技术没有包打一切的银弹，只有适合相应场景的技术，要学会tradeoff。 参考文献及资料1、TiDB项目地址，链接：https://github.com/pingcap/tidb 2、《数据库系统实现》，[美] 加西亚·莫利纳 等 著，杨冬青 等 译 3、《深入理解计算机系统》，作者: Randal E.Bryant / David O’Hallaron]]></content>
      <categories>
        <category>Row-Based，Column-Based</category>
      </categories>
      <tags>
        <tag>Row-Based、Column-Based</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一性原理]]></title>
    <url>%2F2021%2F03%2F21%2F2021-04-03-%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的表达能力]]></title>
    <url>%2F2021%2F03%2F21%2F2021-04-03-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E8%BE%BE%E8%83%BD%E5%8A%9B%2F</url>
    <content type="text"><![CDATA[背景人工神经网络（Artificial Neural Network）首次提出要追溯到上个世纪40年代，由于当时缺少有效的算法支撑，学界研究较为低迷。直到1984年，Hinton等人提出了反向传播算法（Backpropagation），才重新点燃了学界的研究热潮，开始训练多层（更深）神经网络。这时候神经网络就改头换面，有了一个更高大上的名字：深度学习（Deep learning）。深度学习的工程应用在工业界异常火热，但是理论基础在学界的研究却是滞后的，所以人送外号：“炼金术”。 神经网络有很多基本问题需要理论解答： 为什么深度学习优于其他模型？泛化性能优于其他模型？ 为什么深度模型优于浅层模型？ 深度学习是否存在学习边界，即表达能力的极限在哪？ 为什么是深度学习，而不是宽度学习？ 数据分布在流形上，为什么深度学习直接使用欧式距离计算通常是有效的？ 上面的开放问题有些已经有接近的答案。 第一部分 Universal Approximation Theorem在人工神经网络领域的数学观点中，「通用近似定理 (Universal approximation theorem，一译万能逼近定理)」指的是：如果一个前馈神经网络具有线性输出层和至少一层隐藏层，只要给予网络足够数量的神经元，便可以实现以足够高精度来逼近任意一个在 \mathbb{R}^nRn 的紧子集 (Compact subset) 上的连续函数。 这一定理表明，只要给予了适当的参数，我们便可以通过简单的神经网络架构去拟合一些现实中非常有趣、复杂的函数。这一拟合能力也是神经网络架构能够完成现实世界中复杂任务的原因。尽管如此，此定理并没有涉及到这些参数的算法可学性 (Algorithmic learnablity)。 通用近似定理用数学语言描述如下： 令 $\varphi$ 为一单调递增、有界的非常数连续函数。记 $m$ 维单元超立方体 (Unit hypercube) $[0,1]^{m}$ 为 $I_{m},$ 并记在 $I_{m}$ 上 的连续函数的值域为 $C\left(I_{m}\right)$ 。则对任意实数 $\epsilon&gt;0$ 与函数 $f \in C\left(I_{m}\right),$ 存在整数 $N_{\text {、常数 }} v_{i}, b_{i} \in \mathbb{R}$ 与向量 $w_{i} \in \mathbb{R}^{m}(i=1, \ldots, n),$ 使得我们可以定义 :$$F(x)=\sum_{i=1}^{N} v_{i} \varphi\left(w_{i}^{T} x+b_{i}\right)$$为 $f$ 的目标拟合实现。在这里， $f$ 与 $\varphi$ 无关，亦即对任意 $x \in I_{m},$ 有：$$|F(x)-f(x)|&lt;\epsilon$$因此, 形为 $F(x)$ 这样的函数在 $C\left(I_{m}\right)$ 里是稠密的。替换上述 $I_{m}$ 为 $\mathbb{R}^{m}$ 的任意紧子集，结论依然成立。 在 1989 年，George Cybenko 最早提出并证明了这一定理在激活函数为 Sigmoid 函数时的特殊情况。那时，这一定理被看作是 Sigmoid 函数的特殊性质。但两年之后，Kurt Hornik 研究发现，造就「通用拟合」这一特性的根源并非 Sigmoid 函数，而是多层前馈神经网络这一架构本身。当然，所用的激活函数仍然必须满足一定的弱条件假设，常数函数便是显然无法实现的。 https://www.jiqizhixin.com/articles/2019-02-21-12 https://zhuanlan.zhihu.com/p/64802339 https://iphysresearch.github.io/blog/post/ml_notes/liweiwang_dl_from_theory_to_algorithm/ https://blog.csdn.net/KeEN_Xwh/article/details/113478160 http://neuralnetworksanddeeplearning.com/chap4.html 第四部分 总结1、数据流形中数据采样充分的时候，模型的训练效果较好 2、深度学习有效的重要假设前提是数据流形的训练数据是充分采样的。 参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto使用介绍]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-31-Presto%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[背景Presto是一个分布式SQL查询引擎， 它被设计为用来专门进行高速、实时的数据分析。它支持标准的ANSI SQL，包括复杂查询、聚合（aggregation）、连接（join）和窗口函数（window functions)。Presto的运行模型和Hive或MapReduce有着本质的区别。Hive将查询翻译成多阶段的MapReduce任务， 一个接着一个地运行。 每一个任务从磁盘上读取输入数据并且将中间结果输出到磁盘上。 然而Presto引擎没有使用MapReduce。它使用了一个定制的查询和执行引擎和响应的操作符来支持SQL的语法。除了改进的调度算法之外， 所有的数据处理都是在内存中进行的。 不同的处理端通过网络组成处理的流水线。 这样会避免不必要的磁盘读写和额外的延迟。 这种流水线式的执行模型会在同一时间运行多个数据处理段， 一旦数据可用的时候就会将数据从一个处理段传入到下一个处理段。 这样的方式会大大的减少各种查询的端到端响应时间。 参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡的总结]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-30-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料]]></content>
      <categories>
        <category>负载均衡</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark研发各类报错汇总]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-29-Spark%E7%A0%94%E5%8F%91%E5%90%84%E7%B1%BB%E6%8A%A5%E9%94%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[背景主要记录日常的爬坑记录，以Spark相关为主。 第一部分 报错关键字：System memory must be at least 1.1 报错背景本地ideal研发环境（windows）运行spark程序调试，报错如下： 1221/03/29 12:28:36 ERROR SparkContext: Error initializing SparkContext. java.lang.IllegalArgumentException: System memory 259522560 must be at least 471859200. Please increase heap size using the --driver-memory option or spark.driver.memory in Spark configuration. 1.2 原因分析从报错内容上看SparkContext没有初始化成功，错误示内存资源不够，需要至少471859200。 1.2.1 spark本地运行机制Spark在本地运行（local）原理是使用线程模拟进程，所以整个集群启动在一个进程中。 1.2.2 源码分析上源码(Spark 3.1.0，org.apache.spark.memory.UnifiedMemoryManager)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445object UnifiedMemoryManager &#123; // Set aside a fixed amount of memory for non-storage, non-execution purposes. // This serves a function similar to `spark.memory.fraction`, but guarantees that we reserve // sufficient memory for the system even for small heaps. E.g. if we have a 1GB JVM, then // the memory used for execution and storage will be (1024 - 300) * 0.6 = 434MB by default. private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024 def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = &#123; val maxMemory = getMaxMemory(conf) new UnifiedMemoryManager( conf, maxHeapMemory = maxMemory, onHeapStorageRegionSize = (maxMemory * conf.get(config.MEMORY_STORAGE_FRACTION)).toLong, numCores = numCores) &#125; /** * Return the total amount of memory shared between execution and storage, in bytes. */ private def getMaxMemory(conf: SparkConf): Long = &#123; val systemMemory = conf.get(TEST_MEMORY) val reservedMemory = conf.getLong(TEST_RESERVED_MEMORY.key, if (conf.contains(IS_TESTING)) 0 else RESERVED_SYSTEM_MEMORY_BYTES) val minSystemMemory = (reservedMemory * 1.5).ceil.toLong if (systemMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s"System memory $systemMemory must " + s"be at least $minSystemMemory. Please increase heap size using the --driver-memory " + s"option or $&#123;config.DRIVER_MEMORY.key&#125; in Spark configuration.") &#125; // SPARK-12759 Check executor memory to fail fast if memory is insufficient if (conf.contains(config.EXECUTOR_MEMORY)) &#123; val executorMemory = conf.getSizeAsBytes(config.EXECUTOR_MEMORY.key) if (executorMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s"Executor memory $executorMemory must be at least " + s"$minSystemMemory. Please increase executor memory using the " + s"--executor-memory option or $&#123;config.EXECUTOR_MEMORY.key&#125; in Spark configuration.") &#125; &#125; val usableMemory = systemMemory - reservedMemory val memoryFraction = conf.get(config.MEMORY_FRACTION) (usableMemory * memoryFraction).toLong &#125;&#125; 主要功能点： systemMemory变量定义了系统内存资源。 123456package org.apache.spark.internal.configval TEST_MEMORY = ConfigBuilder("spark.testing.memory") .version("1.6.0") .longConf .createWithDefault(Runtime.getRuntime.maxMemory) 其中默认值为Runtime.getRuntime.maxMemory，这个值为java虚拟机（JVM）能够从操作系统获取的最大内存资源，如果启动虚拟机时候没有配置-Xmx参数，那么就是256M=256*1024*1024 beytes。 reservedMemory变量为系统保留内存资源。优先使用TEST_RESERVED_MEMORY的值，默认值是个表达式，如果IS_TESTING=True（测试模式）则值为0，否则为：RESERVED_SYSTEM_MEMORY_BYTES=300M。 1234567891011 val TEST_RESERVED_MEMORY = ConfigBuilder("spark.testing.reservedMemory") .version("1.6.0") .longConf .createOptional val IS_TESTING = ConfigBuilder("spark.testing") .version("1.0.1") .booleanConf .createOptional//定义private val RESERVED_SYSTEM_MEMORY_BYTES = 300 * 1024 * 1024 minSystemMemory变量为系统最小内存资源。定义为reservedMemory的1.5倍。 从报错信息看，明显是触发下面的代码逻辑： 12345if (systemMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s"System memory $systemMemory must " + s"be at least $minSystemMemory. Please increase heap size using the --driver-memory " + s"option or $&#123;config.DRIVER_MEMORY.key&#125; in Spark configuration.") &#125; spark应用中未进行相关参数配置，reservedMemory值为300M，那么minSystemMemory值为450M，而应用程序为设置JVM参数，systemMemory默认是256M。显然触发systemMemory &lt; minSystemMemory条件。 可以使用下面的命令查看java环境的默认Xmx的默认值。 12345#In Windows:java -XX:+PrintFlagsFinal -version | findstr /i "HeapSize PermSize ThreadStackSize"#In Linux:java -XX:+PrintFlagsFinal -version | grep -iE 'HeapSize|PermSize|ThreadStackSize' 1.3 修复方法根据上面的源码分析，我们有下面的修复方法。 1.3.1 生产环境生产代码按照源码分析要求jvm虚拟机至少是450M以上的内存。 配置-Xmx参数，使得其远大于450M。例如： 1-Xmx1024m 1.3.2 测试调试 配置spark.testing.memory参数 这时候systemMemory=spark.testing.memory参数的值，例如： 123val sparkConf = new SparkConf() .set("spark.testing.memory","2147480000")//2147480000=2G 开启测试模式（IS_TESTING=True） 12val sparkConf = new SparkConf() .set("spark.testing","true") 这时候minSystemMemory即为0。这时候异常条件不会触发。 指定spark.testing.reservedMemory参数的值（尽可能的小） 12val sparkConf = new SparkConf() .set("spark.testing.reservedMemory","0") 上面的配置下，minSystemMemory的值也为0。 1.4 总结Spark运行对内存资源进行了门槛限制，如果降低这个限制必须要特意显示配置测试相关的指标配置。 第二部分 报错关键字：The maximum recommended task size is 100 KB1.1 报错背景1Stage 0 contains a task of very large size (183239 KB). The maximum recommended task size is 100 KB. 1.2 原因分析此错误消息意味着将一些较大的对象从driver端发送到executors。 spark rpc传输序列化数据是有大小的限制，默认大小是128M（即131072K, 134217728 字节）。所以需要修改spark.rpc.message.maxSize配置的值。 1.3 修复方法在Dirver程序中，修改spark.rpc.message.maxSize 值，例如，增大到1024M： 1--conf spark.rpc.message.maxSize=1024 1.4 总结参考文献及资料https://blog.csdn.net/wangshuminjava/article/details/79792961]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机中存储概念介绍]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-27-%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E5%AD%98%E5%82%A8%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[背景扇区（sector）硬盘的读写以扇区为基本单位。硬盘中每个磁道被等分为若干弧段，这些弧段称为扇区。硬盘的读写以扇区为基本单位。 通常每个扇区的大小是512字节。 123456789101112131415root@deeplearning:~# fdisk -lDisk /dev/loop1: 55.5 MiB, 58159104 bytes, 113592 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/loop2: 55.4 MiB, 58073088 bytes, 113424 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes#（省略）Device Start End Sectors Size Type/dev/sda1 2048 1050623 1048576 512M EFI System/dev/sda2 1050624 909946879 908896256 433.4G Linux filesystem/dev/sda3 909946880 976771071 66824192 31.9G Linux swap 其中显示Sector size就是扇区的大小，案例中为512 bytes（0.5K）。 这是1956年由industry trade organization, International Disk Drive Equipment和Materials Association三家机构确定的行业标准。有时代和技术的限制，因为磁盘技术发展初期，存储容量非常小。512字节的扇区也够用，但是随着时代的发展，512字节大小的扇区（Sector）明显太小了，由于每个扇区（Sector）还要存放很多其他信息，因此增大sector size可以降低扇区（Sector）的数量，从而提高实际存储量，同时降低了差错校验等很多CPU计算量。但遗憾的是由于这个标准太根深蒂固，要想改变一些底层代码或架构势必非常困难，所以现在4KB扇区硬盘暂时还没有全部普及。 关于物理扇区（physical setctor）与逻辑扇区，这个还得扯上扇区大小，由于近年来，随着对硬盘容量的要求不断增加，为了提高数据记录密度，硬盘厂商往往采用增大扇区大小的方法，于是出现了扇区大小为4096字节的硬盘。我们将这样的扇区称之为“物理扇区”。但是这样的大扇区会有兼容性问题，有的系统或软件无法适应。为了解决这个问题，硬盘内部将物理扇区在逻辑上划分为多个扇区片段并将其作为普通的扇区（一般为512字节大小）报告给操作系统及应用软件。这样的扇区片段我们称之为“逻辑扇区”。实际读写时由硬盘内的程序（固件）负责在逻辑扇区与物理扇区之间进行转换，上层程序“感觉”不到物理扇区的存在。 逻辑扇区是硬盘可以接受读写指令的最小操作单元，是操作系统及应用程序可以访问的扇区，多数情况下其大小为512字节。我们通常所说的扇区一般就是指的逻辑扇区。物理扇区是硬盘底层硬件意义上的扇区，是实际执行读写操作的最小单元。是只能由硬盘直接访问的扇区，操作系统及应用程序一般无法直接访问物理扇区。一个物理扇区可以包含一个或多个逻辑扇区（比如多数硬盘的物理扇区包含了8个逻辑扇区）。当要读写某个逻辑扇区时，硬盘底层在实际操作时都会读写逻辑扇区所在的整个物理扇区。 注意，扇区是磁盘物理层面的概念，操作系统是不直接与扇区交互的，而是与多个连续扇区组成的磁盘块交互。由于扇区是物理层面的概念，所以无法在系统中进行大小的更改。 磁盘块，IO Block文件系统读写数据的最小单位，也称磁盘簇。扇区是磁盘最小的物理存储单元，操作系统将相邻的扇区组合在一起，形成一个快，对块进行管理。每个磁盘块可以包括2、4、8、16、64个扇区。磁盘块是操作系统所使用的逻辑概念，而非磁盘的物理概念。磁盘块的大小可以通过下面的命令查看： 123456789root@deeplearning:~# stat /boot File: '/boot' Size: 12288 Blocks: 24 IO Block: 4096 directoryDevice: 802h/2050d Inode: 15859713 Links: 4Access: (0755/drwxr-xr-x) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2021-03-27 12:54:48.241977827 +0800Modify: 2021-01-30 12:38:28.524642113 +0800Change: 2021-01-30 12:38:28.524642113 +0800 Birth: - 其中IO Block: 4096就是磁盘块的大小。案例中是4096 Bytes。也就是4K，连续8个扇区组成。 为了更好地管理磁盘空间和更高效地从硬盘读取数据，操作系统规定一个磁盘块中只能放置一个文件，因此文件所占用的空间，只能是磁盘块的整数倍，那就意味着会出现文件的实际大小，会小于其所占用的磁盘空间的情况。 页（page）页是内存最小存储单位。页的大小通常为磁盘块大小的2^n倍。可以通过命令查看大小。 12root@deeplearning:~# getconf PAGE_SIZE4096 案例中页大小为4096 Bytes,与磁盘块大小一致。 总结 扇区大小，fdisk -l 磁盘块大小，stat /boot 内存页大小，getconf PAGE_SIZE 参考文献及资料1、项目地址，链接：https://github.com/ClickHouse/ClickHouse]]></content>
      <categories>
        <category>Clickhouse</category>
      </categories>
      <tags>
        <tag>Clickhouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Clickhouse学习笔记]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-27-Clickhouse%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、项目地址，链接：https://github.com/ClickHouse/ClickHouse]]></content>
      <categories>
        <category>Clickhouse</category>
      </categories>
      <tags>
        <tag>Clickhouse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink各类资源管理器部署总结]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-Flink%E5%90%84%E7%B1%BB%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E9%83%A8%E7%BD%B2%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景第一部分 Flink集群基础架构Flink集群通常下图所示的组件组成。Flink客户端将Flink应用转换成JobGraph，然后提交给JobManager。JobManager将任务分发给TaskManager运行，当然集群通常还有一些支持组件组成。 1.1 Flink 客户端（Flink Client）本地执行Flink任务的main()方法，解析生成JobGraph对象，最后将JobGraph提交至JobManager，同时监控job认为运行状态。 1.2 JobManager JobManager为Flink集群中管理服务，管理整个集群的计算资源、job任务管理和调度。 1.3 TaskManager集群中具体执行计算任务的服务节点。 1.4 High Availability Service Provider1.5 File Storage and Persistency1.6 Resource Provider1.7 Metrics Storage1.8 Application-level data sources and sinks第二部分 Flink部署模式Flink应用有3种运行模式： Session Mode Per-Job Mode Application Mode 3种模式区别主要是： 第三部分 Flink资源管理器第四部分 Flink Stabdalone模式第五部分 Flink on Yarn模式 第六部分 Flink on K8s模式第七部分 高可用模式原理参考文献及资料1、DeOldify项目地址，链接：https://github.com/jantic/DeOldify]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用DeOldify项目修复老照片]]></title>
    <url>%2F2021%2F03%2F21%2F2021-03-21-%E4%BD%BF%E7%94%A8DeOldify%E9%A1%B9%E7%9B%AE%E4%BF%AE%E5%A4%8D%E8%80%81%E7%85%A7%E7%89%87%2F</url>
    <content type="text"><![CDATA[背景DeOldify是由Jason Antic创建的一个开源深度学习模型，用于为灰度图像添加高质量的色彩效果，效果令人赞叹。简而言之，这种深度学习模型的目标是对旧图像和胶片进行着色，还原并赋予新的生命。 第一部分 部署项目从github上拉取项目（考虑到墙的因素可能需要下载zip包方式）： 1git clone https://github.com/jantic/DeOldify.git 安装依赖包（过程较为缓慢）： 1root@deeplearning:/data/DeOldify# pip install -r requirements.txt 第二部分 准备预训练模型下载预训练模型： 1root@deeplearning:/data/DeOldify# mkdir models 照片类： 1wget https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth 视频类： 1wget https://data.deepai.org/deoldify/ColorizeVideo_gen.pth 注意：以上都是缓慢的过程。 第三部分 实验结果启动jupyter。 1root@deeplearning:/data/DeOldify# jupyter notebook --no-browser --port 8888 --ip=192.168.1.3 --allow-root &amp; 打开ImageColorizer.ipynb文件。运行前需要进行提前配置： 图片附件参数配置 123456source_url= None# 需要修复的老照片地址，如果来自网络使用url地址，如果是本地值为Nonesource_path = 'test_images/lihuanyin.jpg'# 需要修复的老照片路径result_path = 'result_images'# 输出结果路径 预训练模型文件配置 12root@deeplearning:/data/DeOldify/models# lsColorizeArtistic_gen.pth 最后我们运行模型，主要测试结果对比如下： 第一张：李焕英。 第二张:1860年上海豫园。 第四部分 总结效果上看图片还是模糊的，只是涂色。需要对图片进行增强。 参考文献及资料1、DeOldify项目地址，链接：https://github.com/jantic/DeOldify]]></content>
      <categories>
        <category>DeOldify</category>
      </categories>
      <tags>
        <tag>DeOldify</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto使用介绍]]></title>
    <url>%2F2021%2F03%2F21%2F2021-04-01-%E9%93%B6%E8%A1%8C%E7%A7%91%E6%8A%80%E7%9A%84%E8%A1%B0%E8%90%BD%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料]]></content>
      <categories>
        <category>Presto</category>
      </categories>
      <tags>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue学习笔记]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-Vue%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景第一部分 环境准备直接使用webpack创建项目： 12345$ npm install -g vue-cli$ vue init webpack my-project$ cd my-project$ npm install$ npm run dev https://juejin.cn/post/6844903585403109390 参考文献及资料1、RuoYi-Cloud项目文档，链接：http://doc.ruoyi.vip/ruoyi-cloud/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[若依Spring-cloud项目部署介绍]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-%E8%8B%A5%E4%BE%9Dspring-cloud%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景第一部分 基础环境准备1.1 MySql环境本地或远程部署mysql环境，验证服务正常。 1.2 Nacos环境部署Nacos支持单机、集群和多集群部署，对于测试环境我们部署为单机模式。另外单机模式我们使用对接mysql数据库而不是内置的嵌入式数据库存储。主要步骤有： 部署项目文件系统 从github（https://github.com/alibaba/nacos）上下载稳定编译版本包（`nacos-server-$version.zip`），在目标路径解压即可： 1unzip nacos-server-$version.zip 确认mysql数据准备就绪； 创建系统库和表（conf/nacos-mysql.sql 提供初始化语句）； 修改配置文件（conf/application.properties）中mysql链接参数； 1234567891011#*************** Config Module Related Configurations ***************#### If use MySQL as datasource:spring.datasource.platform=mysql### Count of DB:db.num=1### Connect URL of DB:db.url.0=jdbc:mysql://localhost:3306/ry-config?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user=rootdb.password=root 1.3 启动Nacos本地研发环境是windows，使用下面的命令进行启动： 1D:\RuoYi-Cloud\nacos\bin&gt;startup.cmd -m standalone 服务的监听端口为：8848，页面控制台地址为：http://localhost:8848/nacos/#/login，默认用户名和密码为：`nacos/nacos`。 第二部分 开发环境项目部署前往Gitee下载页面(https://gitee.com/y_project/RuoYi-Cloud (opens new window))下载解压到目标目录。使用ideal导入项目。 若依项目模块较多我们启动基础模块： RuoYiGatewayApplication （网关模块 必须） RuoYiAuthApplication （认证模块 必须） RuoYiSystemApplication （系统模块 必须） RuoYiMonitorApplication （监控中心 可选） RuoYiGenApplication （代码生成 可选） RuoYiJobApplication （定时任务 可选） RuoYFileApplication （文件服务 可选） 2.1 数据库初始化项目中有个sql子目录，里面有项目初始化的数据文件： 123quartz.sql # 定时任务表ry_20210108.sql #项目主库ry_config_20201222.sql # nacos服务初始化表 使用sql文件初始化表和库。 2.2 前端启动前端使用Vue编写，项目目录为ruoyi-ui，启动步骤如下： 安装依赖包 12cd ruoyi-uinpm install 启动项目 1npm run dev 回显： 123456App running at:- Local: http://localhost:80/- Network: http://192.168.1.1:80/Note that the development build is not optimized.To create a production build, run npm run build. 前端服务监听端口为：80，页面地址为：http://localhost:80 ，默认账户/密码 admin/admin123，界面如下： 2.3 启动后台服务研发环境分别在项目中启动： RuoYiGatewayApplication （网关模块 必须） RuoYiAuthApplication （认证模块 必须） RuoYiSystemApplication （系统模块 必须） 这是就可以通过前台进行登录： 第三部分 生产部署参考文献及资料1、RuoYi-Cloud项目文档，链接：http://doc.ruoyi.vip/ruoyi-cloud/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（使用SpringBoot部署Hive Restful接口项目）]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E4%BD%BF%E7%94%A8SpringBoot%E9%83%A8%E7%BD%B2Hive%20Restful%E6%8E%A5%E5%8F%A3%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景参考文献及资料1、Spring官网，链接： http://blog.hming.org/2018/11/19/spring-boot-shi-yong-jdbc-lian-jie-hive/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[若依spring-cloud项目前端模块（Vue）介绍]]></title>
    <url>%2F2021%2F03%2F20%2F2021-03-20-%E8%8B%A5%E4%BE%9Dspring-cloud%E9%A1%B9%E7%9B%AE%E5%89%8D%E7%AB%AF%E6%A8%A1%E5%9D%97%EF%BC%88Vue%EF%BC%89%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基础环境准备 第二部分 开发环境项目部署 第三部分 生产部署 第四部分 总结 参考文献及资料 背景http://doc.ruoyi.vip/ruoyi-cloud/document/qdsc.html 第一部分 项目目录结构简介1234567891011121314151617181920212223242526272829303132ruoyi-ui├── babel.config.js├── bin│ ├── build.bat│ ├── package.bat│ └── run-web.bat├── build│ └── index.js├── node_modules├── package-lock.json├── package.json├── public│ ├── favicon.ico│ ├── index.html│ └── robots.txt├── README.md├── src│ ├── api│ ├── App.vue│ ├── assets│ ├── components│ ├── directive│ ├── layout│ ├── main.js│ ├── permission.js│ ├── router│ ├── settings.js│ ├── store│ ├── utils│ └── views├── tree.md└── vue.config.js 参考文献及资料1、RuoYi-Cloud项目文档，链接：http://doc.ruoyi.vip/ruoyi-cloud/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Flink学习笔记]]></title>
    <url>%2F2021%2F03%2F18%2F2021-03-18-Flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[背景第一部分 Flink入门第二部分 Flink应用第三部分 核心抽象第四部分 时间和窗口第五部分 类型和序列化第六部分 内存管理第七部分 状态原理第八部分 作业提交第九部分 资源管理第十部分 作业调度第十一部分 作业执行第十二部分 数据交换第十三部分 应用容错第十四部分 Flink SQL第十五部分 运维监控第十六部分 RPC框架参考文献及资料]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币的数学原理]]></title>
    <url>%2F2021%2F02%2F28%2F2021-02-17-%E7%BB%8F%E6%B5%8E%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、]]></content>
      <categories>
        <category>bitcoin</category>
      </categories>
      <tags>
        <tag>bitcoin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-数据科学中的几何]]></title>
    <url>%2F2021%2F02%2F15%2F2021-03-10-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%87%A0%E4%BD%95%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[背景 数据科学赋予了几何才有了灵魂。 在正文前，我们先介绍一个机器学习案例。 案例提供二维数据集，数据有：(116.331398,39.897445)、(121.545451,31.178669）、(120.184051,30.320602)、(117.038113,30.496019)等，然后要求对数据进行聚类。 对于这个案例通常做法就是计算数据之间的距离（欧式距离），使用各类聚类算法寻找质心。这样就默认给数据集赋予了二维欧式距离，即这些点是取自二维欧式空间的。这很有可能已经破坏了数据集中所蕴含的信息。事实上，这些数据是地球上经纬度坐标数据，它有天然的几何结构：球面几何。在几何学中，它属于非欧几何范畴。案例数据属于球面空间的子空间，自然应该使用球面距离(Haversine公式)。 我们在处理数据集的时候不能先入为主的认为是欧式空间的子集。如果直接使用欧式度量，这是一个很强的假设前提（先验）。 很多数据处理算法均是默认欧式距离。例如神经网络模型中，假定数据为欧式空间，学习的函数空间就是欧式空间之间的非线性函数。数据特征工程中将各种无关属性的数据拼接成高维特征均是欧式空间。在数学角度看都是很强的假设前提，甚至已经破坏了数据的内在关系，从信息论角度已经丢失信息或者引进的噪声信息。 但是面对高维数据，低维生命无法肉眼看清复杂的结构关系（或目前还没有有效的刻画技术）。但是可以尝试减少先验，给数据集合赋予一个比欧式空间较弱的几何结构：拓扑流形。特别是对于非线性数据，赋予流形结构提供的视角和高度是欧式距离无法给予的。 第一部分 数据微分流形对于给定的多维数据集，数学上首先它是一个集合$S$，这时候数据和数据是没有关系的（结构）。接下来我们给数据集定义几何结构： 赋予拓扑结构（数据集是一个拓扑空间）：数据集合$S$ 的子集构成的新的集合 $\tau$, 称为数据集$S$上的拓扑 (topology)，需要满足以下条件： (1) $\emptyset \in \tau$,$S \in \tau$; (2) $\tau$ 中有限个集合的交集和任意（有限、可数、不可数）个集合的并集还属于$\tau$。简单来说就是$\tau$ 中的集合对于有限交和任意并运算封闭。 赋予流形结构：数据集$S$上可以赋予很多拓扑结构，并不是唯一的。我们进一步增加约束条件：令$S$是一个Hausdorff空间。若对任一点 $p\in S $ , 都存在 $x$ 在$S$中的一个邻域 $U\subset S$ ,使得 $U$ 同胚于$n$ 维欧氏空间$R^n$ , 则$S$就称为是一个$n$维流形。 赋予微分流形结构：为了能在数据流形上有微分运算（类似欧式空间），我们需要数据流形是可微的：一个$n$维微分流形$S$就是一个赋予了微分结构的$n$维流形，即存在一组坐标卡 $\mathcal{A}=\left{\left(U_{i}, \varphi_{i}\right)\right}$（即流形定义中的局部同胚映射），使得：（i）$\mathcal{A}$ 为$S$ 的一个开覆盖；（ii）任意两个不相交的坐标卡；（iii）$(U, \varphi)$ 和 $(V, \psi)$ 满足$ \psi \circ \varphi^{-1}$$与$ $\varphi \circ \psi^{-1}$皆为 $ C^{\infty}$函数为$\mathcal{A}$ 极大的（该条件可由前两条唯一生成）。 最后数据集集合具有微分流形几何结构，而经典数据科学中常用的欧式空间是微分流形的特例，我们弱化了假设前提。但是微分流形仍具有较好的性质： 微分流形局部同胚与欧式空间（每个点都存在一个邻域同胚于欧氏空间中的开集）； 微分流形可以嵌入至欧式空间； 其中第二个性质是微分拓扑中重要定理：Whitney嵌入定理。 Whitney嵌入定理：设$M$是$m$维微分（光滑）流形，存在$M$到欧氏空间$R^n$的光滑嵌入映射$f$，其中$n&gt;=2m+1$。并且$f$的像是$R^n$中的闭集。 Whitney嵌入定理是微分拓扑中重要的定理，也许可以认为正是Whitney发现了这个定理，开创了微分拓扑。在这个定理之前，人们对于流形还是把握不定的。但是在这个定理后，由于流形可以嵌入到维数较大的欧氏空间中，所以有了一系列的关于流形的重要结果，形成了微分拓扑这个分支。 第二部分 流行学习有了Whitney嵌入定理的理论保证，高维数据集可以假设是嵌入在高维欧式空间中低维流形。基于这个假设就有了一个研究分支：流形学习（manifold learning）。 对于地球位置的数据集，在3维欧式空间可以用3维数据坐标$(x,y,z)$ 来表示，实际上又可以用2维经纬度坐标（内蕴坐标）表示，地球位置数据实际是一个二维球面子流形嵌入在3维欧式空间中。当数据集是3维欧式空间表示时，我们可以降维到二维流形。 上面的分析前提是我们开启了上帝视角。实际应用中，对于任意给定的数据集，很难判断或假设合理的内蕴流形结构。 2.1 数据降维数据科学中对于线性空间中数据，有较多成熟的降维技术：例如主成分分析（PCA）、独立分量分析（ICA）、Fish判别法（FDA）、多尺度分析（MDS）等。那么在降维的时候，寻找恰当的降维映射是算法的目的。降维映射空间同样是庞大的，需要定义相关的性能度量来度量映射的优劣。 然而对于非线性数据流形，空间上是没有距离（欧氏距离）概念的。这时候我们需要利用微分流形另一个重要特性：微分流形局部同胚与欧式空间。 宇宙是一个微分流形，我们人类所处的局部是一个三维欧式空间。 2.1.1 保持局部线性关系映射需要保持流形上每个很小的局部欧式线性。代表算法有：局部改线嵌入（local Linear Embedding，LLE）。算法大体分成三个部分：寻找近邻、线性重构、低维嵌入。 寻找近邻 对于每个数据点，需要定义邻域。邻域有两种方式：(1)最近的K个点;(2)邻域距离阀值限定局部大小。LLE算法使用前者，其中K是一个预先给定值（超参数），使用欧式距离度量。 线性重构 每个数据点由K个近邻线性表达； 低维嵌入 每个点在低维空间中的降维映射值在项空间同样有近似的线性表达。 具体算法过程，参考论文：《Nonlinear Dimensionality Reduction by Locally Linear Embedding》。LEE算法对于闭合流形、稀疏数据集效果有限，另外结果对于K值选取较为敏感。 2.1.2 保持点之间测地线距离微分流形中整体是没有距离概念的，但是有推广的测地线距离（geodesic）。代表算法有Isomap(Isometric Feature Mapping)算法。算法步骤大体分成三个部分：构造近邻图、计算最短路径、低维嵌入。 构造近邻图 和LLE算法相同，基于欧式距离找出邻近点，建立整体集合的近邻连接图（图论理论中图概念）。 计算最短路径 近邻连接图中近邻点之间存在连接关系，而非近邻点之间不存在连接关系。这样计算两点之间测地线距离的问题就转变为计算近邻连接图上两点之间的最短路径问题。最短路径上逐点距离和近似代替几何意义上的测地线。最短路径计算，采用图论中Dikstra算法或Floyd算法。 低维嵌入 在得到任意两点的距离之后，结果就可以直接应用 MDS(Multidimensional Scaling) 算法了。 上图瑞士卷数据集是常用的验证数据集。A图中蓝色线表示流形上两点之间的实际测地线距离，B图中红色是Dijkstra算法在近邻图上找到的最短距离，C图显示了降维后三维数据集在二维平面中的嵌入效果，结果较为近似。但是当流形的曲率较大、数据稀疏时或者流形上有孔洞的话，算法效果较差。而对于空间中的数据点稠密时，近似效果较好。 2.1.3 保持图的局部邻接关系Isomap 算法使用点与点之间欧式距离构建了整体数据集合的近邻图。在这个图中，点与点之间关系为欧式距离。而这个关系可以使用高斯核(Gaussian Kernel)来定义。 对于连通的两个点 $i,i′$,令图关系值为:$w_{ii′}=k(i,i’)=exp(−∥i−i′∥2/σ2)$ 。而其它点关系值为: 0。这样所有点之间的关系值可以得到图的邻接矩阵W，第i行的第i&#39;个值对应权重：$w_{ii’}$。 如果记在降维映射后的值为$j,j’$ ,那么就是在假设空间中寻找函数使得下面的性能度量最小：$$\sum_{jj’}(j-j’)^2 w_{jj’}=\sum_{jj’}\left(j^{2}+j’^{2}-2j j’\right) W_{j j’}=2 \mathbf{y}^{T} L \mathbf{y}$$其中$$D_{jj’}=\sum_{j’} W_{j j’}, L=D-W$$ ，L为Laplacian矩阵。 这个目标函数形式是不是和PCA十分的相似。目标函数的求解是一个广义特征值分解问题：$$L \mathbf{y}=\lambda D \mathbf{y}$$计算L的特征值，将特征值从小到大排序，取前k个特征值，并计算前k个特征值$\lambda_{0}, \lambda_{2}, \ldots, \lambda_{k-1}$。这样就得到了键数据映射到特征空间中。 这个算法过程和谱聚类(Spectral Clustering)算法技术是相同的。 这就是Laplacian Eigenmaps算法，具体算法过程，参考论文：《Laplacian Eigenmaps for Dimensionality Reduction and Data Representation》。LEE算法对于闭合流形、稀疏数据集效果有限，另外结果对于K值选取较为敏感。 LE算法是非线性的，另外还有推广的线性版本：保持保持投影LPP（Locality Preserving Projections）算法，参考论文：《Locality Preserving Projections》。 2.1.4 保持概率分布对于数据流形中，Isomap 算法使用近似测地线来度量近邻点之间的距离（相似性）。2002年Hinton提出一个奇妙的思路，使用概率密度来度量这个相似性。这就是SNE（Stochastic Neighbor Embedding）算法。 对于数据流形$X$中两个数据点$x_i$和$x_j$，考虑$x_i$为中心的高斯分布（$\sigma_i$为分布的方差），定义一个条件概率$p_{ij}$（数据点$x_i$选择$x_j$为近邻点的概率）：$$p_{i j}=\frac{\exp \left(-d_{i j}^{2}\right)}{\sum_{k \neq i} \exp \left(-d_{i k}^{2}\right)},其中d_{i j}^{2}=\frac{\left|\mathbf{x}{i}-\mathbf{x}{j}\right|^{2}}{2 \sigma_{i}^{2}},(局部欧式使用欧式度量)$$当映射将流形数据映射到低维空间$Y$后，需要保持数据点的相似性。记在低维空间中数据点$x_i$和$x_j$的值分别为$y_i$和$y_j$，同样赋予条件概率度量，特别的将高斯分布的方差固定为$1/\sqrt{2}$。$$q_{i j}=\frac{\exp \left(-\left|\mathbf{y}{i}-\mathbf{y}{j}\right|^{2}\right)}{\sum_{k \neq i} \exp \left(-\left|\mathbf{y}{i}-\mathbf{y}{k}\right|^{2}\right)}$$我们定义了数据流形中点$x_i$和值空间中点$y_i$条件概率。如果遍历所有点，就定义了一个关于$x_i$和$y_i$的概率分布：$$P_i={p_{ij}(x_j)}_{j\in X},Q_i={q_{ij}(y_j)}{j\in Y}$$映射保持概率分布，这时候概率分布的度量KL（Kullback-Leibler Divergence）散度就用上了,最后得到假设空间的性能度量指标：$$C=\sum{i} K L\left(P_{i} | Q_{i}\right)=\sum_{i} \sum_{i} p_{j \mid i} \log \frac{p_{j \mid i}}{q_{j \mid i}}$$最后算法根据这个目标函数寻找最优解。 我们知道KL散度是非对称的，SNE算法更关注于局部，容易忽视全局结构。后续Hinton将低维空间的分布由高斯分布换成t分布（利用t分布的长尾性特征），就是降维可视化中最常用的算法t-SNE算法。 下图为mnist在t-SNE算法下降维至2维空间的可视化效果。 .png) 对于t-SNE算法，实际工程实现时需要计算大量的概率值，计算量较大。实际上，对于相距较远的点成为邻域点的概率会很小，所以只需要考虑一定范围邻域的点集即可，提前构建近邻图。 2.2 可视化数据降维的一个衍生功能就是可视化化。将数据映射到二维或者三维欧式空间中（映射保持指定的信息结构），便于三维生物体更容易感知到数据之间的结构关系。 但是原始数据蕴含的流形并不一定是恰好的三维或二维，强行降维会破坏数据的真实结构信息。 2.3 半监督学习在实际数据处理中，标签数据是珍贵而稀缺的。更多的应用场景是：有少量已经标记的标签数据和大量的无标签数据。在流形学习假设前提下，相似数据应该在流形的同一个局部邻域中（或者说局部领域中数据标签相似）。基于这个假设，大量的非标签数据就可以通过少量标签获取到标签。 2.4 工程实现对于工程实现，Python中scikit-learn包提供相关流形学习的算法实现包。主要有： 封装类名 对应算法 manifold.Isomap() Isomap Embedding manifold.LocallyLinearEmbedding() Locally Linear Embedding manifold.SpectralEmbedding() Spectral embedding for non-linear dimensionality reduction. manifold.TSNE) t-distributed Stochastic Neighbor Embedding. 第三部分 总结3.1 谱图理论Isomap算法和LE算法在构建数据近邻图的过程使用的思想其实是谱图理论(spectral graph theory)。其本质是利用数据相似度的关系矩阵，然后计算矩阵的特征值和特征向量，最后选择合适的特征向量投影得到的数据的低维嵌入。图谱理论的技术要求数据流形的数据采样是稠密的、流形曲率不能太大，否则对噪声是敏感的。 图谱理论的技术使得数据流形学习由理论落地为实践。近几年在图神经网络方向仍然是重要技术。 3.2 流形学习中问题3.2.1 基本问题1对于给定的多维数据集，研究和刻画数据微分流形的几何和拓扑性质。寻找一些基本判别定理确定数据内蕴微分流形结构。 3.2.2 基本问题2在对数据流形处理过程中，如何刻画数据微分流形中信息量，如何建立数据中信息量和几何结构的关系。数据微分流形在哪一类函数变换下能保持信息量的不变。 3.3 解释性对于一些常用的数据集，很多研究结果表明具有流形结构，例如： 1、MNIST数据是一个6维流形； 2、从不同角度拍摄得到的一系列图片数据集实际上是分布在一个低维流形中； 通常机器学习中，在数据上构建模型的时候，人们更多的是关注于模型的泛化性能，强调模型是端到端的，就是我们经常在英文文献中看到的概念：end-to-end learning 。将大量时间花费在缺乏理论指导的实验性调参上（自嘲为”炼丹师”）。例如深度学习应用中，更多是优化网络层结构和参数，忽略背后数据特征的变化原理。当然也引起了人们的怀疑态度，即模型的可解释性危机。 数据科学中很多问题本质是几何问题，解释性危机极大可能由几何理论来最终解答。例如国内顾险峰团队使用最优传输理论（Optimal Mass Transportation ），凸几何（Convex Geometry），蒙日-安培方程（Monge-Ampere Equation）的交汇给出了生成模型GAN（Generative Adversarial Networks）的几何观点。 3.4 交叉学科除了流形学习，近些年数据科学和几何学出现很多交叉方向，例如信息几何学、计算共性几何。 信息几何学 概率是信息学和数据科学中重要概念。信息几何学将概率密度函数集合看成微分流形，把Fisher信息作为黎曼度量，使用测度距离作为概率密度函数之间的距离，获得丰富结果。 计算共形几何 计算共形几何将现代几何拓扑理论与计算机科学相融合，将经典微分几何、Riemann面理论、代数拓扑、几何偏微分方程的概念、定理和方法推广至离散情形，转换成计算机算法，广泛应用于计算机图形学、计算机视觉、计算机辅助几何设计、数字几何处理、计算机网络、计算力学、机械设计以及医学影像等领域中。 由于笔者水平有限，不能保证全部内容均正确无误，若读者有不同意见，非常欢迎留言指教一起探讨。 参考文献1、《微分拓扑新讲》，张筑生，北京大学出版社； 2、《黎曼几何引论》，陈维恒 李兴校，北京大学出版社； 3、Nonlinear Dimensionality Reduction by Locally Linear Embedding，Sam T. Roweis和Lawrence K. Saul，《Science》2000 4、Laplacian Eigenmaps for Dimensionality Reduction and Data Representation，M.Belkin，NIPS2002论文 5、Locality Preserving Projections，何晓飞，NIPS (2003) 6、深度学习的几何学解释，链接：http://www.engineering.org.cn/ch/10.1016/j.eng.2019.09.010]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-数据科学中度量距离总结]]></title>
    <url>%2F2021%2F02%2F15%2F2021-02-17-%E7%BB%8F%E6%B5%8E%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-2021%E5%B9%B4%E7%BB%8F%E6%B5%8E%E8%B6%8B%E5%8A%BF%E4%B8%AA%E4%BA%BA%E8%A7%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[背景参考文献及资料1、]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-数据科学中度量距离总结]]></title>
    <url>%2F2021%2F02%2F15%2F2021-02-15-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E4%B8%AD%E5%BA%A6%E9%87%8F%E8%B7%9D%E7%A6%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景在数据科学中，需要对数据空间中点度量差异（相似度），就需要对数据集合赋予恰当合适的测度（度量），用来刻画点与点之间的差异（距离）。通常这种度量种类很多，选取时需要针对集合特点进行选取。 https://www.cnblogs.com/ranjiewen/p/5958803.html https://www.cnblogs.com/wt869054461/p/7156397.html 第一部分 前言机器学习中有监督和无监督算法都会用到距离度量。例如常用算法k-NN，UMAP，HDBSCAN中经常使用的度量：欧氏度量和余弦相似性度量等。其中欧式距离是我们使用最多的距离，但是它是否适应所有的数据集合呢？答案是否定的，欧式距离不是银弹。 特别的当我们数据集合中数据本身具有遗传度量，例如地球上物体经纬度坐标，这个本身就有自己的球面距离，如果使用欧式距离就破坏了数据天然的度量。 第一部分 欧氏距离我们从最常见的距离度量开始，即欧几里得距离。最好将距离量度解释为连接两个点的线段的长度。该公式非常简单，因为使用勾股定理从这些点的笛卡尔坐标计算距离。 1.1 缺点尽管这是一种常用的距离度量，但欧几里得距离并不是比例不变的，这意味着所计算的距离可能会根据要素的单位而发生偏斜。通常，在使用此距离度量之前，需要对数据进行标准化。 此外，随着数据维数的增加，欧氏距离的用处也就越小。这与维数的诅咒有关，维数的诅咒与高维空间不能像直觉上期望的二维或3维空间那样起作用。有关摘要，请参阅这篇文章。 1.2 用例当您拥有低维数据并且向量的大小非常重要时，欧几里得距离的效果非常好。如果在低维数据上使用欧几里得距离，则kNN和HDBSCAN之类的方法将显示出出色的结果。 尽管已开发出许多其他措施来解决欧几里得距离的缺点，但出于充分的原因，它仍然是最常用的距离措施之一。它使用起来非常直观，易于实现，并且在许多用例中都显示出了极好的效果。 第二部分 余弦相似度余弦相似度经常被用作解决高维数欧几里德距离问题的方法。余弦相似度只是两个向量之间角度的余弦。如果将向量归一化为长度均为1，则向量的内积也相同。 两个方向完全相同的向量的余弦相似度为1，而两个彼此相对的向量的相似度为-1。注意，它们的大小并不重要，因为这是方向的度量。 余弦相似度 缺点余弦相似度的主要缺点之一是不考虑向量的大小，仅考虑其方向。实际上，这意味着没有充分考虑值的差异。例如，如果采用推荐系统，则余弦相似度不会考虑不同用户之间的评分等级差异。 用例当我们拥有高维数据并且向量的大小不重要时，通常会使用余弦相似度。对于文本分析，当数据由字数表示时，此度量非常常用。例如，当一个单词在一个文档中比另一个单词更频繁出现时，这并不一定意味着一个文档与该单词更相关。可能是文件长度不均匀，计数的重要性不太重要。然后，我们最好使用忽略幅度的余弦相似度。 3.海明距离海明距离。图片由作者提供。 汉明距离是两个向量之间不同的值的数量。它通常用于比较两个等长的二进制字符串。它也可用于字符串，以通过计算彼此不同的字符数来比较它们彼此之间的相似程度。 缺点如您所料，当两个向量的长度不相等时，很难使用汉明距离。您可能希望将相同长度的向量相互比较，以了解哪些位置不匹配。 此外，只要它们不同或相等，就不会考虑实际值。因此，当幅度是重要指标时，建议不要使用此距离指标。 用例典型的用例包括通过计算机网络传输数据时的纠错/检测。它可以用来确定二进制字中失真比特的数量，作为估计错误的一种方法。 此外，您还可以使用汉明距离来测量分类变量之间的距离。 4.曼哈顿距离 曼哈顿距离。图片由作者提供。 曼哈顿距离（通常称为出租车距离或城市街区距离）可计算实值向量之间的距离。想象一下在统一的网格上描述对象的矢量，例如棋盘。然后，曼哈顿距离是指两个向量只能以直角移动时的距离。计算距离时不涉及对角线运动。 曼哈顿距离 缺点尽管曼哈顿距离对于高维数据似乎还可以，但它比欧几里德距离直观性差一些，特别是在使用高维数据时。 此外，由于它不是最短的路径，因此比欧几里得距离更有可能提供更高的距离值。这不一定会带来问题，但是您应该考虑这一点。 用例当您的数据集具有离散和/或二进制属性时，Manhattan似乎工作得很好，因为它考虑了可以在这些属性的值内实际采用的路径。以欧几里得距离为例，实际上可能不可能在两个向量之间创建一条直线。 5.切比雪夫距离 切比雪夫距离。图片由作者提供。 切比雪夫距离定义为沿着任何坐标维度的两个向量之间的最大差。换句话说，它只是一个轴上的最大距离。由于其性质，通常将其称为棋盘距离，因为国王从一个正方形到另一个正方形所需的最小移动次数等于切比雪夫距离。 切比雪夫距离 缺点切比雪夫（Chebyshev）通常用在非常特定的用例中，这使得很难将其用作通用距离度量标准，例如欧几里得距离或余弦相似度。因此，建议仅在绝对确定它适合您的用例时才使用它。 用例如前所述，切比雪夫距离可用于提取从一个正方形移动到另一个正方形所需的最小移动次数。此外，在允许无限制八向移动的游戏中，这可能是有用的措施。 实际上，切比雪夫距离通常用于仓库物流中，因为它与高架起重机移动物体所需的时间非常相似。 6.闵可夫斯基 Minkowski距离。图片由作者提供。 Minkowski距离比大多数距离更复杂。它是在范数向量空间（n维实空间）中使用的度量，这意味着它可以在距离可以表示为具有长度的向量的空间中使用。 该措施具有三个要求： 零向量—零向量的长度为零，而每个其他向量的长度为正。例如，如果我们从一个地方到另一个地方旅行，那么该距离始终为正。但是，如果我们从一个地方到自己的地方旅行，则该距离为零。 标量因数—当向量与正数相乘时，其长度会更改，同时保持其方向。例如，如果我们在一个方向上走了一定距离并添加了相同的距离，则方向不会改变。 三角形不等式—两点之间的最短距离是一条直线。 Minkowski距离的公式如下所示： 明可夫斯基距离 关于此距离度量最有趣的是使用parameter **p**。我们可以使用此参数来操纵距离度量以使其与其他度量非常相似。 的常见值p是： p = 1 —曼哈顿距离 p = 2 —欧几里德距离 p = ∞-切比雪夫距离 缺点Minkowski与它们所代表的距离度量具有相同的缺点，因此对曼哈顿，欧几里得和契比雪夫距离等度量的良好理解非常重要。 而且，p根据您的用例，使用合适的参数可能会很麻烦，因为找到正确的值可能在计算上效率低下。 用例好处p是可以对其进行迭代并找到最适合您的用例的距离度量。它为您的距离度量提供了极大的灵活性，如果您熟悉p并且有很多距离度量，那么这将是一个巨大的好处。 7.贾卡德指数 Jaccard索引。图片由作者提供。 Jaccard索引（或“联合上的交集”）是用于计算样本集的相似性和多样性的度量。它是交集的大小除以样本集并集的大小。 实际上，它是集合之间相似实体的总数除以实体的总数。例如，如果两个集合共有1个实体，并且共有5个不同的实体，则Jaccard索引将为1/5 = 0.2。 要计算Jaccard距离，我们只需从1中减去Jaccard索引： 提卡距离 缺点Jaccard索引的主要缺点是它受到数据大小的很大影响。大型数据集可能会对索引产生很大影响，因为它可以显着增加联合并同时保持相交相似。 用例Jaccard索引通常用于使用二进制或二进制数据的应用程序中。当您拥有一个预测图像片段（例如汽车）的深度学习模型时，可以使用Jaccard索引来计算给定真实标签的预测片段的准确性。 同样，它可用于文本相似性分析中，以衡量文档之间单词选择重叠的程度。因此，它可以用来比较模式集。 8.哈弗碱 Haversine距离。图片由作者提供。 Haversine距离是指球面上两个点之间的经度和纬度。它与欧几里得距离非常相似，因为它可以计算两点之间的最短线。主要区别在于不可能有直线，因为这里的假设是两个点都在一个球面上。 两点之间的正弦距离 缺点这种距离测量的一个缺点是假定这些点位于一个球体上。实际上，这种情况很少出现，例如，地球不是完美的圆形，在某些情况下可能会使计算变得困难。取而代之的是，将目光转向假定椭圆形的Vincenty距离。 用例如您所料，Haversine距离通常用于导航。例如，您可以使用它来计算两个国家之间的飞行距离。请注意，如果距离本身不那么大，则不太适合。曲率不会产生太大的影响。 9.索伦森-骰子指数 Sørensen-骰子系数。图片由作者提供。 Sørensen-Dice索引与Jaccard索引非常相似，因为它可以测量样本集的相似性和多样性。尽管它们的计算方式相似，但索伦森-迪斯指数却更直观一些，因为可以将其视为两组之间重叠的百分比，该值介于0和1之间： 索伦森–骰子系数 缺点像Jaccard索引一样，它们都夸大了几乎没有或没有地面真理肯定集的集合的重要性。结果，它可以控制多组平均得分。它会按相关集合的大小成反比地加权每个项目，而不是平等对待它们。 用例用例与Jaccard索引相似（如果不同）。您会发现它通常用于图像分割任务或文本相似性分析中。 注意：比这里提到的9种距离测量更多。如果您正在寻找更有趣的指标，我建议您研究以下内容之一：马哈拉诺比斯（Mahalanobis），堪培拉（Canberra），Braycurtis和KL分歧。 切比雪夫距离闵可夫斯基距离(Minkowski Distance)标准化欧氏距离 (Standardized Euclidean distance )马氏距离(Mahalanobis Distance)巴氏距离（Bhattacharyya Distance）皮尔逊系数(Pearson Correlation Coefficient)参考文献及资料1、9 Distance Measures in Data Science，链接：https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa]]></content>
      <categories>
        <category>Distance Measures</category>
      </categories>
      <tags>
        <tag>Data Science</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat介绍]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-02-Tomcat%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 # 第一部分 Tomcat的基础知识1.1 Web 概念1.1.1 软件架构Web应用有两种架构： C/S架构；QQ软件 B/S架构；各类网站 1.1.2 资源分类 静态资源； 所有用户访问后，返回的结果相同，称为静态资源。静态资源可以直接被浏览器解析。例如：html、css、js、图片文件等。 动态资源； 用户访问资源后，返回的结果不同，称为动态资源。动态资源被访问后，需要先转换成静态资源，然后再返回给浏览器，通过浏览器进行最后解析。例如：servlet/jsp、php、asp等 1.1.3 网络通讯 ip地址；ip地址作为用户在互联网上唯一标识。 端口；端口是计算机中应用进程的唯一标识。 通信协议；定义了数据传输的规则，例如：tcp/udp、http。 1.2 常见的Web服务器1.2.1 概念 服务器；安装服务器软件的计算机节点。 服务器软件；接受用户的请求，处理请求和应答请求。在Web服务软件中，可以部署Web项目，用户可以通过浏览器进行访问项目。 1.2.2 常见的Web服务器软件 BEA WebLogic 是用于开发、集成、部署和管理大型分布式Web应用、网络应用和数据库应用的Java应用服务器。将Java的动态功能和Java Enterprise标准的安全性引入大型网络应用的开发、集成、部署和管理之中。 BEA WebLogic Server拥有处理关键Web应用系统问题所需的性能、可扩展性和高可用性 Apache Apache 是世界使用排名第一的Web服务器软件。它可以运行在几乎所有广泛使用的计算机平台上。Apache源于NCSAhttpd服务器，经过多次修改，成为世界上最流行的Web服务器软件之一。Apache取自”a patchy server”的读音，意思是充满补丁的服务器，因为它是自由软件，所以不断有人来为它开发新的功能、新的特性、修改原来的缺陷。Apache的特点是简单、速度快、性能稳定，并可做代理服务器来使用。 IIS 是英文Internet Information Server的缩写，译成中文就是”Internet信息服务”的意思。它是微软公司主推的服务器，最新的版本是Windows2008里面包含的IIS 7，IIS与Window Server完全集成在一起，因而用户能够利用Windows Server和NTFS（NT File System，NT的文件系统）内置的安全特性，建立强大，灵活而安全的Internet和Intranet站点。 WebSphere软件平台能够帮助客户在Web上创建自己的业务或将自己的业务扩展到Web上，为客户提供了一个可靠、可扩展、跨平台的解决方案。作为IBM电子商务应用框架的一个关键组成部分，WebSphere软件平台为客户提供了一个使其能够充分利用Internet的集成解决方案。 Tomcat是一个开放源代码、运行servlet和JSP Web应用软件的基于Java的Web应用软件容器。Tomcat Server是根据servlet和JSP规范进行执行的，因此我们就可以说Tomcat Server也实行了Apache-Jakarta规范且比绝大多数商业应用软件服务器要好。 其中Apache和Tomcat是开源免费的。 1.3 Tomcat的历史Tomcat是Apache 软件基金会(Apache Software Foundation)的Jakarta 项目中的一个核心项目，由Apache、Sun 和其他一些公司及个人共同开发而成。由于有了Sun 的参与和支持，最新的Servlet 和JSP 规范总是能在Tomcat 中得到体现，Tomcat 5支持最新的Servlet 2.4 和JSP 2.0 规范。因为Tomcat 技术先进、性能稳定，而且免费，因而深受Java 爱好者的喜爱并得到了部分软件开发商的认可，成为目前比较流行的Web 应用服务器。 Tomcat 服务器是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选。对于一个初学者来说，可以这样认为，当在一台机器上配置好Apache 服务器，可利用它响应HTML(标准通用标记语言下的一个应用)页面的访问请求。实际上Tomcat是Apache 服务器的扩展，但运行时它是独立运行的，所以当你运行tomcat 时，它实际上作为一个与Apache 独立的进程单独运行的。 诀窍是，当配置正确时，Apache 为HTML页面服务，而Tomcat 实际上运行JSP 页面和Servlet。另外，Tomcat和IIS等Web服务器一样，具有处理HTML页面的功能，另外它还是一个Servlet和JSP容器，独立的Servlet容器是Tomcat的默认模式。不过，Tomcat处理静态HTML的能力不如Apache服务器。目前Tomcat最新版本为9.0.37。 1.4 Tomcat安装1.4.1 下载可以在Tomcat官网下载安。官网地址为：http://tomcat.apache.org/ 1.4.2 安装ubuntu系统中下载安装介质： 1root@deeplearning:/data/tomcat# wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-9/v9.0.41/bin/apache-tomcat-9.0.41.tar.gz 解压： 1root@deeplearning:/data/tomcat# tar -zxvf apache-tomcat-9.0.41.tar.gz 1.5 Tomcat 目录结构目录结构如下： 目录及文件 说明 bin 用于存放 Tomcat的启动、停止等批处理脚本和Shell脚本 bin/startup. bat 用于在 Windows下启动 Tomcat bin/startup.sh 用于在 Linux下启动 Tomcat bin/shutdown. bat 用于在 Windows下停止 Tomcat bin/shutdown.sh 用于在 Linux下停止 Tomcat conf 用于存放 Tomcat的相关配置文件 conf/Catalina 用于存储针对每个虚拟机的 Context 配置 conf/context.xml 用于定义所有Web应用均需要加载的 Context 配置，如果Web应用指定了自己的context.xml，那么该文件的配置将被覆盖 conf/catalina.properties Tomcat环境变量配置 conf/catalina.policy 当 Tomcat在安全模式下运行时，此文件为默认的安全策略配置 conf/logging.properties Tomcat日志配置文件，可通过该文件修改 Tomcat日志级别以及日志路径等 conf/server.xml Tomcat服务器核心配置文件，用于配置 Tomcat的链接器、监听端口、处理请求的虚拟主机等。可以说，Tomcat主要根据该文件的配置信息创建服务器实例 conf/tomcat-users.xml 用于定义 Tomcat默认用户及角色映射信息，Tomcat的 Manager模块即用该文件中定义的用户进行安全认证 conf/web.xml Tomcat中所有应用默认的部署描述文件，主要定义了基础 Servlet和MIME映射。如果应用中不包含 web. xml，那么 Tomcat将使用此文件初始化部署描述，反之，Tomcat会在启动时将默认部署描述与自定义配置进行合并 lib Tomcat服务器依赖库目录，包含 Tomcat服务器运行环境依赖jar包 logs Tomcat默认的日志存放路径 webapps Tomcat默认的Web应用部署目录 work 存放Web应用JSP代码生成和编译后产生的class文件目录 temp 存放tomcat在运行过程中产生的临时文件 1.6 Tomcat启停 启动 使用startup.sh脚本启动如下： 12345678root@deeplearning:/data/tomcat/apache-tomcat-9.0.41/bin# ./startup.shUsing CATALINA_BASE: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_HOME: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_TMPDIR: /data/tomcat/apache-tomcat-9.0.41/tempUsing JRE_HOME: /usr/lib/jvm/java-8-openjdk-amd64Using CLASSPATH: /data/tomcat/apache-tomcat-9.0.41/bin/bootstrap.jar:/data/tomcat/apache-tomcat-9.0.41/bin/tomcat-juli.jarUsing CATALINA_OPTS: Tomcat started. tomcat的默认监听端口为8080，这时候浏览器打开网址：http://ip:8080 ,显示出tomcat的网址。 停止 使用shutdown.sh脚本停止如下： 1234567root@deeplearning:/data/tomcat/apache-tomcat-9.0.41/bin# ./shutdown.shUsing CATALINA_BASE: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_HOME: /data/tomcat/apache-tomcat-9.0.41Using CATALINA_TMPDIR: /data/tomcat/apache-tomcat-9.0.41/tempUsing JRE_HOME: /usr/lib/jvm/java-8-openjdk-amd64Using CLASSPATH: /data/tomcat/apache-tomcat-9.0.41/bin/bootstrap.jar:/data/tomcat/apache-tomcat-9.0.41/bin/tomcat-juli.jarUsing CATALINA_OPTS: 1.7 Tomcat 源码在Tomcat官网可以下载源码压缩包，然后进行研读。 第二部分 Tomcat的架构2.1 Http工作原理Http协议是浏览器和服务器之间的数据传送协议，属于应用层协议。Http协议是基于TCP/IP协议来传递数据（html文件、图片、查询结构等）。Http协议不涉及数据包的传输，主要是规定客户端和服务端直接的通讯格式。 参考这篇文章：https://www.cnblogs.com/an-wen/p/11180076.html 主要过程为： 用户通过浏览器进行了一个操作，比如输入网址并回车，或者点击了链接，接着浏览器获取了这个事件。 浏览器向服务端发出TCP请求。 服务端程序接受到浏览器的连接请求，并经过了三次握手建立连接。 浏览器开始数据通信，将请求数据打包成一个http协议格式的数据包。 浏览器将数据包推入网络，数据包经过网络传输，最终到达服务端。 服务端程序拿到数据包后，使用http协议格式进行解包，获取客户端的请求。 服务端程序解析获得请求意图，比如提供静态文件等。 服务端程序将响应结果按照http协议格式进行打包。 服务器将响应结果的包推入网络，数据包经过网络传输最终到达浏览器程序。 浏览器拿到数据包后，同样以http协议格式进行解包，然后解析。 浏览器将响应内容展示在页面上。 2.2 Tomcat的整体架构2.2.1 Http服务器请求处理浏览器发给服务端的是一个http格式的请求，http服务器收到这个请求后，需要调用服务端程序来处理，所谓的服务端程序就是Java类，不同的请求需要由不同的Java类来处理。 两种方案： 方案一、Http服务直接和业务类交互 结构上紧耦合。 方案二、Servlet容器中间层 Http服务器不直接调用业务类，所有请求先发给Servlet容器。容器通过Servlet接口调用业务类，因此servlet接口和servlet容器的出现，达到了http服务器和业务类解耦的目的。servlet容器和servlet接口这一套规范称为Servlet规范。Tomcat按照这个规范要求实现了servlet容器，同时也具有http服务器的功能。作为java程序员。如果我们要实现新的业务功能，只需要实现一个servlet，并把它注册到Tomcat中，剩下的事情交给Tomcat帮助我们处理。 2.2.2 Servlet容器的工作流程为了解耦，http服务器不直接调用servlet，而是把请求交给servlet容器处理。当客户端请求资源的时候，http服务器会用一个servletRequest对象把客户的请求封装起来，然后调用servlet容器的service方法。servlet容器拿到请求后，根据请求的URL和servlet映射关系，找到相应的servlet。在这个过程中如果servlet没有被加载，就用反射机制创建这个servlet，并调用servlet的init方法来完成初始化。接着调用servlet方法来处理请求，把servletResponse对象返回给http服务器，http服务器会把相应发送到客户端。 2.2.3 Tomcat整体架构Tomcat整体架构主要是实现两个核心功能。 1、处理socket链接，负责网络字节流与Request和Response对象的转化 2、加载和管理servlet，以及具体处理Request请求。 Tomcat设计了两个核心组件：连接器（Connector）和容器（Container）来分别负责这两个功能。连接器负责对外交流，容器负责内部的处理。 2.3 连接器（Coyote）2.3.1 架构介绍2.3.2 IO模型与协议2.3.3 连接器组件2.3.4 源码解析2.4 容器2.5 Tomcat的启动流程2.6 Tomcat请求处理流程第三部分 Jasper第四部分 Tomcat服务器配置第五部分 Web应用配置第六部分 JVM配置第七部分 Tomcat集群第八部分 Tomcat 安全第九部分 Tomcat性能调优第十部分 Tomcat 附加功能参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Orange API 网关部署安装总结]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-18-Orange%20API%20%E7%BD%91%E5%85%B3%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 编译安装 第二部分 高版本安装 第三部分 docker安装 第四部分 总结 参考文献及资料 背景orange是基于Nginx+lua研发的API网关项目。该项目为国人自研项目，大部分组件参考Kong网关项目。但是项目活跃性不大，文档不够丰富详细（至今官网文档还是0.6.4版本，大量issue状态open），插件缺少详细的说明。另外项目部署自动化较低，文档不够详细，部署有一定困难。 本文在suse系统上部署orange高低版本（0.6.1和0.7.1两个版本）。详细说明的编译部署的过程和注意事项，算是为项目文档做点贡献，用爱发电。 文中讲解了对接Tidb数据的配置注意事项。 第一部分 编译安装编译安装以0.6.4版本为例。 1.1 编译依赖环境安装为了部署安装OpenResty，需要提前准备编译环境，依赖有： GCC 编译器 nginx是C语言编写的，编译安装需要安装 gcc和gcc-c++。不同操作系统安装有差异，在线情况较为简单，例如ubuntu使用下面的命令即可： 1# apt-get install gcc g++ libreadline-dev libncurses5-dev libpcre3-dev libssl-dev perl make build-essential 如果是suse或者centos操作系统： 12# yum install -y gcc# yum install -y gcc-c++ 注：离线安装较为繁琐，例如suse需要离线下载rpm包安装。 1.2 OpenResty安装openResty是一个基于nginx+lua的WEB服务器,所以安装此软件的过程中也会将nginx一并安装好，配置要把http_stub_status_module模块加上。具体安装如下： 1.2.1 安装介质准备将安装介质上传至/approot1/orangesoft(路径自定义),介质清单： 123456lor-0.3.4.tar.gzopenresty-1.19.3.1.tar.gzorange-0.6.4.tar.gzpcre-8.39.tar.gzzlib-1.2.11.tar.gzopenssl-1.0.2p.tar.gz 使用tar命令将包依次解压。 1.2.2 安装安装命令： 1234567root@deeplearning:/approot1/orangesoft # tar -zxvf pcre-8.39.tar.gzroot@deeplearning:/approot1/orangesoft # tar -zxvf zlib-1.2.11.tar.gzroot@deeplearning:/approot1/orangesoft # tar -zxvf openssl-1.0.2p.tar.gz# root@deeplearning:/approot1/orangesoft # tar -zxvf openresty-1.19.3.1.tar.gzroot@deeplearning:/approot1/orangesoft # cd openresty-1.19.3.1root@deeplearning:/approot1/orangesoft/openresty-1.19.3.1 # ./configure --with-luajit --with-http_stub_status_module --with-openssl=/approot1/orangesoft/openssl-1.0.2p --with-zlib=/approot1/orangesoft/zlib-1.2.11 --with-pcre=/approot1/orangesoft/pcre-8.39 注意： 参数：--with-http_stub_status_module，必须依赖，orange启动依赖组件 参数：--prefix=/approot1/openresty，参数可以指定安装目录。这里没有配置，会使用默认的路径：/usr/local/openresty。 配置完成后，开始编译和安装： 12root@deeplearning:/approot1/orangesoft/openresty-1.19.3.1 # makeroot@deeplearning:/approot1/orangesoft/openresty-1.19.3.1 # make install 1.2.3 配置环境变量配置环境变量，在/etc/profile文件中最佳下面的配置： 12export PATH=$PATH:/usr/local/openresty/nginx/sbin export PATH=$PATH:/usr/local/openresty/bin 最后当前shell中生效环境变量（或者重新开一个shell）： 1# source /etc/profile 1.2.4 验证验证nginx： 12# nginx -vnginx version: openresty/1.19.3.1 验证openresty： 1234567# resty -vresty 0.27nginx version: openresty/1.19.3.1built by gcc 4.8.5 (SUSE Linux) built with OpenSSL 1.0.2p 14 Aug 2018TLS SNI support enabledconfigure arguments: --prefix=/usr/local/openresty/nginx --with-cc-opt=-O2 --add-module=../ngx_devel_kit-0.3.1 --add-module=../echo-nginx-module-0.62 --add-module=../xss-nginx-module-0.06 --add-module=../ngx_coolkit-0.2 --add-module=../set-misc-nginx-module-0.32 --add-module=../form-input-nginx-module-0.12 --add-module=../encrypted-session-nginx-module-0.08 --add-module=../srcache-nginx-module-0.32 --add-module=../ngx_lua-0.10.19 --add-module=../ngx_lua_upstream-0.07 --add-module=../headers-more-nginx-module-0.33 --add-module=../array-var-nginx-module-0.05 --add-module=../memc-nginx-module-0.19 --add-module=../redis2-nginx-module-0.15 --add-module=../redis-nginx-module-0.3.7 --add-module=../rds-json-nginx-module-0.15 --add-module=../rds-csv-nginx-module-0.09 --add-module=../ngx_stream_lua-0.0.9 --with-ld-opt=-Wl,-rpath,/usr/local/openresty/luajit/lib --with-http_stub_status_module --with-openssl=/approot1/orange/soft/openssl-1.0.2p --with-zlib=/approot1/orange/soft/zlib-1.2.11 --with-pcre=/approot1/orange/soft/pcre-8.39 --with-openssl-opt=-g --with-pcre-opt=-g --with-zlib-opt=-g --with-stream --with-stream_ssl_module --with-stream_ssl_preread_module --with-http_ssl_module openresty 组件安装成功。 1.3 安装lor1.3.1 介质准备加压安装包： 12root@deeplearning:/approot1/orangesoft # tar -zxvf lor-0.3.4.tar.gzroot@deeplearning:/approot1/orangesoft # cd lor-0.3.4 执行编译安装： 123456root@deeplearning:/approot1/orangesoft/lor-0.3.4 # make install install lor runtime files to /usr/local/lorlor runtime files installed.install lord cli to /usr/local/bin/lord cli installed.lor framework installed successfully. 1.4 安装orange4.1 介质准备12root@deeplearning:/approot1/orangesoft # tar -zxvf orange-0.6.4.tar.gzroot@deeplearning:/approot1/orangesoft # cd orange-0.6.4 4.2 编译安装1root@deeplearning:/approot1/orangesoft/orange-0.6.4 # make install 默认安装目录为/usr/local/orange： 12345678910111213141516root@deeplearning:/usr/local # cd orange/root@deeplearning:/usr/local/orange # lltotal 52drwxrwxr-x 2 sysop root 4096 May 16 2017 apidrwxrwxr-x 5 sysop root 4096 May 16 2017 bindrwx------ 2 nobody root 4096 Jun 4 01:25 client_body_tempdrwxrwxr-x 2 sysop root 4096 Jun 4 01:25 confdrwxrwxr-x 7 sysop root 4096 May 16 2017 dashboarddrwx------ 2 nobody root 4096 Jun 4 01:25 fastcgi_tempdrwxrwxr-x 2 sysop root 4096 May 16 2017 installdrwxr-xr-x 2 sysop root 4096 Jun 4 01:25 logsdrwxrwxr-x 6 sysop root 4096 May 16 2017 orangedrwx------ 2 nobody root 4096 Jun 4 01:25 proxy_tempdrwx------ 2 nobody root 4096 Jun 4 01:25 scgi_tempdrwxr-xr-x 2 sysop root 4096 Jun 4 01:25 tmpdrwx------ 2 nobody root 4096 Jun 4 01:25 uwsgi_temp 4.3 配置连接mysql或Tidb4.3.1 更新mysql依赖包主要使用最新版本的包替换旧版本的包： 1# /usr/local/openresty/lualib/resty/mysql.lua 注意文件的大小检查： 1-rw-r--r-- 1 root bin 34779 Mar 3 07:47 mysql.lua 4.3.2 配置连接mysql或Tidb的连接信息更新conf/orange.conf配置文件中mysql的相关配置： 1root@deeplearning:/usr/local/orange/conf # vi orange.conf 主要有配置： 123456789101112131415161718"store": "mysql","store_mysql": &#123; "timeout": 5000, "connect_config": &#123; "host": "192.169.8.8", "port": 4001, "database": "orange", "user": "orange", "password": "orange", "max_packet_size": 1048576, "charset": "utf8mb4" &#125;, "pool_config": &#123; "max_idle_timeout": 10000, "pool_size": 3 &#125;, "desc": "mysql configuration"&#125;, 特别注意的参数是： 字符集，&quot;charset&quot;: &quot;utf8mb4&quot;为新增字符集参数，适配tidb使用； 4.3.3 数据库初始化数据库的初始化sql文件如下，提前通过mysql客户端多数据库进行初始化： 12root@deeplearning:/approot1/orangesoft/orange-0.6.4/install # ll orange-v0.6.4.sql-rw-rw-r-- 1 root root 10406 May 16 2017 orange-v0.6.4.sql 数据库初始化： 123456# CREATE DATABASE orange;# CREATE USER 'orange'@'%' IDENTIFIED BY 'orange';# GRANT ALL PRIVILEGES ON orange.* TO 'orange'@'%';# FLUSH PRIVILEGES; 然后初始化表：# mysql -uorange -porange orange &lt; orange-v0.6.4.sql 4.3.4 配置用户认证12345678"dashboard": &#123; "auth": true, "session_secret": "y0ji4pdj61aaf3f11c2e65cd2263d3e7e5", "whitelist": [ "^/auth/login$", "^/error/$" ] &#125;, 其中参数&quot;auth&quot;: true为开启权限认证。默认用户名密钥如下，初次登陆可以修改，并新增用户。 12用户名： admin密钥： orange_admin 4.4 启动orange安装完成后，orange执行文件在/usr/local/bin/orange，所用使用下面的命令： 1234567891011121314root@deeplearning:/usr/local/orange/conf # orange helpOrange v0.6.4, OpenResty/Nginx API Gateway.Usage: orange COMMAND [OPTIONS]The commands are: stop Stop current Orangehelp Show help tipsrestart Restart Orangeversion Show the version of Orangereload Reload the config of Orangestart Start the Orange Gatewaystore Init/Update/Backup Orange store 使用下面命令启动： 1root@deeplearning:/usr/local/orange/conf # orange start 4.5 验证启动后orange的前端监听端口在9999，浏览器使用下面的地址验证界面： 1http://192.168.31.3:9999 第二部分 高版本安装前面的编译安装主要针对低版本的0.6.4，对于高版本的0.7.1版本，由于引入了较多的第三方插件，所以在安装orange前需要补充安装一些依赖包。 openresty自带包管理工具opm，新版本建议使用新的包管理工具luarocks。 2.1 安装luarocks首先配置，安装在/usr/local/luarocks。 1234567891011121314151617181920root@deeplearning:/approot1/orange/soft/luarocks-3.0.3 # ./configure --with-lua=/usr/local/openresty/luajit --prefix=/usr/local/luarocksConfiguring LuaRocks version 3.0.3...Lua version detected: 5.1Lua interpreter found: /usr/local/openresty/luajit/bin/luajitlua.h found: /usr/local/openresty/luajit/include/luajit-2.1/lua.hunzip found in PATH: /usr/binDone configuring.LuaRocks will be installed at......: /usr/local/luarocksLuaRocks will install rocks at.....: /usr/local/luarocksLuaRocks configuration directory...: /usr/local/luarocks/etc/luarocksUsing Lua from.....................: /usr/local/openresty/luajit* Type make and make install: to install to /usr/local/luarocks as usual.* Type make bootstrap: to install LuaRocks into /usr/local/luarocks as a rock. 然后安装： 1root@deeplearning:/approot1/orange/soft/luarocks-3.0.3 # make &amp;&amp; make install 2.2 依赖包安装orange高版本引入多个第三方包，这些包依赖如下，需要在线环境进行安装。 1234567891011121314151617181920212223root@deeplearning:/usr/local/orange/logs # luarocks install luafilesystem# ...root@deeplearning:/usr/local/orange/logs # luarocks listRocks installed for Lua 5.1---------------------------binaryheap 0.4-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lrandom 20180729-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-dns-client 1.0.0-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-http 0.13-0 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-jwt 0.2.0-0 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1lua-resty-timer 1.1.0-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1luafilesystem 1.8.0-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1luasocket 3.0rc1-2 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1penlight 1.5.4-1 (installed) - /usr/local/luarocks/lib/luarocks/rocks-5.1 2.3 依赖配置依赖包安装的路径需要添加到rerquire搜索路径中。更新conf/nginx.conf配置文件中下面两个参数： 123#----------------------------Orange configuration----------------------------lua_package_cpath '/usr/local/luarocks/lib/lua/5.1/?.so;/usr/local/luarocks/lib/lua/5.1/socket/?.so;/usr/local/luarocks/lib/lua/5.1/mime/?.so;;';lua_package_path './lualib/?.lua;./lualib/resty/?.lua;/usr/local/orange/?.lua;/usr/local/lor/?.lua;/usr/local/luarocks/share/lua/5.1/?.lua;/usr/local/luarocks/share/lua/5.1/resty/?.lua;;'; 完成以上依赖配置后，参考低版本部署orange即可。 2.4 注意事项2.4.1 高版本sql文件缺失在部署高版本0.7.1时候，数据库表初始化sql文件orange-v0.7.0.sql中缺失组件headers表的初始化语句，可以在版本0.8.1中的初始化文件orange-v0.8.1.sql中获取。 说明项目的版本管理较为混乱。 2.4.2 lua-resty-mysql版本问题orange项目和mysql或者tidb交互使用的是依赖包lua-resty-mysql。版本差异有： 0.6.4版本使用0.19版本 0.7.1版本使用0.23版本 经过测试目前0.23版本兼容tidb，而低版本0.19版本不兼容。所以在部署低版本orange的时候注意替换和安装新版lua-resty-mysql包。 依赖包的位置，可以直接替换文件生效。 1/usr/local/openresty/lualib/resty/mysql.lua 第三部分 docker安装另外orange项目组也制作了docker镜像，项目地址为：syhily/orange。需要注意的是该镜像中orange版本为0.6.4版本，详细可以参考项目的Dockerfile文件。 2.1 部署mysql容器1# docker run --name orange-database -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=orange -p 3306:3306 mysql:5.7 2.2 部署orange集群启动主节点： 123456789101112# docker run -d --name orange \ --link orange-database:orange-database \ -p 7777:7777 \ -p 8888:8888 \ -p 9999:9999 \ --security-opt seccomp:unconfined \ -e ORANGE_DATABASE=orange \ -e ORANGE_HOST=orange-database \ -e ORANGE_PORT=3306 \ -e ORANGE_USER=root \ -e ORANGE_PWD=root \ syhily/orange 启动从节点： 12345678910# docker run -d --name orange \ -p 7777:7777 \ -p 8888:8888 \ -p 9999:9999 \ --security-opt seccomp:unconfined \ -e ORANGE_DATABASE=orange \ -e ORANGE_HOST=192.168.31.3 \ -e ORANGE_PORT=4000 \ -e ORANGE_USER=root \ syhily/orange 注意：官网的镜像配置中映射的api端口为8888，但是启动配置是80，这是错误配置。本文在这里进行了纠正。 第四部分 总结5.1 非编译安装前面的过程都是编译安装的，特别是组件openresty编译过程比较漫长。如果测试环境已经编译完成，只需要将部署目录整体拷贝至生产环境后即可（注意环境变量），不再赘述。 5.2 集群部署orange网关启动后将所有配置信息加载在内存中，mysql或者Tidb数据库只是作为配置的持久化保存。所有可以启动多个orange网关共用一套数据库，架构上组成集群模式。维护中需要注意的是： 同步配置 在集群部署模式下，一台网关应用服务器调整配置，需要及时手动同步配置到数据库持久化，另外其他网关节点需要手动同步将数据库中最新配置加载至网关内存。 研发配置自动同步组件 集群中每个节点登记注册加入，其中一台节点更新了配置，自动将登记的节点集合中所有配置自动刷新。 数据库故障 网关集群中，数据库只是配置的持久化。若短时间数据库故障不可访问，并不影响网关的对外服务。 5.3 动态调整低版本orange项目还不支持动态upstream配置功能。需要通过在nginx配置文件中新增。新增完成后需要使用下面的命令动态加载： 1# orange reload 高版本中有Dynamic Upstream组件实现动态upstream配置功能。 5.4 域名问题在实际生产线上环境，orange背后的持久化数据库对外地址通常是高可用的域名模式，另外网关代理的API地址通常也是内部域名。操作系统配置的DNS并不会被nginx读取，本地是hosts是有效的。 这时候就需要将内部DNS地址配置在conf/nginx.conf文件中，例如： 1resolver 192.168.12.1 192.168.12.2 ipv6=off; 上面两个DNS地址以轮询方式请求，解析间隔默认是3600秒。 写在最后期望orange项目组未来能投入更多的精力继续演进项目。尽快发布新版本，丰富文档，答复issue。否则项目也荒凉的风险呀。 参考文献及资料[1] OpenResty官网，链接：http://openresty.org/cn/download.html [2] orange 使用文档，链接：http://orange.sumory.com/docs/guides/usages/ [3] orange官网，链接：http://orange.sumory.com/]]></content>
      <categories>
        <category>Api网关</category>
      </categories>
      <tags>
        <tag>Api网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源大数据调度平台调研报告]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-elasticsearch%E5%A4%B8%E9%9B%86%E7%BE%A4%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Superset项目部署]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-23-Apache%20Superset%20%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[背景Apache Superset由airbnb公司 (知名在线房屋短租公司)开源的数据可视化项目。项目于 2015 年 6 月开源，活跃度极高。2021年1 月 21 日，Apache Superset项目组宣布毕业并成为 Apache 软件基金会（ASF）的顶级项目（Top-Level Project）。superset 在国内外都有着广泛的应用。superset 的国内比较知名的互联网公司有：bilibili、Douban、Kuaishou、Qunar 等等。 技术栈上，Superset 的前端主要用到了 React 和 NVD3/D3，而后端则基于 Python 的 Flask 框架和 Pandas、SQLAlchemy 等依赖库，主要提供的功能有： 集成数据查询功能，支持多种数据库，包括 MySQL、PostgresSQL、Oracle、SQL Server、SQLite、SparkSQL 等，并深度支持 Druid。 通过 NVD3/D3 预定义了多种可视化图表，满足大部分的数据展示功能。如果还有其他需求，也可以自开发更多的图表类型，或者嵌入其他的 JavaScript 图表库（如 HighCharts、ECharts）。 提供细粒度安全模型，可以在功能层面和数据层面进行访问控制。支持多种鉴权方式（如数据库、OpenID、LDAP、OAuth、REMOTE_USER 等）。 目前中文介绍项目部署的材料较少，本文将详细介绍多种部署方式。需要注意的是：Superset项目更新较快，很多命令最新版本和老版本有较大差异。 目录 背景 第一部分 传统部署安装 第二部分 基于docker部署 第三部分 基于helm在k8s集群部署 第四部分 生产高可用部署 参考文献及资料 第一部分 传统部署安装单机安装环境操作系统为：ubuntu 16.04，Python环境版本为：Python 3.7.6。 需要注意的是项目不支持在windows上运行，部署可以基于虚拟机。 1.1 基础环境准备Superset 使用Python语言编写，运行需要提前部署Python环境（要求版本大于Python 3.6）。如果具备互联网环境下，直接使用pip安装（非生产）即可： 1root@deeplearning:/data/superset# pip install apache-superset 1.2 配置应用Superset默认后台数据库使用sqllite。在生产环境中建议使用Mysql数据库（并具备高可用架构）。使用pip安装后，superset项目的Home目录通常在Python的包目录（具体环境目录存在差异）： 1/usr/anaconda3/lib/python3.7/site-packages/superset 其中目录中文件config.py是superset的配置文件(superset项目使用Flask框架编写)。通常我们关注的配置有： 应用服务配置 12345SUPERSET_WEBSERVER_PROTOCOL = "http"# 服务监听地址SUPERSET_WEBSERVER_ADDRESS = "0.0.0.0"# 服务监听端口SUPERSET_WEBSERVER_PORT = 8088 注意：使用pip安装部署的测试环境的配置在运行的时候是无效的，需要时命令行中重新指定。 后台数据库配置 1234# The SQLAlchemy connection string.SQLALCHEMY_DATABASE_URI = "sqlite:///" + os.path.join(DATA_DIR, "superset.db")# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp' 配置中默认使用的就是sqlite。生产环境中我们调整配置为mysql： 1SQLALCHEMY_DATABASE_URI = 'postgresql://root:root@localhost/superset' 另外只要sqlalchemy支持的数据源，superset都是支持的，列举如下： | database | pypi package | SQLAlchemy URI prefix || :——— | :————————————– | :———————————————————– || MySQL | pip install mysqlclient | mysql:// || Postgres | pip install psycopg2 | postgresql+psycopg2:// || Presto | pip install pyhive | presto:// || Hive | pip install pyhive | hive:// || Oracle | pip install cx_Oracle | oracle:// || sqlite | | sqlite:// || Snowflake | pip install snowflake-sqlalchemy | snowflake:// || Redshift | pip install sqlalchemy-redshift | redshift+psycopg2:// || MSSQL | pip install pymssql | mssql:// || Impala | pip install impyla | impala:// || SparkSQL | pip install pyhive | jdbc+hive:// || Greenplum | pip install psycopg2 | postgresql+psycopg2:// || Athena | pip install &quot;PyAthenaJDBC&gt;1.0.9&quot; | awsathena+jdbc:// || Athena | pip install &quot;PyAthena&gt;1.2.0&quot; | awsathena+rest:// || Vertica | pip install sqlalchemy-vertica-python | vertica+vertica_python:// || ClickHouse | pip install sqlalchemy-clickhouse | clickhouse:// || Kylin | pip install kylinpy | kylin:// || BigQuery | pip install pybigquery | bigquery:// || Teradata | pip install sqlalchemy-teradata | teradata:// || Pinot | pip install pinotdb | pinot+http://controller:5436/ query?server=http://controller:5983/ | 其中对大数据领域常见的 Druid、ClickHouse、Kylin、Presto等OLAP数据库的支持，是最具吸引力的。 汉化 修改Setup default language，BABEL_DEFAULT_LOCALE调整为zh。 1.3 初始化应用首先初始化管理员账号密码： 12345678910111213root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset fab create-adminlogging was configured successfullyINFO:superset.utils.logging_configurator:logging was configured successfully/usr/anaconda3/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled. "Flask-Caching: CACHE_TYPE is set to null, "Username [admin]: adminUser first name [admin]: adminUser last name [user]: adminEmail [admin@fab.org]: admin@fab.orgPassword: Repeat for confirmation: Recognized Database Authentications.Admin User admin created. 查看mysql数据库新增下面的用户表： 1234567891011121314mysql&gt; show tables;+-------------------------+| Tables_in_superset |+-------------------------+| ab_permission || ab_permission_view || ab_permission_view_role || ab_register_user || ab_role || ab_user || ab_user_role || ab_view_menu |+-------------------------+8 rows in set (0.01 sec) 继续初始化应用的表： 1root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset db upgrade 查看数据库会新增应用依赖表。最后创建默认角色和权限： 1root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset init 1.4 启动服务启动服务前我们加载一些案例数据： 1root@deeplearning:/usr/anaconda3/lib/python3.7/site-packages/superset# superset load_examples 注意：加载案例数据需要互联网环境，从互联网加载数据至数据库。完成上面的配置后就可以启动服务。 12# 启动命令root@deeplearning:# superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger 其中端口可以重新制定（覆盖配置文件中定义）。打开浏览器:http://localhost:8088/login/，弹出登陆界面，输入账号密码登陆。 第二部分 基于docker部署首先具备docker、docker-compose（version 1.24.1）和git环境。 12345# 拉取（如果国内拉取慢，可以下载zip包）git clone https://github.com/apache/superset.gitcd superset# you can run this command everytime you need to start superset now:docker-compose up 宿主机会启动下面的容器（一共6个）： IMAGE NAMES apache/superset:latest-dev superset_worker node:12 superset_node apache/superset:latest-dev superset_app apache/superset:latest-dev superset_init redis:3.2 superset_cache postgres:10 superset_db 使用默认用户名：admin/admin登录控制台界面：http://地址:8088/ 第三部分 基于helm在k8s集群部署首先具备helm和k8s集群环境。首先拉取Chart 加入库中： 12# helm repo rm cloudposse-incubator 2&gt;/dev/null# helm repo add cloudposse-incubator https://charts.cloudposse.com/incubator/ 然后安装： 123456789101112131415161718192021222324252627282930root@deeplearning:/data/helm# helm install cloudposse-incubator/supersetNAME: killjoy-squidLAST DEPLOYED: Sun Jan 31 16:06:02 2021NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1/DeploymentNAME READY UP-TO-DATE AVAILABLE AGEkilljoy-squid-superset 0/1 1 0 0s==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEkilljoy-squid-superset-6587d9797f-wv5t4 0/1 ContainerCreating 0 0s==&gt; v1/SecretNAME TYPE DATA AGEkilljoy-squid-superset Opaque 2 0s==&gt; v1/ServiceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkilljoy-squid-superset ClusterIP 10.98.190.160 &lt;none&gt; 9000/TCP 0sNOTES:Superset can be accessed via port 9000 on the following DNS name from within your cluster:killjoy-squid-superset.default.svc.cluster.localInitially you can login with username/password: admin/admin.WARNING: Persistence is DISABLED ! 使用宿主机上地址，打开：http://宿主机IP:9000/login/。查看应用： 123root@deeplearning:/data/superset/superset-master/helm/superset# helm list --allNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEkilljoy-squid 1 Sun Jan 31 16:06:02 2021 DEPLOYED superset-1.2.0 0.35.2 default 如果要删除应用： 1# helm delete killjoy-squid 第四部分 生产高可用配置以上部署均是测试部署，对于生产环境部署建议如下。 4.1 使用web服务部署目前支持web服务器有：Gunicorn，Nginx，Apache进行部署。 4.2 使用生产数据库默认后台数据库使用sqlite，生产建议使用MySQL，Postgres，MariaDB等数据库引擎，消息和缓存层使用redis。 4.3 开启负载均衡使用F5或者软件负载（Nginx）进行负载，组成web集群。 4.4 开源https和权限认证目前支持与OAuth2服务对接，并使用https协议。 第五部分 业务框架介绍5.1 整体框架superset整体使用业务框架并不复杂。主要业务概念有： 数据源。理论上sqlalchemy包支持的数据源，superset都是支持的。数据源配置中【配置读写权限。 图表（Chart）。superset 在这里定义了字段和指标（Metric）的概念。指标是对字段的某种统计结果，比如字段上值的求和、平均值、最大值、最小值等。 大屏板（Dashboard）。通过定义好的图表，组成一个大屏板。 5.2 权限介绍Superset的权限体系是通过Flask AppBuilder (FAB)完成，Flask-AppBuilder是基于Flask实现的一个用于快速构建Web后台管理系统的简单的框架。 Superset的默认角色有：Admin、Alpha、Gamma、sql_lab、Public Admin 管理员有所有的权利，其中包括授予或撤销其他用户和改变其他人的切片和仪表板的权利。 Alpha alpha可以访问所有数据源，但不能授予或撤消其他用户的访问权限，并且他们也只能修改自己的数据。alpha用户可以添加和修改数据源。 Gamma Gamma访问有限。他们只能使用他们通过另一个补充角色访问的数据源中的数据。他们只能访问查看从他们有权访问的数据源制作的切片和仪表板。目前，Gamma用户无法更改或添加数据源。我们假设他们大多是内容消费者，虽然他们可以创建切片和仪表板。 还要注意，当Gamma用户查看仪表板和切片列表视图时，他们只会看到他们有权访问的对象。 sql_lab sql_lab角色用于授予需要访问sql lab的用户，而管理员用户可以访问所有的数据库，默认情况下，Alpha和Gamma用户需要一个数据库的访问权限。 Public 允许登录用户访问一些Superset的一些功能。 另外Superset支持用户自定义创建角色，例如：您可以创建一个角色Financial Analyst，该角色将由一组数据源（表）和/或数据库组成。然后用户将被授予Gamma，Financial Analyst，或者sql_lab角色都可以。 第六部分 技术选型建议数据可视化开源项目较多，github上有个项目收集了大量项目清单，可以参考： https://github.com/thenaturalist/awesome-business-intelligence 6.1 优点 项目使用Python语言研发，用户具备技术栈后可以快速二次开发。社区较为活跃，项目演进较快。 可视化功能选项较为丰富。 6.2 缺点 权限管理。开源项目引入需要改造和内部权限打通。图表和Dashboard没有引入文件夹或者分组的理念，只有检索功能。权限系统特别复杂， 权限体系小规模使用还算方便，大规模使用需要很高的配置和运维成本。 使用Python语言，依赖环境较为复杂，传统环境（非云）部署较为复杂。建议基于云原生部署。 目前只支持每次可视化一张表，对于多表join的情况还无能为力。 依赖于数据库的快速响应，如果数据库本身太慢Superset也没什么办法 6.3 总结对于静态的日报、报表等业务需求，选择superset较好。整体定位上，Superset属于轻量级的BI项目，对于较为复杂的数据关联等逻辑应该在ETL过程中完成，Superset只是读取可视化结果表。 参考文献及资料1、Apache Superset项目代码托管地址，链接：https://github.com/apache/superset 2、pip源地址，链接：https://pypi.org/project/superset/ 3、在线文档，链接：https://apache-superset.readthedocs.io/en/0.36/installation.html 4、docker-superset，链接：https://abhioncbr.github.io/docker-superset/ 5、superset helm库，链接：https://artifacthub.io/packages/helm/cloudposse/superset]]></content>
      <categories>
        <category>Superset</category>
      </categories>
      <tags>
        <tag>Superset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源大数据调度平台调研报告]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-%E5%BC%80%E6%BA%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://mp.weixin.qq.com/s/2Pou8FoHVLuE4UR23wGM6w https://cloud.tencent.com/developer/article/1748204 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[streamsets数据采集平台介绍]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-streamsets%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://blog.csdn.net/ffjl1985/article/details/81391333 https://blog.csdn.net/xfg0218/article/details/80731557?utm_source=blogxgwz0&amp;utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-6&amp;spm=1001.2101.3001.4242 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Airflow数据调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-Airflow%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://www.jianshu.com/p/e878bbc9ead2 https://airflow.apache.org/community/ 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache NiFi数据调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-Apache%20NiFi%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 https://cloud.tencent.com/developer/news/89963 https://blogs.apache.org/nifi/ http://nifi.apache.org/quickstart.html https://github.com/apache/nifi 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache DolphinScheduler调度平台调研]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-03-Apache%20DolphinScheduler%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 单机部署： https://dolphinscheduler.apache.org/zh-cn/docs/1.3.4/user_doc/standalone-deployment.html 最新架构图： https://www.analysys.cn/developer/apache-dolphinscheduler/ 源码分析: https://www.cnblogs.com/gabry/p/12217966.html 参考文献及资料1、 官网地址，链接:https://tomcat.apache.org/]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kong API 网关使用]]></title>
    <url>%2F2021%2F01%2F02%2F2021-01-31-Kong%20API%20%E7%BD%91%E5%85%B3%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[背景目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 # 本次分享我们从百亿流量交易系统API网关（API Gateway）的现状和面临问题出发，阐述微服务架构与 API 网关的关系，理顺流量网关与业务网关的脉络，带来最全面的 API 网关知识与经验。内容涉及： 第一部分：API网关概述 分布式服务架构、微服务架构与 API 网关 API 网关的定义与职能、关注点 API 网关的分类与技术分析 第二部分：开源网关的分析与调研 常见的开源网关介绍 四大开源网关的对比分析（OpenResty/Kong/Zuul2/SpringCloudGateway 等） 开源网关的技术总结 第三部分：百亿流量交易系统 API 网关设计 百亿流量 API 网关的现状和面临问题 业务网关的设计与最佳实践 对API网关的发展展望 第一部分：API网关概述 计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决。 – David Wheeler 分布式服务架构、微服务架构与 API 网关什么是API网关（API Gateway）其实网关跟面向服务架构（Service Oriented Architecture，SOA）和微服务架构（MicroServices Architecture，MSA）有很深的渊源。 十多年以前，银行等金融机构完成全国业务系统大集中以后，分散的系统都变得集中，同时也带来各种问题：业务发展过快如何应对，对接系统过多如何集成和管理。为了解决这些问题，业界实现了作用于渠道与业务系统之间的中间层网关，即综合前置系统，由其适配各类渠道和业务，处理各种协议接入、路由与报文转换、同步异步调用等。 图1 人们基于SOA的理念，在综合前置的基础上，进一步增加了服务的元数据管理、注册、中介、编排、治理等功能，逐渐形成了企业服务总线（ESB，Enterprise Service Bus）。 图2（作者参与设计开发的Primeton ESB系统） 面向服务架构（SOA）是一种建设企业IT生态系统的架构指导思想。SOA的关注点是服务，服务最基本的业务功能单元，由平台中立性的接口契约来定义。通过将业务系统服务化，可以将不同模块解耦，各种异构系统间可以轻松实现服务调用、消息交换和资源共享。 不同于以往的孤立业务系统，SOA强调整个企业IT生态环境是一个大的整体。整个IT生态中的所有业务服务构成了企业的核心IT资源。各系统的业务拆解为不同粒度和层次的模块和服务，服务可以组装到更大的粒度，不同来源的服务可以编排到同一个处理流程，实现非常复杂的集成场景和更加丰富的业务功能。 SOA从更高的层次对整个企业IT生态进行统一的设计与管理，应用软件被划分为具有不同功能的服务单元，并通过标准的软件接口把这些服务联系起来，以SOA架构实现的企业应用可以更灵活快速地响应企业业务变化，实现新旧软件资产的整合和复用，降低软件整体拥有成本。 当然基于ESB这种集中式管理的SOA方案也存在着种种问题，特别是面向互联网技术领域的爆发式发展的情况下。 分布式服务架构、微服务架构与API网关近年来，随着互联网技术的飞速发展，为了解决以ESB为代表的集中式管理的SOA方案的种种问题，以Apache Dubbo（2011年开源后）与Spring Cloud为代表的分布式服务化技术的出现，给了SOA实现的另外一个选择：去中心化的分布式服务架构（DSA）。分布式服务架构技术不再依赖于具体的服务中心容器技术（比如ESB），而是将服务寻址和调用完全分开，这样就不需要通过容器作为服务代理。 之后又在此基础上随着REST、Docker容器化、领域建模、自动化测试运维等领域的发展，逐渐形成了微服务架构（MSA）。在微服务架构里，服务的粒度被进一步细分，各个业务服务可以被独立地设计、开发、测试、部署和管理。这时，各个独立部署单元可以选择不同的开发测试团队维护，可以使用不同的编程语言和技术平台进行设计，但是要求必须使用一种语言和平台无关的服务协议作为各个单元之间的通信方式，如图7-3所示。 图3 在微服务架构中，由于系统和服务的细分，导致系统结构变得非常复杂，RESTAPI由于其简单、高效、跨平台、易开发、易测试、易集成，成为不二选择。此时一个类似综合前置的系统就产生了，这就是API网关（API Gateway）。API网关作为分散在各个业务系统微服务的API聚合点和统一接入点，外部请求通过访问这个接入点，即可访问内部所有的REST API服务。 跟SOA/ESB类似，企业内部向外暴露的所有业务服务能力，都可以通过API网关上管理的API服务得以体现，所以API网关上也就聚合了企业所有直接对外提供的IT业务能力。 API网关的技术趋势我们从百度指数趋势看到，SpringCloud和SOA非常火，MSA、gRPC、Gateway也都有着非常高的关注度，而且这些技术的搜索趋势都正相关。 另一方面，我们可以通过Github的搜索来看，Gateway类型的项目也非常多。 图4 github.com/search?o=de… 可以看到，前10页的100个项目，使用Go语言实现的Gateway差不多占一半，语言分类上来看： Go&gt;NodeJS/JavaScript&gt;Java&gt;Lua&gt;C/C++&gt;PHP&gt;Python/Ruby/Perl API 网关的定义、职能与关注点API网关的定义 网关的角色是作为一个API架构，用来保护、增强和控制对于API服务的访问。（The role of a Gateway in an API architecture is to protect, enrich and control access to API services.）– github.com/strongloop/… API网关是一个处于应用程序或服务（提供REST API接口服务）之前的系统，用来管理授权、访问控制和流量限制等，这样REST API接口服务就被API网关保护起来，对所有的调用者透明。因此，隐藏在API网关后面的业务系统就可以专注于创建和管理服务，而不用去处理这些策略性的基础设施。 这样，网关系统就可以代理业务系统的业务服务API。此时网关接受外部其他系统的服务调用请求，也需要访问后端的实际业务服务。在接受请求的同时，可以实现安全相关的系统保护措施。在访问后端业务服务的时候，可以根据相关的请求信息做出判断，路由到特定的业务服务上，或者调用多个服务后聚合成新的数据返回给调用方。网关系统也可以把请求的数据做一些过程和预处理，同理也可以把返回给调用者的数据做一些过滤和预处理，即根据需要对请求头/响应头、请求报文/响应报文做一些修改处理。如果不做这些额外的处理，最简单直接的代理服务API功能，我们一般叫做透传。 同时，由于REST API的语言无关性，我们可以看出基于API网关，我们的后端服务可以是任何异构系统，不论是Java、Dotnet、Python，还是PHP、ROR、NodeJS等，只要是支持REST API，就可以被API网关管理起来。 API网关的职能 图5 一般来说，API网关有四大职能： 请求接入：作为所有API接口服务请求的接入点，管理所有的接入请求； 业务聚合：作为所有后端业务服务的聚合点，所有的业务服务都可以在这里被调用； 中介策略：实现安全、验证、路由、过滤、流控，缓存等策略，进行一些必要的中介处理； 统一管理：提供配置管理工具，对所有API服务的调用生命周期和相应的中介策略进行统一管理。 API网关的关注点通过以上的分析可以看出，API网关不是一个典型的业务系统， 而是一个为了让业务系统更专注与业务服务本身，给API服务提供更多附加能力的一个中间层。 这样在设计和实现API网关时，两个目标需要考虑： 开发维护简单，节约人力成本和维护成本。这要求我们使用非常成熟的简单可维护的技术体系。 高性能，节约设备成本，提高系统吞吐能力。这要求我们需要针对API网关的特点，进行一些特定的设计和权衡。 当并发量小的时候，这些都不是问题。然后一旦系统的API访问量非常大的时候，这些都会成为关键的问题。 海量并发的Gateway最重要的三个关注点： 保持大规模的inbound请求接入能力（长短连接），比如基于netty实现。 最大程度的复用outbound的HTTP连接能力，比如基于HttpClient4的asynchronizedHttpclient实现。 方便灵活地实现安全、验证、过滤、聚合、限流、监控等各种策略。 API网关的分类与技术分析API网关的分类如果我们对于上述的目标和关注点进行更深入的思考，就会发现一个很重要的问题：所有需要考虑的问题和功能可以分为两类。 一类是全局性的，跟具体的后端业务系统和服务完全无关的部分，比如安全策略、全局性流控策略、流量分发策略等。 一类是针对具体的后端业务系统，或者是服务和业务有一定关联性的部分，并且一般被直接部署在业务服务的前面。 图6 这样，随着互联网的复杂业务系统的发展，这两类功能集合逐渐形成了现在常见的两种网关系统：流量网关和业务网关。 流量网关与WAF我们定义全局性的、跟具体的后端业务系统和服务完全无关的策略网关，即为流量网关。这样流量网关关注于全局流量的稳定与安全，具体比如防止各类SQL注入，黑白名单控制，接入请求到业务系统的Loadbalance等，通常有如下的一些通用性功能： 全局性流控 日志统计 防止SQL注入 防止Web攻击 屏蔽工具扫描 黑白名单控制 等等。 通过这个功能清单，我们可以发现，流量网关的功能跟Web应用防火墙（WAF）非常类似。WAF一般是基于Nginx/OpenResty的ngx_lua模块开发的Web应用防火墙。 WAF一般代码很简单，关注于使用简单，高性能和轻量级。简单的说就是在Nginx本身的代理能力以外，添加了安全相关功能。一句话来描述其原理，就是解析HTTP请求（协议解析模块），规则检测（规则模块），做不同的防御动作（动作模块），并将防御过程（日志模块）记录下来。 一般的WAF具有如下功能： 防止SQL注入，本地包含，部分溢出，fuzzing测试，XSS/SSRF等Web攻击 防止Apache Bench之类压力测试工具的攻击 屏蔽常见的扫描黑客工具，扫描器 屏蔽图片附件类目录执行权限、防止webshell上传 支持IP白名单和黑名单功能，直接将黑名单的IP访问拒绝 支持URL白名单，将不需要过滤的URL进行定义 支持User-Agent的过滤、支持CC攻击防护、限制单个URL指定时间的访问次数 支持支持Cookie过滤，URL与URL参数过滤 支持日志记录，将所有拒绝的操作，记录到日志中去 几个WAF开源实现以上WAF的内容主要参考如下两个项目： github.com/unixhot/waf github.com/loveshell/n… 流量网关的开源实例，还可以参考著名的开源项目Kong（基于OpenResty）。 业务网关我们定义针对具体的后端业务系统，或者是服务和业务有一定关联性的策略网关，即为业务网关。比如针对某个系统、某个服务或者某个用户分类的流控策略，针对某一类服务的缓存策略，针对某个具体系统的权限验证方式，针对某些用户条件判断的请求过滤，针对具体几个相关API的数据聚合封装等等。 业务网关一般部署在流量网关之后，业务系统之前，比流量网关更靠近系统。我们大部分情况下说的API网关，狭义上指的是业务网关。并且如果系统的规模不大，我们也会将两者合二为一，使用一个网关来处理所有的工作。具体的业务网关设计实现，将在下面的篇章详细介绍。 第二部分：开源网关的分析与调研常见的开源网关介绍 图7（开源网关技术图谱） 目前常见的开源网关大致上按照语言分类有如下几类： Nginx+lua：Open Resty、Kong、Orange、Abtesting gateway等 Java：Zuul/Zuul2、Spring Cloud Gateway、Kaazing KWG、gravitee、Dromara soul等 Go：Janus、fagongzi、Grpc-gateway Dotnet：Ocelot NodeJS：Express Gateway、Micro Gateway 按照使用数量、成熟度等来划分，主流的有4个： OpenResty Kong Zuul/Zuul2 Spring Cloud Gateway Nginx+LuaOpenResty*项目地址：openresty.org/ OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。 OpenResty® 通过汇聚各种设计精良的 Nginx 模块（主要由 OpenResty 团队自主开发），从而将 Nginx 有效地变成一个强大的通用 Web 应用平台。这样，Web 开发人员和系统工程师可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10K 乃至 1000K 以上单机并发连接的高性能 Web 应用系统。 OpenResty® 的目标是让你的Web服务直接跑在 Nginx 服务内部，充分利用 Nginx 的非阻塞 I/O 模型，不仅仅对 HTTP 客户端请求,甚至于对远程后端诸如 MySQL、PostgreSQL、Memcached 以及 Redis 等都进行一致的高性能响应。 以上介绍来自于OpenResty网站中文版：openresty.org/cn 简单的说，OpenResty基于Nginx，集成了Lua语言和Lua的各种工具库，可用的第三方模块，这样我们就在Nginx既有的高效HTTP处理的基础上，同时获得了Lua提供的动态扩展能力。因此，我们可以做出各种符合我们需要的网关策略的Lua脚本，以其为基础实现我们的网关系统。 Kong*项目地址：konghq.com/ 与 github.com/kong/kong Kong基于OpenResty，是一个云原生、快速、可扩展、分布式的微服务抽象层（Microservice Abstraction Layer），也叫API网关（API Gateway），在Service Mesh里也叫API中间件（API Middleware）。 Kong开源于2015年，核心价值在于高性能和扩展性。从全球5000强的组织统计数据来看，Kong是现在依然在维护的，在生产环境使用最广泛的API网关。 Kong宣称自己是世界上最流行的开源微服务API网关（The World’s Most Popular Open Source Microservice API Gateway）。 核心优势： 可扩展：可以方便的通过添加节点水平扩展，这意味着可以在很低的延迟下支持很大的系统负载。 模块化：可以通过添加新的插件来扩展Kong的能力，这些插件可以通过RESTful Admin API来安装和配置。 在任何基础架构上运行：Kong可以在任何地方都能运行，比如在云或混合环境中部署Kong，单个或全球的数据中心。 图8 ABTestingGateway项目地址：github.com/CNSRE/ABTes… ABTestingGateway是一个可以动态设置分流策略的网关，关注与灰度发布相关领域，基于nginx和ngx-lua开发，使用 redis 作为分流策略数据库，可以实现动态调度功能。 ABTestingGateway 是新浪微博内部的动态路由系统 dygateway 的一部分，目前已经开源。在以往的基于 nginx 实现的灰度系统中，分流逻辑往往通过 rewrite 阶段的 if 和 rewrite 指令等实现，优点是性能较高，缺点是功能受限、容易出错，以及转发规则固定，只能静态分流。ABTestingGateway则采用 ngx-lua，通过启用lua-shared-dict和lua-resty-lock作为系统缓存和缓存锁，系统获得了较为接近原生nginx转发的性能。 功能特性： 支持多种分流方式，目前包括iprange、uidrange、uid尾数和指定uid分流 支持多级分流，动态设置分流策略，即时生效，无需重启 可扩展性，提供了开发框架，开发者可以灵活添加新的分流方式，实现二次开发 高性能，压测数据接近原生nginx转发 灰度系统配置写在nginx配置文件中，方便管理员配置 适用于多种场景：灰度发布、AB测试和负载均衡等 据了解，美团内部的Oceanus也是基于Nginx和ngx_lua扩展实现，主要提供服务注册与发现、动态负载均衡、可视化管理、定制化路由、安全反扒、session ID复用、熔断降级、一键截流和性能统计等功能。 JAVAZuul/Zuul2*项目地址：github.com/Netflix/zuu… Zuul是Netflix开源的API网关系统，它的主要设计目标是动态路由、监控、弹性和安全。 Zuul的内部原理可以简单看做是很多不同功能filter的集合（PS：作为对比，ESB也可以简单被看做是管道（channel）和过滤器（filter）的集合。），这些filter可以使用groovy或其他基于JVM的脚本编写（当然Java也可以编写），放置在指定的位置，然后可以被Zuul Server轮询发现变动后动态加载并实时生效。 Zuul目前有两个大的版本，1.x和2.x，这两个版本差别很大。 Zuul1.x基于同步IO，也是Spring Cloud全家桶的一部分，可以方便的配合Spring Boot/Spring Cloud配置和使用。 在Zuul1.x里，filter的种类和处理流程可以参见下图，最主要的就是pre、routing、post这三种过滤器，分别作用于调用业务服务API之前的请求处理、直接响应、调用业务服务API之后的响应处理。 图9（Zuul 1.x示意图） Zuul2.x最大的改进就是基于Netty Server实现了异步IO来接入请求，同时基于Netty Client实现了到后端业务服务API的请求。这样就可以实现更高的性能、更低的延迟。此外也调整了filter类型，将原来的三个核心filter显式命名为：Inbound Filter、Endpoint Filter和Outbound Filter。 图10（Zuul 2.x） Zuul2.x核心功能： Service Discovery Load Balancing Connection Pooling Status Categories Retries Request Passport Request Attempts Origin Concurrency Protection HTTP/2 Mutual TLS Proxy Protocol GZip WebSockets Spring Cloud Gateway*项目地址：github.com/spring-clou… Spring Cloud Gateway基于Java8、Spring 5.0、Spring Boot 2.0、Project Reactor，发展的比Zuul2要早，目前也是Spring Cloud全家桶的一部分。 Spring Cloud Gateway可以看做是一个Zuul1.x的升级版和代替品，比Zuul2更早的使用Netty实现异步IO，从而实现了一个简单、比Zuul1.x更高效的、与Spring Cloud紧密配合的API网关。 Spring Cloud Gateway里明确的区分了Router和Filter，并且一个很大的特点是内置了非常多的开箱即用功能，并且都可以通过SpringBoot配置或者手工编码链式调用来使用。 比如内置了10种Router，使得我们可以直接配置一下就可以随心所欲的根据Header，或者Path，或者Host，或者Query来做路由。 比如区分了一般的Filter和全局Filter，内置了20种Filter和9种全局Filter，也都可以直接用。当然自定义Filter也非常方便。 核心特性： Able to match routes on any request attribute. Predicates and filters are specific to routes. Hystrix Circuit Breaker integration. Spring Cloud DiscoveryClient integration Easy to write Predicates and Filters Request Rate Limiting Path Rewriting gravitee gateway项目地址：gravitee.io/ 与 github.com/gravitee-io… Kaazing WebSocket Gateway项目地址：github.com/kaazing/gat… 与 kaazing.com/products/we… Kaazing WebSocket Gateway是一个专门针对和处理Websocket的网关，其宣称提供世界一流的企业级WebSocket服务能力。 具体如下特性： 标准WebSocket支持，支持全双工的双向数据投递 线性扩展，无状态架构意味着可以部署更多机器来扩展服务能力 验证，鉴权，单点登录支持，跨域访问控制 SSL/TLS加密支持 Websocket keepalive和TCP半开半关探测 通过负载均衡和集群实现高可用 Docker支持 JMS/AMQP等支持 IP白名单 自动重连和消息可靠接受保证 Fanout处理策略 实时缓存等 Dromara soul项目地址： github.com/Dromara/sou… Soul是一个异步的、高性能的、跨语言的、响应式的API网关，提供了统一的HTTP访问。 支持各种语言，无缝集成Dubbo和SpringCloud； 丰富的插件支持鉴权、限流、熔断、防火墙等； 网关多种规则动态配置，支持各种策略配置； 插件热插拔，易扩展； 支持集群部署，支持A/B Test。 图11 Gofagongzi项目地址：github.com/fagongzi/ga… fagongzi gateway是一个Go实现的功能全面的API Gateway，自带了一个rails实现的web UI管理界面。 功能特性： 流量控制 熔断 负载均衡 服务发现 插件机制 路由(分流，复制流量) API 聚合 API 参数校验 API 访问控制（黑白名单） API 默认返回值 API 定制返回值 API 结果Cache JWT Authorization API Metric导入Prometheus API 失败重试 后端server的健康检查 开放管理API(GRPC、Restful) 支持Websocket协议 Janus项目地址：github.com/hellofresh/… Janus是一个轻量级的API Gateway和管理平台，它能帮你实现控制谁，什么时候，如何访问这些REST API，同时它也记录了所有的访问交互细节和错误。使用Go实现API网关的一个好处在于，一般只需要一个单独的二进制文件即可运行，没有复杂的依赖关系（No dependency hell）。 功能特性： 热加载配置，不需要重启网关进程 HTTP连接的优雅关闭 支持OpenTracing，从而可以进行分布式跟踪 支持HTTP/2 可以针对每一个API实现断路器 重试机制 流控，可以针对每一个用户或者key CORS过滤，可以针对具体的API 多种开箱即用的验证协议支持，比如JWT、OAuth2.0和Basic Auth docker image支持 DotnetOcelot项目地址：github.com/ThreeMammal… 路由 请求聚合 服务发现（基于Consul或Eureka） 服务Fabric WebSockets 验证与鉴权 流控 缓存 重试策略与QoS 负载均衡 日志与跟踪 请求头、Query字符串转换 自定义的中间处理 配置和管理REST API NodeJSExpress Gateway项目地址：github.com/ExpressGate… 与 www.express-gateway.io/ Express Gateway是一个基于NodeJS开发，Express和Express中间件实现的REST API网关。 功能特性： 动态中心化配置 API消费者和凭证管理 插件机制 分布式数据存储 命令行工具CLI microgateway项目地址：github.com/strongloop/… 与 developer.ibm.com/apiconnect StrongLoop是IBM的一个子公司，Microgateway网关基于Node.js/Express和Nginx构建，作为IBM API Connect，同时也是IBM云生态的一部分。 Microgateway是一个聚焦于开发者，可扩展的网关框架，它可以增强我们对微服务和API的访问能力。 核心特性： 安全和控制，基于Swagger(OpenAPI)规范 内置了多种网关策略，API Key验证，流控，OAuth2.0，JavaScript脚本支持 使用Swagger扩展（API Assembly）实现网关策略（安全、路由、集成等） 方便地自定义网关策略 此外，Microgateway还有几个特性： 通过集成Swagger，实现基于Swagger API定义的验证能力， 使用datastore来保持需要处理的API数据模型， 使用一个流式引擎来处理多种策略，使得API设计者可以更好的控制API的生命周期 核心架构如下图所示： 图12 四大开源网关的对比分析（OpenResty/Kong/Zuul2/SpringCloudGateway等）OpenResty/Kong/Zuul2/SpringCloudGateway重要特性对比 网关 限流 鉴权 监控 易用性 可维护性 成熟度 Spring Cloud Gateway 可以通过IP，用户，集群限流，提供了相应的接口进行扩展 普通鉴权、auth2.0 Gateway Metrics Filter 简单易用 spring系列可扩展强，易配置 可维护性好 spring社区成熟，但gateway资源较少 Zuul2 可以通过配置文件配置集群限流和单服务器限流亦可通过filter实现限流扩展 filter中实现 filter中实现 参考资料较少 可维护性较差 开源不久，资料少 OpenResty 需要lua开发 需要lua开发 需要开发 简单易用，但是需要进行的lua开发很多 可维护性较差，将来需要维护大量lua脚本 很成熟资料很多 Kong 根据秒，分，时，天，月，年，根据用户进行限流。可在原码的基础上进行开发 普通鉴权，Key Auth鉴权，HMAC，auth2.0 可上报datadog，记录请求数量，请求数据量，应答数据量，接收于发送的时间间隔，状态码数量，kong内运行时间 简单易用，api转发通过管理员接口配置，开发需要lua脚本 可维护性较差，将来需要维护大量lua库 相对成熟，用户问题汇总，社区，插件开源 以限流功能为例： Spring Cloud Gateway目前提供了基于Redis的Ratelimiter实现，使用的算法是令牌桶算法，通过yml文件进行配置； Zuul2可以通过配置文件配置集群限流和单服务器限流亦可通过filter实现限流扩展； OpenResty可以使用resty.limit.count、resty.limit.conn、resty.limit.req来实现限流功能可实现漏桶或令牌通算法； Kong拥有基础限流组件，可在基础组件源代码基础上进行lua开发。 对Zuul/Zuul2/Spring Cloud Gateway的一些功能点分析可以参考Spring Cloud Gateway作者Spencer Gibb的文章： spencergibb.netlify.com/preso/detro… OpenResty/Kong/Zuul2/SpringCloudGateway性能测试对比分别使用3台4Core16G内存的机器，作为API服务提供者、Gateway、压力机，使用wrk作为性能测试工具，对OpenResty/Kong/Zuul2/SpringCloudGateway进行简单小报文的情况进行性能测试。 图13（Spring Cloud Gateway、Zuul2、OpenResty、Kong的性能对比） 上图中y轴坐标是QPS，x轴是一个gateway的数据，每根线是一个场景下的不同网关数据，测试结论如下： 实测情况是性能 SCG~Zuul2 &lt;&lt; OpenResty ~&lt; Kong &lt;&lt; Direct（直连）； Spring Cloud Gateway、Zuul2的性能差不多，大概是直连的40%； OpenResty、Kong差不多，大概是直连的60-70%； 大并发下，例如模拟200并发用户、1000并发用户时，Zuul2会有很大概率返回出错。 开源网关的技术总结开源网关的测试分析脱离场景谈性能，都是耍流氓。性能就像温度，不同的场合下标准是不一样的。同样是18摄氏度，老人觉得冷，小孩觉得很合适，企鹅觉得热，冰箱里的蔬菜可能要坏了。 同样基准条件下，不同的参数和软件，相对而言的横向比较，才有价值。比如同样的机器（比如16G内存/4Core），同样的server（用spring boot，配置路径api/hello返回一个helloworld），同样的压测方式和工具（比如用wrk，10线程，20并发连接），我们测试直接访问server得到的极限QPS（QPS-Direct，29K），和配置了一个spring cloud gateway做网关访问的极限QPS（QPS-SCG，11K）、同样方式配置一个Zuul2做网关压测得到的极限QPS（QPS-Zuul2，13K），Kong得到的极限QPS（QPS-Kong，21K），OpenResty得到的极限QPS（QPS-OR，19K），这个对比就有意义了。 Kong的性能非常不错，非常适合做流量网关，并且对于 service，route，upstream，consumer，plugins的抽象，也是自研网关值得借鉴的。 对于复杂系统，不建议业务网关用Kong，或者更明确的说是不建议在Java技术栈的系统深度定制Kong或OpenResty，主要是工程性方面的考虑。举个例子：假如我们有很多个不同业务线，鉴权方式五花八门，都是与业务多少有点相关的。这时如果把鉴权在网关实现，就需要维护大量的Lua脚本，引入一个新的复杂技术栈是一个成本不低的事情。 Spring Cloud Gateway/Zuul2对于Java技术栈来说比较方便，可以依赖业务系统的一些common jar。Lua不方便，不光是语言的问题，更是复用基础设施的问题。另外，对于网关系统来说，性能不是差一个数量级，问题不大，多加2台机器就可以搞定。 目前测试的总结来看，如果服务都是2ms级别，直连的性能假如是100，Kong可以到60，OpenResty是50，Zuul2和Spring Cloud Gateway是35，如果服务本身的latency大一点，这些个差距会逐步缩小。 目前来看Zuul2的坑还是比较多的： 不成熟，没文档，刚出不久，还没有太多的实际应用案例 高并发时出错率较高，1000并发时我们的测试场景近50%的出错 所以简单使用或者轻度定制业务网关系统，目前比较建议使用Spring Cloud Gateway作为基础骨架。 各类网关的demo与测试以上测试用到的模拟服务和网关demo代码，大部分可以在这里找到：github.com/kimmking/sp… 也简单模拟了一个NodeJS做的Gateway，加了keep-alive和pool，demo的性能测试结果大概是直连的1/9，也就是Spring Cloud Gateway或Zuul2的1/4左右。 第三部分：百亿流量交易系统 API 网关设计百亿流量交易系统 API网关的现状和面临问题百亿流量系统面对的业务现状 图14 我们目前面临的现状是日常十几万的并发在线长连接数（不算短连接），每天长连接总数3000万+，每天API的调用次数超过100亿，每天交易订单数1.5亿。 在这个情况下，API网关设计的一个重要目标就是：如何借助API网关为各类客户提供精准、专业、个性化的服务，保障客户实时的获得业务系统的数据和业务能力。 网关系统与其他系统的关系我们的业务里，API网关系统与其他系统的关系大致如下图所示： 图15 网关系统典型的应用场景我们的API网关系统为Web端、移动APP端客户提供服务，同时也为大量API客户提供API调用服务，同时支持REST API和WebSocket协议。 作为实时交易系统的前置系统，必须精准及时为客户提供最新的行情和交易信息。一旦出现数据的延迟或者错误，都会给客户造成无法挽回的损失。 另外针对不同的客户和渠道，网关系统需要提供不同的安全、验证、流控、缓存策略，同时可以随时聚合不同视角的数据进行预处理，保障系统的稳定可靠和数据的实时精确。 图16、图17 交易系统API的特点作为一个全球性的交易系统，API的特点总结如下： 访问非常集中：最核心的一组API，占据了访问量的一半以上 访问非常频繁：QPS非常高，日均访问量非常大 数据格式固定：交易系统处理的数据格式非常固定 报文数据量小：每次请求传输的数据一般不超过10K 用户全世界分布：客户分布在全世界的各个国家 分内部调用和外部调用：除了API客户直接调用的API，其他的API都是由内部其他系统调用的 7x24小时不间断服务：系统需要提供高可用、不间断的服务能力，以满足不同时区客户的交易和自动化策略交易 外部用户有一定技术能力：外部API客户，一般是自己集成我们的API，实现自己的交易系统 交易系统API网关面临的问题问题1：流量的不断增加 如何合理控制流量，如何应对突发流量，怎么样最大程度的保障系统稳定，都是重要的问题。特别网关作为一个直接面对客户的系统，任何问题都会放大百倍。很多千奇百怪的重来没人遇到的问题都随时可能出现。 问题2：网关系统越来越复杂 现有的业务网关经过多年发展，里面有大量的业务嵌入，并且存在很多个不同的业务网关，相互之间没有任何关系，也没有沉淀出基础设施。 同时技术债务太多，系统里硬编码实现了全局性网关策略以及很多业务规则，导致维护成本较大。 问题3：API网关管理比较困难 海量并发下API的监控指标设计和数据的收集也是一个不小的问题。7x24小时运行的技术支持也导致维护成本较高。 问题4：推送还是拉取的选择 使用短连接还是长连接，REST API还是WebSocket？ 业务渠道较多（多个不同产品线的Web、App、API等形成十几个不同的渠道），导致用户的使用行为难以控制。 业务网关的设计与最佳实践API网关1.0我们的API网关1.0版本是多年前开发的，是直接使用OpenResty定制的，全局的安全测试、流量的路由转发策略、针对不同级别的限流等都是直接用Lua脚本实现。 这样就导致在经历了业务飞速发展以后，系统里存在了非常多的相同功能或不同功能的Lua脚本，每次上线或维护都需要找到影响的其中几个或几十个Lua脚本，进行策略调整，非常不方便，策略控制的粒度也不够细。 API网关2.0在区分了流量网关和业务网关以后，2017年开始实现了流量网关和业务网关的分离，流量网关继续使用OpenResty定制，只保留少量全局性，不经常改动的配置功能和对应的Lua脚本。 业务网关使用Vert.x实现的Java系统，部署在流量网关和后端业务服务系统之间，利用Vert.x的反应式编程能力和异步非阻塞IO能力、分布式部署的扩展能力，这样就初步解决了问题1和问题2。 图18 Vert.x是一个基于事件驱动和异步非阻塞IO、运行于JVM上的框架，如下图所示。在Vert.x里，Verticle是最基础的开发和部署单元，不同的Vert.x可以通过Event Bus传递数据，进而方便的实现高并发性能的网络程序。关于Vert.x原理的分析可以参考阿里宿何的blog：www.sczyh30.com/tags/Vert-x… 图19 Vert.x同时也很好的支持Websocket协议，所以可以方便的实现支持REST API和Websocket、完全异步的网关系统。 图20 一个高性能的API网关系统，缓存是必不可少的部分。无论是分发冷热数据，降低对业务系统的压力，还是作为中间数据源，为服务聚合提供高效可复用的业务数据，都发挥了巨大作用。 而一个优秀、高效的缓存系统，也必须是需要针对所承载的业务数据特点，进行特定设计和实现的。 图21 API网关的日常监控我们使用多种工具对API进行监控和管理，全链路访问跟踪、连接数统计分析、全世界重要国家和城市的波测访问统计。网关技术团队每时每刻都关注着数据的变化趋势。各个业务系统研发团队，每天安排专人关注自己系统的API性能，推进性能问题解决和持续优化。这就初步解决了问题3。 图22、23、24、25 推荐外部客户使用Websocket由于外部客户需要自己通过API网关调用API服务来集成业务服务能力到自己的系统。各个客户的技术能力和系统处理能力有较大差异，使用行为也各有不同。对于不断发展变动的交易业务数据，客户调用API频率太低则会影响数据实时性，调用频率太高则可能会浪费双方的系统资源。同时利用Websocket的消息推送特点，我们可以在网关系统控制客户接受消息的频率、单个用户的连接数量等，随时根据业务系统的情况动态进行策略调整。 综合考虑，Websocket是一个比REST API更加实时可靠，更加易于管理的方式。通过逐步协助和鼓励客户使用Websocket协议上，基本解决了问题4。 API网关的性能优化API网关系统作为API的统一接入点，为了给用户提供最优质的用户体验，必须长期做性能优化工作。 不仅API网关自己做优化，同时可以根据监控情况，时刻发现各业务系统的API服务能力，以此为出发点，推动各个业务系统不断优化API性能。 在此举一个具体的例子，某个网关系统发现连接经常剧烈抖动（如下图所示），严重影响系统的稳定性、浪费系统资源，经过排除发现： 有爬虫IP不断爬取我们的交易数据，且这些IP所在网段都没有在平台产生任何实际交易，最高单爬虫IP的每日新建连接近100万次，平均每秒10几次； 有部分API客户的程序存在bug，且处理速度有限，不断的断开并重新连接，尝试重新对API数据进行处理，严重影响了客户的用户体验。 针对如上分析，我们采取了几个处理方式： 对于每天认定的爬虫IP，加入黑名单，直接在流量网关限制其访问我们的API网关； 对于存在bug的API客户，协助对方进行问题定位和bug修复，增强客户使用信心； 对于处理速度和技术能力有限的客户，基于定制的Websocket服务，使用滑动时间窗口算法，在业务数据变化非常大时，对分发的消息进行批量优化； 对于未登录和识别身份的API调用，流量网关实现全局的流控策略，增加缓存时间和限制调用次数，保障系统稳定； 业务网关则根据API服务的重要等级和客户的分类，进一步细化和实时控制网关策略，最大程度保障核心业务和客户的使用。 优化前： 图26 优化后： 图27 对API网关的发展展望 图28 现有的API Gateway是以Vert.x为基础、结合业务自研的网关系统Gateway 2.0。 目前计划年底前基于Spring Cloud和Spring Cloud Gateway实现新一代微服务架构的网关系统Gateway 3.0。 计划整合了流量网关和业务网关、并增加了很多开箱即用功能组件的微服务架构网关，作为Apollo Gateway 1.0开源。 期待大家的共同参与。 参考文献及资料1、 案例，链接]]></content>
      <categories>
        <category>Api网关</category>
      </categories>
      <tags>
        <tag>Api网关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nohup命令的日志重定向总结]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-19-nohup%E5%91%BD%E4%BB%A4%E7%9A%84%E6%97%A5%E5%BF%97%E9%87%8D%E5%AE%9A%E5%90%91%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景第一部分 Linux shell的输入和输出一般情况下，每个 Unix/Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，默认从stdin读取数据。硬件为键盘。 标准输出文件(stdout)：stdout 的文件描述符为1，默认向stdout输出数据。硬件为屏幕。 标准错误文件(stderr)：stderr的文件描述符为2，会向stderr流中写入错误信息。硬件为屏幕。 Linux 程序在执行任何形式的 I/O 操作时，都是在读取或者写入一个文件描述符。一个文件描述符只是一个和打开的文件相关联的整数，它的背后可能是一个硬盘上的普通文件、FIFO、管道、终端、键盘、显示器，甚至是一个网络连接。 第二部分 重定向重定向的使用有如下规律： 1）标准输入0、输出1、错误2需要分别重定向，一个重定向只能改变它们中的一个。 2）标准输入0和标准输出1可以省略。（当其出现重定向符号左侧时） 3）文件描述符在重定向符号左侧时直接写即可，在右侧时前面加&amp;。 4）文件描述符与重定向符号之间不能有空格！ 2.1 输入重定向 标准输入重定向至文件： 12command &lt; filenamecommand 0&lt; filename 2.2 输出重定向 标准输出重定向至文件(覆盖)： 12command &gt; filenamecommand 1&gt; filename 标准输出重定向至文件(文件尾追加)： 12command &gt;&gt; filenamecommand 1&gt;&gt; filename 错误输出重定向至文件 12command 2&gt; filenamecommand 2&gt;&gt; filename dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。 参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Reindex实现Elasticsearch数据迁移]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-13-%E4%BD%BF%E7%94%A8Reindex%E5%AE%9E%E7%8E%B0Elasticsearch%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景在实际生产环境中，需要对线上Elasticsearch集群进行置换。业务上需要集群对外服务不中断的前提上，将原集群的数据迁移至新集群。目前迁移有多种方案，主要有：elasticsearch-dump、logstash、reindex、snapshot等方式。 elasticsearch-dump logstash snapshot reindex 基本原理 逻辑备份，类似mysqldump将数据一条一条导出后再执行导入 从一个 ES 集群中读取数据然后写入到另一个 ES 集群 从源 ES 集群通过备份api创建数据快照，然后在目标 ES 集群中进行恢复 reindex是Elasticsearch提供的一个api接口，可以把数据从一个集群迁移到另外一个集群 网络要求 网络需要互通 网络需要互通 无网络互通要求 网络需要互通 迁移速度 慢 一般 快 一般 运维配置复杂度 复杂，索引的分片数量和副本数量需要对每个索引单独进行迁移，或者直接在目标集群提前将索引创建完成，再迁移数据 复杂，需要提前在目标集群创建mapping和setting等，再迁移数据 简单 需要在目标ES集群中配置reindex.remote.whitelist参数，指明能够reindex的远程集群的白名单 适合场景 适用于数据量小的场景 适用于数据量一般，近实时数据传输 适用于数据量大，接受离线数据迁移的场景 本地索引更新Mapping实现索引层面迁移，或者跨集群的索引迁移 scroll query + bulk: 批量读取旧集群的数据然后再批量写入新集群，elasticsearch-dump、logstash、reindex都是采用这种方式 snapshot: 直接把旧集群的底层的文件进行备份，在新的集群中恢复出来，相比较scroll query + bulk的方式，snapshot的方式迁移速度最快。 https://cloud.tencent.com/developer/article/1611786 第一部分 Reindex接口介绍reindex 是 ES 提供的一个 api 接口，可以把数据从源 ES 集群导入到当前 ES 集群，实现集群内部或跨集群同步数据。 但仅限于腾讯云 ES 的实现方式（跨集群迁移需要elasticsearch.yml中加上ip白名单，并重启集群），所以腾讯云ES不支持 reindex 操作。具体见官方文档说明：https://www.elastic.co/guide/en/elasticsearch/reference/7.3/reindex-upgrade-remote.html 第三部分 Reindex迁移方案下面简单介绍 reindex 接口的使用方法： 1) 配置 elasticsearch.yml中的reindex.remote.whitelist 参数 需要在目标 ES 集群中配置该参数，指明能够 reindex 的远程集群的白名单。 2) 调用 reindex api 以下操作表示从源 ES 集群中查询名为 test1 的索引，查询条件为 title 字段为 elasticsearch，将结果写入当前集群的 test2 索引。 1234567891011121314151617POST _reindex &#123; "source": &#123; "remote": &#123; "host": "http://172.16.0.39:9200" &#125;, "index": "test1", "query": &#123; "match": &#123; "title": "elasticsearch" &#125; &#125; &#125;, "dest": &#123; "index": "test2" &#125; &#125; 从源索引中提取文档源，并将文档索引到目标索引中。可以将所有文档复制到目标索引，或为文档的子集重新索引。_reindex获取源索引的快照，但是其目标必须是其他索引，因此不会发生版本冲突。 第四部分 参数调优https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html https://www.cnblogs.com/Ace-suiyuan008/p/9985249.html https://elkguide.elasticsearch.cn/elasticsearch/api/reindex.html 参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Zeppelin安装部署介绍]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-19-Zeppelin%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Zeppelin</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark Mllib模块的学习总结]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-19-Spark%20Mllib%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景MLlib 是 Spark 的机器学习库，旨在简化机器学习的工程实践工作，并方便扩展到更大规模。 MLlib 由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的管道 API。 本节将对 Spark MLlib 进行简单介绍，在介绍数据挖掘算法时，将使用 Spark MLlib 提供的算法进行实例讲解。 Spark MLlib的构成Spark 是基于内存计算的，天然适应于数据挖掘的迭代式计算，但是对于普通开发者来说，实现分布式的数据挖掘算法仍然具有极大的挑战性。因此，Spark 提供了一个基于海量数据的机器学习库 MLlib，它提供了常用数据挖掘算法的分布式实现功能。 开发者只需要有 Spark 基础并且了解数据挖掘算法的原理，以及算法参数的含义，就可以通过调用相应的算法的 API 来实现基于海量数据的挖掘过程。 MLlib 由 4 部分组成：数据类型，数学统计计算库，算法评测和机器学习算法。 名称 说明 数据类型 向量、带类别的向量、矩阵等 数学统计计算库 基本统计量、相关分析、随机数产生器、假设检验等 算法评测 AUC、准确率、召回率、F-Measure 等 机器学习算法 分类算法、回归算法、聚类算法、协同过滤等 具体来讲，分类算法和回归算法包括逻辑回归、SVM、朴素贝叶斯、决策树和随机森林等算法。用于聚类算法包括 k-means 和 LDA 算法。协同过滤算法包括交替最小二乘法（ALS）算法。 Spark 机器学习库从 1.2 版本以后被分为两个包： spark.mllib包含基于RDD的原始算法API。Spark MLlib 历史比较长，在1.0 以前的版本即已经包含了，提供的算法实现都是基于原始的 RDD。 spark.ml 则提供了基于DataFrames 高层次的API，可以用来构建机器学习工作流（PipeLine）。ML Pipeline 弥补了原始 MLlib 库的不足，向用户提供了一个基于 DataFrame 的机器学习工作流式 API 套件。 使用 ML Pipeline API可以很方便的把数据处理，特征转换，正则化，以及多个机器学习算法联合起来，构建一个单一完整的机器学习流水线。这种方式给我们提供了更灵活的方法，更符合机器学习过程的特点，也更容易从其他语言迁移。Spark官方推荐使用spark.ml。如果新的算法能够适用于机器学习管道的概念，就应该将其放到spark.ml包中，如：特征提取器和转换器。开发者需要注意的是，从Spark2.0开始，基于RDD的API进入维护模式（即不增加任何新的特性），并预期于3.0版本的时候被移除出MLLib。 Spark在机器学习方面的发展非常快，目前已经支持了主流的统计和机器学习算法。纵观所有基于分布式架构的开源机器学习库，MLlib可以算是计算效率最高的。MLlib目前支持4种常见的机器学习问题: 分类、回归、聚类和协同过滤。下表列出了目前MLlib支持的主要的机器学习算法： Spark MLlib 的优势相比于基于 Hadoop MapReduce 实现的机器学习算法（如 Hadoop Manhout），Spark MLlib 在机器学习方面具有一些得天独厚的优势。 首先，机器学习算法一般都有由多个步骤组成迭代计算的过程，机器学习的计算需要在多次迭代后获得足够小的误差或者足够收敛时才会停止。如果迭代时使用 Hadoop MapReduce 计算框架，则每次计算都要读/写磁盘及完成任务的启动等工作，从而会导致非常大的 I/O 和 CPU 消耗。 而 Spark 基于内存的计算模型就是针对迭代计算而设计的，多个迭代直接在内存中完成，只有在必要时才会操作磁盘和网络，所以说，Spark MLlib 正是机器学习的理想的平台。其次，Spark 具有出色而高效的 Akka 和 Netty 通信系统，通信效率高于 Hadoop MapReduce 计算框架的通信机制。 在 Spark 官方首页中展示了 Logistic Regression 算法在 Spark 和 Hadoop 中运行的性能比较，可以看出 Spark 比 Hadoop 要快 100 倍以上。 MLlib(Machine Learnig lib) 是Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。Spark的设计初衷就是为了支持一些迭代的Job, 这正好符合很多机器学习算法的特点。在Spark官方首页中展示了Logistic Regression算法在Spark和Hadoop中运行的性能比较，如图下图所示。 第四部分 参数调优参考文献及资料1、Reindex from a remote cluster，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/reindex-upgrade-remote.html]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Anaconda相关信息汇总]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-13-Anaconda%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 参考文献及资料 背景登记Anaconda相关的下载信息，备用查阅。 anaconda 镜像下载地址 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=N&amp;O=D https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/?C=N&amp;O=D 官方首页 https://www.anaconda.com/distribution/ https://docs.conda.io/en/latest/miniconda.html 官方下载地址 https://repo.anaconda.com/archive/ https://repo.anaconda.com/miniconda/ 官方文档 https://docs.anaconda.com/anaconda/ old package lists https://docs.anaconda.com/anaconda/packages/oldpkglists/ release notes https://docs.anaconda.com/anaconda/reference/release-notes/ 参考文献及资料1、官网文档，链接：https://docs.anaconda.com/]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中命令别名设置]]></title>
    <url>%2F2020%2F12%2F12%2F2020-12-12-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%91%BD%E4%BB%A4%E5%88%AB%E5%90%8D%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 alias配置和取消 第二部分 alias查看 参考文献及资料 背景linux中提供用户自定义命名别名，即alias命令。 第一部分 alias配置和取消1.1 配置 临时设置 使用下面的命令格式： 1alias 新的命令='原命令 -选项/参数' 例如下面的例子中，我们将命令cd ..简化成..，方便使用。 1alias ..='cd ..' ​ 但是这种设置是临时在当前shell中生效了，重启开启新的shell就会失效。 永久生效 如果需要永久生效就需要将设置配置在环境变量中。需要注意的是环境变量有效范围（用户环境和系统环境变量）。例如系统环境变量中，在/etc/profile中追加： 12alias rm='rm –i'source /etc/profile 所有用户shell均具有该命令别名。 1.2 取消如果需要取消命令别名，可以使用下面的命令： 1unalias rm='rm –i' 对于配置在环境变量中的就需要手动注释，并source生效（当前shell）。 第二部分 alias查看如果需要参看当前shell环境已经配置的命令别名，可以直接使用命令： 12345678root@VM-0-5-ubuntu:~# aliasalias egrep='egrep --color=auto'alias fgrep='fgrep --color=auto'alias grep='grep --color=auto'alias l='ls -CF'alias la='ls -A'alias ll='ls -alF'alias ls='ls --color=auto' 上面是ubuntu系统自带的命令别名。另外比较常用的还有： 123alias ..='cd ..'alias ...='cd ../..'alias ....='cd ../../../' 日常操作中目录的进退是常用了，上面的命令别名大大提高了输入效率。 参考文献及资料1、Using alias Command in Linux to Improve Your Efficiency，链接：https://linuxhandbook.com/linux-alias-command/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Pyspark进行机器学习]]></title>
    <url>%2F2020%2F11%2F23%2F2020-12-24-%E4%BD%BF%E7%94%A8Pyspark%E8%BF%9B%E8%A1%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景配置和启动jupyter notebook。 12root@hadoop01:/opt# jupyter notebook --generate-configWriting default config to: /root/.jupyter/jupyter_notebook_config.py 123456789101112131415161718192021root@hadoop01:/opt# vi /root/anaconda3/share/jupyter/kernels/python3/kernel.json&#123; "argv": [ "/root/anaconda3/bin/python", "-m", "ipykernel_launcher", "-f", "&#123;connection_file&#125;" ], "display_name": "Python 3", "language": "python","env": &#123;"SPARK_HOME": "/opt/spark-2.3.2/","PYSPARK_PYTHON": "/root/anaconda3/bin/python","PYSPARK_DRIVER_PYTHON": "ipython3", "PYTHONPATH": "/opt/spark-2.3.2/python/:/opt/spark-2.3.2/python/lib/py4j-0.10.7-src.zip","PYTHONSTARTUP": "/opt/spark-2.3.2/python/pyspark/shell.py","PYSPARK_SUBMIT_ARGS": "--name pyspark --master local pyspark-shell"&#125;&#125; 123import pysparkfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate() 第一部分 数据的读取通常数据存储在csv文件中，需要使用pyspark进行读取。 1df = spark.read.option("header",True).csv("/data/test.csv") 如果是从hdfs文件系统读取： 1df = spark.read.options(header='True', delimiter=',').csv("hdfs://hadoop01:9000/data/test.csv") 可以使用show方法进行查看： 1df.show() 第二部分 数据清洗数据的清理，去除含有空字段的记录。 12345from pyspark.sql.functions import isnull, when, count, col# |user_id|age_range|gender|merchant_id|label|# 去除字段为空的异常数据df = df.filter(df.label.isNotNull()&amp;df.merchant_id.isNotNull()&amp;df.gender.isNotNull()&amp;df.age_range.isNotNull()&amp;df.user_id.isNotNull()) 另外还有些时候我们需要替换指定条件的。比如替换空值。 123# 使用数字0替换null字段df = df.na.fill(value=0).show()df = df.fillna(value=0,subset=["population"]).show() 对于一些不需要的列进行去除。 123# 去除列df = df.drop('user_id')df.show() 列值转换。 12345678910111213141516171819def encode_columns(df, col_list): indexers = [ StringIndexer(inputCol=c, outputCol=f'&#123;c&#125;_indexed').setHandleInvalid("keep") for c in col_list ] encoder = OneHotEncoderEstimator( inputCols = [indexer.getOutputCol()) for index in indexers]) #.setDropLast(False) newColumns = [] for f in col_list: colMap = df.select(f'&#123;f&#125;', f'&#123;f&#125;_indexed').distinct().rdd.collectAsMap() colTuple = sorted( (v, f'&#123;f&#125;_&#123;k&#125;') for k,v in colMap.items()) newColumns.append(v[1] for v in colTuple) pipeline = Pipeline(stages =indexers + [encoder]) piped_encoder = pipeline.fit(df) encoded_df = piped_encoder.transfrom(df) return piped_encoder, encoded_df, newColumnsdf = encode_columns(df, ['user_id']) 特征数据向量化。 12345678# Assemble all the features with VectorAssemblerfrom pyspark.ml.feature import VectorAssemblerrequired_features = ['age_range','gender','merchant_id']assembler = VectorAssembler(inputCols=required_features, outputCol='features')df = assembler.transform(df)df.show() 切分数据集： 1234567#Decide on the split between training and testing data from the dataframetrainingFraction = 0.7testingFraction = (1-trainingFraction)seed = 1234# Split the dataframe into test and training dataframestraining_data, test_data = df.randomSplit([trainingFraction, testingFraction], seed=seed) 第三部分 模型训练1234567from pyspark.ml.classification import RandomForestClassifierrf = RandomForestClassifier(labelCol='label', featuresCol='features', maxDepth=5)model = rf.fit(training_data)rf_predictions = model.transform(test_data) 第四部分 模型预测评分1234from pyspark.ml.evaluation import MulticlassClassificationEvaluatormulti_evaluator = MulticlassClassificationEvaluator(labelCol = 'label', metricName = 'accuracy')print('Random Forest classifier Accuracy:', multi_evaluator.evaluate(rf_predictions)) 第五部分 模型应用数据准备： 12df.write.option("header",True).csv("/data/output.csv")df.write.option("header",True).csv("hdfs://hadoop01:9000/data/output") 参考文献及资料1、 案例，链接：https://hackernoon.com/building-a-machine-learning-model-with-pyspark-a-step-by-step-guide-1z2d3ycd 2、案例，链接：https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-machine-learning-mllib-notebook 3、pyspark介绍，链接：https://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
      <tags>
        <tag>pyspark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将文件数据导入hive表中]]></title>
    <url>%2F2020%2F11%2F23%2F2020-12-23-%E5%A6%82%E4%BD%95%E5%B0%86%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive%E8%A1%A8%E4%B8%AD%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景使用head命令看一下。 第一步 创建库和hive表使用下面的命令进入hive shell交互模式。 1root@hadoop01:/opt/hive/bin/#hive 创建库： 1CREATE database cda; 创建表： 12345678910111213CREATE TABLE IF NOT EXISTS cda.users (user_id string,item_id string,cat_id string,merchant_id string,brand_id string,month string,day string,action string,age_range string,gender string,province string )COMMENT 'user_log.csv Table' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' tblproperties("skip.header.line.count"="1"); 如果需要分区： 1234567891011CREATE TABLE IF NOT EXISTS cda.users (user_id string,item_id string,cat_id string,merchant_id string,brand_id string,action string,age_range string,gender string,province string )PARTITIONED BY(month string,day string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' tblproperties("skip.header.line.count"="1"); 第二部分 准备数据和导入将数据上传hdfs： 1root@hadoop03:/opt/data# hdfs dfs -put user_log.csv /data/user_log.csv 导入数据： 1LOAD DATA INPATH 'hdfs:///data/user_log.csv' INTO TABLE cda.users; 第三部分 查询1234567891011root@hadoop01:/opt/hive/bin# hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hbase-1.4.13/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/opt/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive&gt; select * from cda.users limit 10; 回显数据，完成导入。 参考文献及资料1、 hive-load-csv-file-into-table，链接:https://sparkbyexamples.com/apache-hive/hive-load-csv-file-into-table/]]></content>
      <categories>
        <category>hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn上长任务报Token失效问题总结(Invalid AMRMToken)]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-28-ubuntu%E6%90%AD%E5%BB%BAhadoop%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://cloud.tencent.com/developer/article/1350441 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Unix和Windows中文本行末结束符]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-12-Unix%E5%92%8CWindows%E4%B8%AD%E6%96%87%E6%9C%AC%E8%A1%8C%E6%9C%AB%E7%BB%93%E6%9D%9F%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回车和换行 第二部分 兼容性问题解决 参考文献及资料 背景或许你遇到过这样的坑。当你信心满满将自己编写的程序文件或配置文件上传到生产环境（linux），却发现无法运行或者生效。但是明明在本地（Windows）测试运行没有问题。那么很大几率遇到文本行末结束符的坑。 Unix和Windows中文本行末结束符是不同的。本文将详细讲解这个坑的背景、产生原因和解决办法。 第一部分 回车和换行1.1 历史背景关于回车（Carriage Return）和换行（Line Feed）的来历，需要从英文打字机讲起。在机械英文打字机中有个叫“字车”的部件，每打印一个英文字符，“字车”前进一位。但是纸张每行是有限制的，打满一行后，“字车”需要重置回到起始位置。从机械角度看，打印机有两个响应动作：（1）“字车”归位；（2）纸张的滚筒上卷一行，开始新的一行。这里“字车”归位就是“回车”，而滚筒上卷一行就是“换行”。 到了电传打印机时候，需要使用控制字符来通知打印机执行非打印操作的指令（回车（CR）和换行（NL））。而在ASCII码中分别使用\r（值13）和\n（值10）表示。 再后来计算机发明后，两个概念也就被搬到计算机中（计算机通常需要和打印机交互，保持兼容性）。考虑到当时存储资源的昂贵，就有人提出来文本中使用两个操作符表示行末结束较为浪费，于是分歧就产生了。 1.2 分歧目前主流操作系统（Unix、Windows、Mac）中分歧如下： 操作系统 系统行末结束符 备注 UNIX \n window \n\r MAC OS \n v9 之前 Mac OS 用 ‘\r’ 注：从2001年3月发布的Mac OS 10.0开始，系统行末结束符采用”\n”。 这种分歧就导致不同操作系统之间兼容问题。 第二部分 兼容性问题解决这种兼容问题通常发生在：不同操作系统之间传输纯文本文件。 Unix/Mac系统创建的文件在Windows里打开，文字会变成一行。因为没有\r。 Windows里的文件在Unix/Mac下打开的话，在每行会多出一个^M符号。多了\r。 2.1 Windows文件上传Linux问题我们经常遇到的问题是：Windows下编写的Shell脚本或者Python脚本，放到Linux下执行会出错。通常上传文件前，使用UE（Ultraedit）或者Nodepad++来转换。 UE中，执行“File-&gt;conversions-&gt;Dos to Unix”，将文件中\n\r转换成\n。 Nodepad++中，执行“编辑-&gt;档案格式转换-&gt;转换为UNIX格式”。 上传到Linux后还可以使用下面的命令查看是否准确： 1cat -v test.txt 如果每行换行处都有^M，这说明仍然是Windows下的文本文件。 注：cat -A命令：显示不可见字符。如换行符显示为“$”，TAB 显示为^I等。在这种模式下，回车（\r）字符将显示为^M 可以使用下面的命令进行统一替换： 1sed -i 's/^M$//g' test.txt 另外还有专用命令dos2unix(如果操作系统没有改命令需要安装一下)。 1dos2unix test.txt 最后还可以使用vi打开文件，输入:set fileformat=unix，回车。最后保存退出即可。 2.2 Linux文件上传Windows问题在Windows中使用Ultraedit或者Nodepad++文本编辑器查看Linux文件，而不是系统自带的记事本。 2.3 FTP中传输问题FTP软件在传输文件的时候，通常有两种模式：文本模式（ASCII模式）和二进制模式（BINARY模式）。两种模式的区别就是行末结束符的处理，BINARY模式不会对数据进行任何处理。而ASCII模式将行末结束符转换为本机操作系统的行末结束符。例如Windows系统将文本文件上传至Linux，就会将\r\n替换成\n。 在使用过程中需要注意两种模式的差别。特别的上实际生产环境上线投产过程中建议统一使用二进制模式，避免FTP对文件进行转换。 2.4 编程语言中 Python 使用 “Universal Newline“ 处理这个问题。文本使用 open() 方法打开时，会对行末结束符进行识别并一致处理成 ‘\n’，在文件写入的时候，使用 write(‘\n’) 即可，Python 会根据当前程序执行的操作系统自动处理。 Java中行末结束符使用下面的函数方法统一处理： 1System.getProperty("line.separator") 参考文献及资料1、Line_feed，链接：https://nl.wikipedia.org/wiki/Line_feed]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn上长任务报Token失效问题总结(Invalid AMRMToken)]]></title>
    <url>%2F2020%2F11%2F12%2F2020-12-11-%E5%A6%82%E4%BD%95%E4%BB%8Ehive%E8%A1%A8%E4%B8%AD%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景\4. hive 从非分区表插入数据到分区表时出错： Cannot insert into target table because column number/types are different ‘’分区名’’: Table insclause-0 has 2 columns, but query has 3 columns. 首先解释一下这个错误：因为hive的分区列是作为元信息存放在mysql中的，他们并不在数据文件中，相反他们以子目录的名字被使用，因此你的分区表实际含有的数据列，注意是数据列是不包含分区列的，所以在你向分区表插入数据时，不能插入分区列； 举个简单的例子：体育课上站队时，老师经常让男生、女生各站一队，你觉得有必要再给他们每一个人加上一个性别的标签吗？ 下面是stackoverflow上大神关于这个问题的解释： *\*In Hive the partitioning “columns” are managed as** **metadata** **&gt;&gt; they are not included in the data files, instead they are used as sub-directory names. So your partitioned table has just 2 real columns, and you must feed just 2 columns with your SELECT.**** 比如你的非分区表non_part的内容如下： 123id name sex1 tom M2 mary F 假如你的分区表part是以sex来分区的，当你想把以上非分区表中的数据插入到分区表中： 你应该：insert into table part partition(sex=’M’) select id,name from non_part where sex=’M’;（对，你要插入的就是两列，分区列不作为数据保存在数据表中） \而不是\*：****insert into table part partition(sex=’M’) select \ from non_part where sex=’M’;**** 另外需要注意的地方： 1.从非分区表插入数据到分区表时hive会将HiveQL转换为MR来执行的,官方提示Hive-on-MR在将来可能不再被支持； 2.只要是向分区表内装数据，无论是load还是insert都要在表名后指明分区名；而且load时，会将你要load的文件内的所有内容放在指定的分区下； https://www.cnblogs.com/lemonu/p/11279979.html 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yarn上长任务报Token失效问题总结(Invalid AMRMToken)]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-28-Yarn%E4%B8%8A%E9%95%BF%E4%BB%BB%E5%8A%A1%E6%8A%A5Token%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93(Invalid%20AMRMToken)%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://www.shuzhiduo.com/A/xl5600X0zr/ http://flume.cn/2016/11/24/Spark%E8%B8%A9%E5%9D%91%E4%B9%8BStreaming%E5%9C%A8Kerberos%E7%9A%84hadoop%E4%B8%ADrenew%E5%A4%B1%E8%B4%A5/ http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/ https://www.shuzhiduo.com/A/xl5600X0zr/ 参考文献及资料1、 Apache Spark support，链接]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS Federation架构介绍]]></title>
    <url>%2F2020%2F11%2F12%2F2020-11-28-HDFS%20Federation%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景http://dongxicheng.org/mapreduce/hdfs-federation-introduction/ 参考文献及资料1、 Apache Spark support，链接：http://dongxicheng.org/mapreduce/hdfs-federation-introduction/]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive的分区和分桶总结]]></title>
    <url>%2F2020%2F11%2F05%2F2020-11-05-Hive%E7%9A%84%E5%88%86%E5%8C%BA%E5%92%8C%E5%88%86%E6%A1%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://data-flair.training/blogs/hive-partitioning-vs-bucketing/ https://data-flair.training/blogs/hive-partitioning-vs-bucketing/ https://blog.csdn.net/whdxjbw/article/details/82219022 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark和oracle交互总结]]></title>
    <url>%2F2020%2F11%2F05%2F2020-11-05-Spark%E5%92%8Coracle%E4%BA%A4%E4%BA%92%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景https://issues.apache.org/jira/browse/SPARK-10909 Spark SPARK-10909 Spark sql jdbc fails for Oracle NUMBER type columns https://blog.csdn.net/cuichunchi/article/details/107838633 https://stackoverflow.com/questions/34067124/the-java-lang-illegalargumentexception-requirement-failed-overflowed-precisio 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[云服务器部署WordPress介绍]]></title>
    <url>%2F2020%2F10%2F31%2F2020-10-31-%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2WordPress%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景第一部分 环境部署笔者购买了腾讯云的云主机，操作系统环境为：Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-88-generic x86_64)。主机具有互联网环境，所以依赖组件的安装将依赖在线安装。安装用户使用root用户。首先更新操作系统： 1apt-get update 1.1 部署Apache环境使用apt安装： 1apt install apache2 使用下面的命令检查进程状态： 123456789101112131415root@VM-0-5-ubuntu:/apphome# systemctl status apache2● apache2.service - The Apache HTTP Server Loaded: loaded (/lib/systemd/system/apache2.service; enabled; vendor preset: enabled) Drop-In: /lib/systemd/system/apache2.service.d └─apache2-systemd.conf Active: active (running) since Sun 2020-11-01 12:26:24 CST; 1min 9s ago Main PID: 704 (apache2) Tasks: 55 (limit: 2122) CGroup: /system.slice/apache2.service ├─704 /usr/sbin/apache2 -k start ├─706 /usr/sbin/apache2 -k start └─707 /usr/sbin/apache2 -k startNov 01 12:26:23 VM-0-5-ubuntu systemd[1]: Starting The Apache HTTP Server...Nov 01 12:26:24 VM-0-5-ubuntu systemd[1]: Started The Apache HTTP Server. 浏览器地址栏： 1http://主机公网IP/ 可以看到默认的Apache 欢迎页面。 1.2 部署Mysql环境1.3 部署PHP环境第二部分 部署和配置WordPresshttps://www.cnblogs.com/jiangfeilong/p/11142181.html https://cloud.tencent.com/developer/article/1627432#:~:text=%20%E5%A6%82%E4%BD%95%E5%9C%A8%20Ubuntu%2020.04%20%E4%B8%8A%E5%AE%89%E8%A3%85%20Apache%20%201,5%20%E4%BA%94%E3%80%81%E8%AE%BE%E7%BD%AE%E4%B8%80%206%20%E8%99%9A%E6%8B%9F%E4%B8%BB%E6%9C%BA%207%20%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%20More%20 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html]]></content>
      <categories>
        <category>WordPress</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive的Metastore介绍]]></title>
    <url>%2F2020%2F10%2F27%2F2020-10-27-Hive%E7%9A%84Metastore%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景Hive Metastore，也称为HCatalog，是一个关系数据库存储库，其中包含有关您在Hive中创建的对象的元数据。创建Hive表时，表定义（列名，数据类型，注释等）存储在Hive Metastore中。这是自动的，只是Hive架构的一部分。Hive Metastore之所以如此重要，是因为它充当中央架构存储库，可供其他访问工具（如Spark和Pig）使用。此外，通过Hiveserver2，您可以使用ODBC和JDBC连接访问Hive Metastore。这将为可视化工具（如PowerBi或Tableau）打开架构。 https://www.infoq.cn/article/uM7TSwszJlsvv7veixga https://bbs.huaweicloud.com/forum/viewthreaduni-66881-filter-reply-orderby-lastpost-page-7-1.html https://blog.csdn.net/lalaguozhe/article/details/9070203 https://www.infoq.cn/article/lXJisUVTgOjgHzRMSIBW https://www.codeobj.com/2019/01/hive-metastore%e5%b8%b8%e7%94%a8%e7%9a%84%e5%85%83%e6%95%b0%e6%8d%ae%e5%9c%a8mysql%e4%b8%ad%e5%af%b9%e5%ba%94%e7%9a%84%e8%a1%a8/ Hive Metastore Federation 在滴滴的实践 https://blog.didiyun.com/index.php/2019/03/25/hive-metastore-federation/ 网易杭研大数据实践：Apache Hive稳定性测试 https://dun.163.com/news/p/83abc4931c1349b086c73dfbad0fb57f 如何使用带有大量SPARK分区的HIVE表 https://andr83.io/en/1090/ 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark和Elasticsearch交互实践总结]]></title>
    <url>%2F2020%2F10%2F13%2F2020-10-13-Spark%E5%92%8CElasticsearch%E4%BA%A4%E4%BA%92%E5%AE%9E%E8%B7%B5%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境依赖 第二部分 交互接口 第三部分 任务提交 参考文献及资料 背景为了更好的支持Spark应用和Elasticsearch交互，Elasticsearch官方推出了elasticsearch-hadoop项目。本文将详细介绍Spark Java应用和Elasticsearch的交互细节。 第一部分 环境依赖1.1 配置Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-spark-13_2.11&lt;/artifactId&gt; &lt;version&gt;6.8.2&lt;/version&gt;&lt;/dependency&gt; 需要注意Spark版本和elasticsearch-hadoop版本的兼容性，参考版本对照表： Spark Version Scala Version ES-Hadoop Artifact ID 1.0 - 1.2 2.10 1.0 - 1.2 2.11 1.3 - 1.6 2.10 elasticsearch-spark-13_2.10 1.3 - 1.6 2.11 elasticsearch-spark-13_2.11 2.0+ 2.10 elasticsearch-spark-20_2.10 2.0+ 2.11 elasticsearch-spark-20_2.11 1.2 Spark配置关于elasticsearch集群的交互配置，定义在SparkConf中，例如下面的案例： 123456789import org.apache.spark.SparkConf;SparkConf sparkConf = new SparkConf().setAppName("JavaSpark").setMaster("local");//config elasticsearchsparkConf.set("es.nodes","192.168.31.3:9200");sparkConf.set("es.port","9200");sparkConf.set("es.index.auto.create","true");JavaSparkContext jsc = new JavaSparkContext(sparkConf); es.nodes，集群节点； es.port，服务端口； es.index.auto.create，参数指定index是否自动创建； 其他配置参考官方文档：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html 第二部分 交互接口2.1 自定义id的写入在业务数据写入elasticsearch集群的时候，需要数据去重。这时候就需要自己指定元数据字段中的_id。elasticsearch在处理_id相同的数据时，会覆盖写入。例如下面的例子： 1234567891011121314151617181920212223242526272829303132import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaSparkContext;import org.elasticsearch.spark.rdd.Metadata;import org.elasticsearch.spark.rdd.api.java.JavaEsSpark;import scala.Tuple2;import java.util.ArrayList;import java.util.HashMap;import static org.elasticsearch.spark.rdd.Metadata.ID;try&#123; ArrayList&lt;Tuple2&lt;HashMap,HashMap&gt;&gt; metaList = new ArrayList&lt;&gt;(); for(int i=0;i&lt;100;i++) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("id", String.valueOf(i)); map.put("name", "one"); HashMap&lt;Metadata, String&gt; metamap = new HashMap&lt;Metadata, String&gt;(); metamap.put(ID, String.valueOf(i)); metaList.add(new Tuple2(metamap, map)); &#125; JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs(metaList); JavaEsSpark.saveToEsWithMeta(pairRdd,"spark/doc"); &#125;catch (Exception e)&#123; e.printStackTrace(); System.out.println("finish!"); jsc.stop(); &#125; 例子中我们使用ArrayList&lt;Tuple2&lt;HashMap,HashMap&gt;&gt;数据结构来存储待写入的数据，然后构造RDD，最后使用JavaEsSpark.saveToEsWithMeta方法写入。需要注意这里构造的两个HashMap: 数据HashMap，数据结构为：HashMap&lt;String, String&gt;，用于存储数据键值对。 元数据HashMap，数据结构为：HashMap&lt;Metadata, String&gt;，用于存储元数据键值对。例如ID即为_id。 其他类型读写可以参考官方网站：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 第三部分 任务提交最后编译运行。主要是setMaster()指定运行方式，分为如下几种。 运行模式 说明 local Run Spark locally with one worker thread (i.e. no parallelism at all). local[K] Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). local[*] Run Spark locally with as many worker threads as logical cores on your machine. spark://HOST:PORT Connect to the given Spark standalone cluster master. The port must be whichever one your master is configured to use, which is 7077 by default. mesos://HOST:PORT Connect to the given Mesos cluster. The port must be whichever one your is configured to use, which is 5050 by default. Or, for a Mesos cluster using ZooKeeper, use mesos://zk://.... To submit with --deploy-mode cluster, the HOST:PORT should be configured to connect to the MesosClusterDispatcher. yarn Connect to a YARN cluster in client or cluster mode depending on the value of --deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable. yarn-client Equivalent to yarn with --deploy-mode client, which is preferred to yarn-client yarn-cluster Equivalent to yarn with --deploy-mode cluster, which is preferred to yarn-cluster 除了在eclipse、Intellij中运行local模式的任务，也可以打成jar包，使用spark-submit来进行任务提交。 参考文献及资料1、 Apache Spark support，链接：https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html 2、elasticsearch-hadoop项目，链接：https://github.com/elastic/elasticsearch-hadoop]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中的递归和限制]]></title>
    <url>%2F2020%2F10%2F10%2F2020-10-10-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E7%9A%84%E9%80%92%E5%BD%92%E5%92%8C%E9%99%90%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 参考文献及资料 背景参考文献及资料1、YARN Application Security，链接：https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python中局部函数]]></title>
    <url>%2F2020%2F10%2F10%2F2020-12-13-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E4%B8%AD%E5%B1%80%E9%83%A8%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 参考文献及资料 背景通过前面的学习我们知道，Python 函数内部可以定义变量，这样就产生了局部变量，有读者可能会问，Python 函数内部能定义函数吗？答案是肯定的。Python 支持在函数内部定义函数，此类函数又称为局部函数。 那么，局部函数有哪些特征，在使用时需要注意什么呢？接下来就给读者详细介绍 Python 局部函数的用法。 首先，和局部变量一样，默认情况下局部函数只能在其所在函数的作用域内使用。举个例子： 1#全局函数def outdef (): #局部函数 def indef(): print("http://c.biancheng.net/python/") #调用局部函数 indef()#调用全局函数outdef() 程序执行结果为： http://c.biancheng.net/python/ 就如同全局函数返回其局部变量，就可以扩大该变量的作用域一样，通过将局部函数作为所在函数的返回值，也可以扩大局部函数的使用范围。例如，修改上面程序为： 1#全局函数def outdef (): #局部函数 def indef(): print("调用局部函数") #调用局部函数 return indef#调用全局函数new_indef = outdef()调用全局函数中的局部函数new_indef() 程序执行结果为： 调用局部函数 因此，对于局部函数的作用域，可以总结为：如果所在函数没有返回局部函数，则局部函数的可用范围仅限于所在函数内部；反之，如果所在函数将局部函数作为返回值，则局部函数的作用域就会扩大，既可以在所在函数内部使用，也可以在所在函数的作用域中使用。 以上面程序中的 outdef() 和 indef() 为例，如果 outdef() 不将 indef 作为返回值，则 indef() 只能在 outdef() 函数内部使用；反之，则 indef() 函数既可以在 outdef() 函数内部使用，也可以在 outdef() 函数的作用域，也就是全局范围内使用。 有关函数返回函数，更详细的讲解，可阅读《Python函数高级方法》一节。 另外值得一提的是，如果局部函数中定义有和所在函数中变量同名的变量，也会发生“遮蔽”的问题。例如： 1#全局函数def outdef (): name = "所在函数中定义的 name 变量" #局部函数 def indef(): print(name) name = "局部函数中定义的 name 变量" indef()#调用全局函数outdef() 执行此程序，Python 解释器会报如下错误： UnboundLocalError: local variable ‘name’ referenced before assignment 此错误直译过来的意思是“局部变量 name 还没定义就使用”。导致该错误的原因就在于，局部函数 indef() 中定义的 name 变量遮蔽了所在函数 outdef() 中定义的 name 变量。再加上，indef() 函数中 name 变量的定义位于 print() 输出语句之后，导致 print(name) 语句在执行时找不到定义的 name 变量，因此程序报错。 由于这里的 name 变量也是局部变量，因此前面章节讲解的 globals() 函数或者 globals 关键字，并不适用于解决此问题。这里可以使用 Python 提供的 nonlocal 关键字。 例如，修改上面程序为： 1#全局函数def outdef (): name = "所在函数中定义的 name 变量" #局部函数 def indef(): nonlocal name print(name) #修改name变量的值 name = "局部函数中定义的 name 变量" indef()#调用全局函数outdef() 程序执行结果为： 所在函数中定义的 name 变量 参考文献及资料1、YARN Application Security，链接：https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MapReduce on Yarn机制总结]]></title>
    <url>%2F2020%2F10%2F06%2F2020-10-06-MapReduce%20on%20Yarn%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景Yarn作为资源统一管理平台是从第一代MapReduce（MRv1）演进而来。在最初的MRv1架构中，主要构成有三部分： 编程模型，MapReduce API； 资源管理和作业控制模块，JobTracker（作业跟踪器）、TaskTracker（任务跟踪器）； 数据处理引擎，MapTask、ReduceTask； 其中JobTracker单一服务运行在主节点，负责调度集群所有的任务，并监控任务的状态。每个工作节点均运行一个TaskTracker负责执行JobTracker分发的任务。架构上不足有： JobTracker服务单点故障风险； JobTracker服务太重，承担了集群的资源管理和任务调度； JobTracker承担了集中式调度器角色，将集群中资源分配、任务分配和监控一个人扛，在高并发场景下容易出现负载过重，产生单点风险。 另外TaskTracker将资源强制划分为Map task slot和Reduce task slot，并且map task和reduce task只能使用对应的solts资源。当任务的map和reduce任务数量发生倾斜的时候，就会出现计算资源使用低效。 集群中solts资源由两个参数决定：（mapred.tasktracker.map.tasks.maximum、mapred.tasktracker.reduce.tasks.maximum）。 Yahoo和Hortonworks于2012年在Hadoop 2.0版中引入了YARN，架构上将JobTracker功能进行了拆分。将调度工作拆解为两层：中央调度器和计算框架调度器。中央调度器管理集群中所有资源的状态，它掌握整个集群的资源信息，按照一定的调度策略（如：FIFO、Fair、Capacity、Delay、Dominant Resource Fair等）将资源粗粒度地分配给计算框架调度器，各个框架收到资源后再根据作业特性细粒度地将资源分配给执行器执行具体的计算任务。双层调度架构大大减轻了中央调度器的负载，提升了集群并发性能和资源利用率。 新的架构就是Yahoo发布的Yarn架构（Yet Another Resource Negotiator）。原架构中的JobTarcker被拆解为： ResourceManger，集群管理器。负责整个集群的资源管理和调度，承担中央调度器角色。ResourceManager有两个组件构成： 应用程序管理器（Application Manager简称ASM）。主要负责：1.管理监控各个系统的应用，包括启动ApplicaitonMaster，监控ApplicaitonMaster运行状态; 2.跟踪分配给ApplicationMaster的进度和状态。 调度器(Scheduler)。调度器根据资源容量、队列等限制条件（如队列资源和队列任务数量限制），将集群资源分配给正在运行的应用程序。即分配资源容器（Resource Container）给ApplicaitonMaster，分配单位Resource Container将CPU和内存资源封装。调度器是一个可配置可插拔的组件，用户可以自己设计新的调度器，Yarn提供多个可选项（Fair Scheduler和Capacity Scheduler等）。 ApplicationMaster，负责应用任务内部的调度和监控，承担框架调度器角色。 用户提交的每个应用都会对应一个ApplicationMaster，主要负责监控应用，任务容错（重启失败的task）等。它同时会和ResourceManager和NodeManager有交互，向ResourceManager申请资源，请求NodeManager启动或停止task。 Yarn（MRv2）架构上已经不单单支持运行MapReduce任务，架构解耦后可以支持Tez、Spark、Storm、Flink等多种计算框架的运行。本文将详细阐述MapReduce任务在新架构Yarn上运行细节。 第一部分 角色说明MapReduce on Yarn运行模式下，主要涉及下面的角色： ResourceMange，中央调度器角色； NodeManagers，节点管理器，负责管理Container; MapReduce ApplicationManager，框架调度器角色。负责协调运行MapReduce任务。 整个调度过程如下： 第二部分 任务提交第三部分 任务初始化第四部分 任务分配第五部分 任务执行第六部分 任务监控第七部分 任务完成第八部分 总结参考文献及资料1、Hadoop 新 MapReduce 框架 Yarn 详解，链接：https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/ 2、Hadoop: Writing YARN Applications，链接：https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html 3、MapReduce Tutorial，链接：https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[监控Yarn资源调度平台资源状态]]></title>
    <url>%2F2020%2F10%2F06%2F2020-10-06-%E7%9B%91%E6%8E%A7Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E8%B5%84%E6%BA%90%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Yarn状态数据接口 第二部分 Java实现 第三部分 总结 参考文献及资料 背景目前国内大部分企业级的大数据平台资源调度系统都是基于Yarn集群。生产环境上，各种大数据计算框架运行在Yarn上，就需要对Yarn平台的资源情况进行实时监控。虽然Yarn本身提供一个Web管理界面展示平台资源使用情况，但是这些运行状态数据需要实时获取和监控。随着智能化运维推进，需要对监控数据能实时分析、异常检测、自动故障处理。这些场景都需要能实时获取到Yarn平台的状态监控数据。 本文将详细介绍各种监控实现的方法，并重点介绍Java实现。 第一部分 Yarn状态数据接口1.1 命令行方式yarn命令在{hadoop_home}/bin路径下，对于部署hadoop客户端的客户端需要加载命令环境变量。 参看任务信息 1234# 查看所有任务信息yarn application -list# 查看正在运行的任务信息（带过滤参数appStates）yarn application -list -appStates RUNNING 这里参数appStates的状态有：ALL,NEW,NEW_SAVING,SUBMITTED,ACCEPTED,RUNNING,FINISHED,FAILED,KILLED 另外还可以指定计算框架的类型，例如： 12# 参看所有MapReduce任务yarn application -list -appTypes MAPREDUCE 参看指定任务状态信息 1yarn application -status application_1575989345612_32134 1.2 Restful Api接口ResourceManager允许用户通过REST API获取有关群集的信息：群集上的状态、群集上的指标、调度程序信息，另外还有群集中节点的信息以及集群上应用程序的运行信息。 查询整个集群指标 1GET http://http address:port/ws/v1/cluster/metrics 查询集群调度器详情 1GET http://http address:port/ws/v1/cluster/scheduler 监控任务 12curl http://http address:port/ws/v1/cluster/apps/stateGET http://http address:port/ws/v1/cluster/apps/state 查看指定任务 1GET http://http address:port/ws/v1/cluster/apps/ 查看指定任务的详细信息 1curl http://http address:port/proxy/ws/v2/mapreduce/info 杀死任务 yarn application -kill application_id 12curl -v -X PUT -d '&#123;"state": "KILLED"&#125;' http://http address:port&gt;/ws/v1/cluster/apps/PUT http://http address:port/ws/v1/cluster/apps/state 1.2 JMX Metrics监控首先需要开启jmx，编辑{hadoop_home}/etc/hadoop/yarn-env.sh配置文件，最后天下下面三行配置： 123YARN_OPTS="$YARN_OPTS -Dcom.sun.management.jmxremote.authenticate=false"YARN_OPTS="$YARN_OPTS -Dcom.sun.management.jmxremote.port=8001"YARN_OPTS="$YARN_OPTS -Dcom.sun.management.jmxremote.ssl=false" 其中8001是服务监听端口。jmx提供了Cluster、Queue、Jvm、FSQueue等Metrics信息。 1234# 获取YARN相关的jmxhttp://http address:8088/jmx# 如果想获取NameNode的jmxhttp://http address:50070/jmx 上面的方式会获取服务所有的信息(json格式)。如果需要精准获得准确信息，org.apache.hadoop.jmx.JMXJsonServlet类支持三个参数：callback、qry、get。其中qry用于过滤，下面的url用于查询Yarn上spark用户在default队列上任务信息。 1http://192.168.1.2:8088/jmx?qry=Hadoop:service=ResourceManager,name=QueueMetrics,q0=root,q1=default,user=spark 更详细的信息参考官网：https://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/jmx/JMXJsonServlet.html 1.3 Python Api接口对于Python有第三方包支持和yarn进行交互，github地址为:https://github.com/CODAIT/hadoop-yarn-api-python-client 案例代码： 1234567from yarn_api_client import ApplicationMaster, HistoryServer, NodeManager, ResourceManagerrm = ResourceManager(address='192.168.1.2', port=8088)# 获取到ResourceManager的所有apps的信息rm.cluster_applications().data# 获取到ResourceManager的具体任务的信息rm.cluster_application('application_1437445095118_265798').data 对于Hadoop安全集群，还需要部署认证包requests_kerberos。具体可以参考说明文档：https://python-client-for-hadoop-yarn-api.readthedocs.io/en/latest/index.html 第二部分 Java实现2.1 maven依赖根据Hadoop的版本添加下面的依赖包： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-api&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt;&lt;/dependency&gt; 2.2 接口实现我们将相关配置文件放在resources/conf路径下面，涉及的文件有： 1234567# 集群配置文件core-site.xmlhdfs-site.xmlyarn-site.xml# 安全认证文件user.keytabkrb5.conf 下面是案例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122package com.main.yarnmonitor;import com.google.common.collect.Sets;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.security.UserGroupInformation;import org.apache.hadoop.yarn.api.records.ApplicationReport;import org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport;import org.apache.hadoop.yarn.api.records.ContainerReport;import org.apache.hadoop.yarn.api.records.YarnApplicationState;import org.apache.hadoop.yarn.client.api.YarnClient;import org.apache.hadoop.yarn.exceptions.YarnException;import java.io.File;import java.io.IOException;import java.util.EnumSet;import java.util.List;import java.util.Set;/** * @program: yarnmonitor * @description: * @author: rongxiang * @create: 2020-03-27 16:44 **/public class yarnmonitor &#123; //配置文件路径 private static String confPath = Thread.currentThread().getContextClassLoader().getResource("").getPath()+ File.separator + "conf"; public static void main(String[] args) &#123; //加载配置文件 Configuration configuration = initConfiguration(confPath); //初始化安全集群Kerberos配置 initKerberosENV(configuration); //初始化Yarn 客户端 YarnClient yarnClient = YarnClient.createYarnClient(); yarnClient.init(configuration); yarnClient.start(); try &#123; //获得运行的任务应用清单 List&lt;ApplicationReport&gt; applications = yarnClient.getApplications(EnumSet.of(YarnApplicationState.RUNNING)); //存储需要关注的任务信息 HashMap&lt;String, ArrayList&lt;String&gt;&gt; applicationInformation = new HashMap&lt;&gt;(); for (ApplicationReport application:applications) &#123; String applicationType = application.getApplicationType(); // 只关注CPU核数资源使用超过500的任务 if (getApplicationInfo(application).get(1)&gt;=500) &#123; applicationInformation.put(String.valueOf(application.getApplicationId()), new ArrayList&lt;String&gt;()&#123;&#123; add(String.valueOf(application.getName())); add(String.valueOf(application.getApplicationType())); add(String.valueOf(application.getQueue())); add(String.valueOf(getApplicationInfo(application).get(0))); add(String.valueOf(getApplicationInfo(application).get(1))); add(String.valueOf(getApplicationInfo(application).get(2))); &#125;&#125;); &#125; System.out.println(applicationInformation); &#125; //设置监控信息发送邮件 if(!applicationInformation.isEmpty())&#123; //发送邮件 sendMailYarn(); &#125; &#125; catch (YarnException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 从ApplicationReport获取信息 * @return applicationInfo */ public static ArrayList&lt;Integer&gt; getApplicationInfo(ApplicationReport Application) &#123; ArrayList&lt;Integer&gt; applicationInfo = new ArrayList&lt;&gt;(); ApplicationResourceUsageReport resourceReport = Application.getApplicationResourceUsageReport(); if (resourceReport != null) &#123; Resource usedResources = resourceReport.getUsedResources(); int allocatedMb = usedResources.getMemory(); int allocatedVcores = usedResources.getVirtualCores(); int runningContainers = resourceReport.getNumUsedContainers(); //赋值 applicationInfo.add(allocatedMb); applicationInfo.add(allocatedVcores); applicationInfo.add(runningContainers); &#125; return applicationInfo; &#125; /** * 初始化YARN Configuration * @return configuration */ public static Configuration initConfiguration(String confPath) &#123; Configuration configuration = new Configuration(); System.out.println(confPath + File.separator + "core-site.xml"); configuration.addResource(new Path(confPath + File.separator + "core-site.xml")); configuration.addResource(new Path(confPath + File.separator + "hdfs-site.xml")); configuration.addResource(new Path(confPath + File.separator + "yarn-site.xml")); return configuration; &#125; /** * 安全集群配置（如果非安全集群这无需该方法） */ public static void initKerberosENV(Configuration conf) &#123; System.setProperty("java.security.krb5.conf", confPath+File.separator+"krb5.conf"); System.setProperty("javax.security.auth.useSubjectCredsOnly", "false"); System.setProperty("sun.security.krb5.debug", "false"); try &#123; UserGroupInformation.setConfiguration(conf); UserGroupInformation.loginUserFromKeytab("user@HADOOP.COM", confPath+File.separator+"user.keytab"); System.out.println(UserGroupInformation.getCurrentUser()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Yarn客户端YarnClient中定义了方法getApplications，获取到正在运行的任务清单，返回数据类型是：List&lt;ApplicationReport&gt;，如下： 1List&lt;ApplicationReport&gt; applications = yarnClient.getApplications(EnumSet.of(YarnApplicationState.RUNNING)); 对于数据类型ApplicationReport具有方法getApplicationResourceUsageReport()获得每个Yarn任务的ApplicationResourceUsageReport(任务资源报告)： 1ApplicationResourceUsageReport resourceReport = Application.getApplicationResourceUsageReport(); ApplicationResourceUsageReport提供了获取各类资源的方法： 1234567Resource usedResources = resourceReport.getUsedResources();//任务使用的内存资源int allocatedMb = usedResources.getMemory();//任务使用的CPU资源int allocatedVcores = usedResources.getVirtualCores();//任务使用的容器的数量int runningContainers = resourceReport.getNumUsedContainers(); 第三部分 总结Java的案例中我们使用了HashMap(applicationInformation)数据类型存储关注的任务信息，然后使用邮件接口发出。在实际使用中可以根据需要存储在elasticsearch集群。 另外对于其他方法，作者没有实际使用，可能存在部分信息未涵盖，可以参考官网文档使用。 参考文献及资料1、YARN Application Security，链接：https://hadoop.apache.org/docs/r2.7.4/hadoop-yarn/hadoop-yarn-site/YarnApplicationSecurity.html 2、ApplicationResourceUsageReport接口，链接：http://hadoop.apache.org/docs/r2.7.0/api/org/apache/hadoop/yarn/api/records/ApplicationResourceUsageReport.html 3、ResourceManager REST API’s，链接：https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html 4、基于Yarn API的Spark程序监控，链接:https://yq.aliyun.com/articles/710902]]></content>
      <categories>
        <category>Yarn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据资源调度平台粒度的说明]]></title>
    <url>%2F2020%2F10%2F06%2F2020-10-06-%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%E7%B2%92%E5%BA%A6%E7%9A%84%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 各种资源调度器粒度 第二部分 动态分配（Dynamic Allocation） 参考文献及资料 背景我们在Yarn资源管理器上提交MapReduce任务的时候发现（Yarn Web控制台），任务使用的container数量是变化的。其实这是由于MapReduce任务在Yarn上资源分配的粒度是：细粒度的。接下来我们将介绍资源调度管理器的粒度。 第一部分 各种资源调度器粒度我们知道Spark最开始没有Yarn和Srandalone模式的时候，只有Mesos资源管理器。后来才有了Yarn，最后为了推广才产生了Standalone模式。 1.1 Mesos集群Spark on Mesos模式下，同时支持粗粒度（coarse-grained）和细粒度（fine-grained），等到高版本Spark 2.X版本后不再支持细粒度。低版本默认配置是细粒度。 Fine-grained mode is deprecated as of Spark 2.0.0。Consider using Dynamic Allocation for some of the benefits. For a full explanation see SPARK-11857 设置为粗粒度 Spark配置项spark.mesos.coarse设置为true（可以在spark-default.conf配置文件中或者代码配置中）。在粗粒度模式下，可以通过下面3个参数指定静态资源： 设置spark.cores.max，最大使用CPU资源； 设置spark.executor.memory，每个executor的内存资源； 设置spark.executor.cores，每个executor的CPU资源； 设置为细粒度 注释spark-default.conf文件中的配置参数：spark.mesos.coarse，或者将其设置为false。 在细粒度模式下，Spark执行器中的每个Spark任务都作为单独的Mesos任务运行。这允许Spark的多个实例（和其他框架）以非常精细的粒度共享内核，其中每个应用程序在逐步增加和减少时将分配或多或少的内核，但是在启动每个任务时会带来额外的开销。此模式不适用于低延迟要求。 在Spark 2.x版本后，取消了对Mesos细粒度的支持，而是引入： Dynamic Allocation 1.2 Yarn集群1.2.1 细粒度MapReduce任务在Yarn上是细粒度模式。资源按需动态分配，每一个task均可以去申请资源，使用完后立即释放回收。提高集群计算资源的高效利用。但是如果task任务轻而多，那么就会出现task实际使用计算资源时间短，但是申请数量多，这就导致大量运行时间其实是花费在资源申请分配和释放上。 注：在实际生产中，使用MapReduce定时任务拉取Kafka数据汇入hive，就会出现短时间申请大量资源，实际计算时间很短，使得集群资源出现脉冲现象，资源使用极不平稳，引起相关性能容量告警。 1.2.2 粗粒度Spark on Yarn是粗粒度模式。资源静态分配。Spark Application在运行前就已经分配好需要的计算资源，没有task申请资源的时间延迟。但是资源释放上需要等待所有的task均执行完毕，才会触发所有资源的释放回收。特别极端的例子就是，一个job有100个task，完成了99个，还有一个没完成，那么所有资源就会闲置在那里等待那个task完成后才释放 。 1.3 Standalone集群Spark自身的Standalone集群也是粗粒度，和Spark on Yarn的粗粒度类似。 第二部分 动态分配（Dynamic Allocation）Spark在2.x开始抛弃在Mesos集群上对细粒度模式的支持，而是转而在粗粒度模式下提供动态分配机制（Dynamic Allocation）。其实Spark在1.2版本后，在Spark on Yarn上就已经提供对动态分配机制（Dynamic Allocation）的支持。所以动态分配机制是未来架构的方向，可以补充粗粒度在资源有效利用方面的不足。我们将在其他文章中介绍这种资源调度模式。 参考文献及资料1、Running Spark on Mesos，链接：https://spark.apache.org/docs/latest/running-on-mesos.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[监控Kafka的Topic数据]]></title>
    <url>%2F2020%2F10%2F05%2F2020-10-05-%E7%9B%91%E6%8E%A7Kafka%E7%9A%84Topic%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分实现原理 第二部分 实现源码 参考文献及资料 背景业务上需要实现对Kafka的Topic中数据进行监控。业务正常下，Kafka生产者是持续生产数据的。如果一段时间出现Kafka中指定Topic没有新的数据，那么说明业务生产者可能出现异常。 第一部分 实现原理1.1 Kafka生产者的写入原理Kafka每个Topic在创建时，划分为若干个Partition。消息在写入的时候，分布式写入指定的多个Partitions中。对于每个Partition，消息是顺序写入的。Topic在创建时，每个Partition的Offset是0，当消息顺序写入后逐步累加Offset值。 Kafka中Offset变量定义是一个长整型（Long），这个值最大为：9223372036854775807。那么逐步累加会不会用完呢？多虑了哈，这是一个天文级别的数值哈。 1.2 监控原理既然Offset记录了每个Topic的每个Partition的消息量，最朴素的方法就是监控这个值的变化来判断是否有新的数据写入。 编写语言我们选择Python，而且对于处理Offset这个超大数据，Python是天然支持。 那么可行性是没问题的。 第二部分 实现源码1.1 Python Api 接口与Kafka交互的Python包我们使用：Kafka-Python。对于消费者有下面的函数方法： end_offsets(partitions)[source] Get the last offset for the given partitions. The last offset of a partition is the offset of the upcoming message, i.e. the offset of the last available message + 1. This method does not change the current consumer position of the partitions. Note This method may block indefinitely if the partition does not exist. Parameters: partitions (list) – List of TopicPartition instances to fetch offsets for. Returns: int}: The end offsets for the given partitions. Return type: {TopicPartition Raises: UnsupportedVersionError – If the broker does not support looking up the offsets by timestamp.KafkaTimeoutError – If fetch failed in request_timeout_ms 我们写一个简单的测试： 1234567891011121314from kafka import KafkaConsumer, TopicPartition# Kafka配置BOOTSTRAP="192.168.1.1:9092"TOPIC="test"# 定义消费者consumer = KafkaConsumer(bootstrap_servers=[BOOTSTRAP])# 获取指定Topic的PartitionsPARTITIONS = []for partition in consumer.partitions_for_topic(TOPIC): PARTITIONS.append(TopicPartition(TOPIC, partition))# 获取Offset信息partitions = consumer.end_offsets(PARTITIONS)print(partitions)#&#123;TopicPartition(topic='test', partition=0): 4759, TopicPartition(topic='test', partition=1): 4823&#125; 接口比较简洁。案例中，我们创建了一个Topic，取名为test。一共2个分区，1个副本： 123Topic:test PartitionCount:2 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: test Partition: 1 Leader: 0 Replicas: 0 Isr: 0 所以回显分别显示了目前Topic的两个Partition的Offset值。 另外我们还可以使用kafka-consumer-groups.sh脚本查看一下目前某个group的offset情况： 123456root@deeplearning:/data/kafka/kafka_2.12-2.1.0/bin# ./kafka-consumer-groups.sh --bootstrap-server 192.168.1.1:9092 --describe --group my-groupConsumer group 'my-group' has no active members.TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-IDtest 1 4827 4835 8 - - -test 0 4763 4772 9 - - - 其中回显： CURRENT-OFFSET，目前group-id名称为’my-group’的消费群组Offset； LOG-END-OFFSET，目前topic的的Offset，也就是我们的监控对象； LAG，这是LOG-END-OFFSET-CURRENT-OFFSET的差，即’my-group’群组还有多少消息未消费； 1.2 完整的代码实现对多个Topic的监控： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# -*- coding: utf-8 -*-"""Copyright 2020 RongXiang.Licensed under the terms of the Apache 2.0 license.Please see LICENSE file in the project root for terms."""from kafka import KafkaProducer, KafkaConsumer, TopicPartitionimport timeimport logging# Kafka 配置BOOTSTRAP = ["192.168.31.3:9092"]MONITORTOPIC = ["test", "testTopic"]def monitoroffset(bootstrap, topicList): try: consumer = KafkaConsumer(bootstrap_servers=bootstrap) topicoffset = &#123;&#125; for topic in topicList: PARTITIONS = [] for partition in consumer.partitions_for_topic(topic): PARTITIONS.append(TopicPartition(topic, partition)) partitions = consumer.end_offsets(PARTITIONS) print(partitions) topicoffset[topic] = sum([partitions[item] for item in partitions]) consumer.close() return topicoffset except Exception as e: print(e)def diffdict(dictFirst, dictEnd): try: diffdict = &#123;&#125; for item in dictFirst: diffdict[item] = dictEnd[item]-dictFirst[item] return diffdict except Exception as e: print(e)def sendMessage(): passif __name__ == "__main__": logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) firstOffset = monitoroffset(BOOTSTRAP, MONITORTOPIC) time.sleep(300) secondOffset = monitoroffset(BOOTSTRAP, MONITORTOPIC) print(diffdict(firstOffset, secondOffset)) 输出： 1&#123;'test': 4, 'testTopic': 0&#125; 参考文献及资料1、kafka-python API介绍，链接：https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch中的GC以及监控]]></title>
    <url>%2F2020%2F10%2F05%2F2020-10-05-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Elasticsearch%E4%B8%AD%E7%9A%84GC%E4%BB%A5%E5%8F%8A%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分实现原理 第二部分 实现源码 参考文献及资料 背景garbage collection 第一部分 实现原理第二部分 实现源码参考文献及资料1、Nodes stats API介绍，链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html https://mincong.io/2020/08/30/gc-in-elasticsearch/ https://discuss.elastic.co/t/frequent-gc-in-elasticsearch/57681]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Dockerfile定制docker镜像总结]]></title>
    <url>%2F2020%2F10%2F03%2F2020-10-03-Docker%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Dockerfile%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Hive 性能瓶颈根源 Hive 配置优化 Hive 语句优化 总结 背景构建Docker镜像通常有两种方式： 基于容器制作； 通过Dockerfile； Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 第一部分 Dockerfile文件规范Docker镜像的分层 Dockerfile中的每个指令都会创建一个新的镜像层 镜像层将被缓存和复用 当Dockerfile的指令被修改了，复制的文件变化了，或者构建镜像时指定的变量不同了，对应的镜像层缓存就会失效 某一层的镜像缓存失效后，其之后的镜像层缓存都会随之失效 镜像层是不可变的，如果在某一层中添加一个文件，然后在下一层中删除则镜像中依然会包含该文件 Dockerfile编写规则Dockerfile中是基于其指令进行编写的，其规则可以参考下面的表格，当然，在编写Dockerfile时，其格式是需要严格遵循的： 除注释外，第一行必须使用FROM指令所基于的镜像名称；之后使用MAINTAINER指明维护信息；然后就是一系列镜像操作指令，如RUN、 ADD等；最后便是CMD指令来指定启动容器时要运行的命令操作。其中RUN指令可以使用多条，CMD只有最后一条可以生效！ 第二部分 Dockerfile文件组成2.1 注释2.2 FROM命令第三部分 编译Dcokerfile文件]]></content>
      <categories>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Dockerfile定制docker镜像总结]]></title>
    <url>%2F2020%2F10%2F03%2F2020-10-03-Docker%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Hive 性能瓶颈根源 Hive 配置优化 Hive 语句优化 总结 背景构建Docker镜像通常有两种方式： 基于容器制作； 通过Dockerfile； Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 第一部分 Dockerfile文件规范Docker镜像的分层 Dockerfile中的每个指令都会创建一个新的镜像层 镜像层将被缓存和复用 当Dockerfile的指令被修改了，复制的文件变化了，或者构建镜像时指定的变量不同了，对应的镜像层缓存就会失效 某一层的镜像缓存失效后，其之后的镜像层缓存都会随之失效 镜像层是不可变的，如果在某一层中添加一个文件，然后在下一层中删除则镜像中依然会包含该文件 Dockerfile编写规则Dockerfile中是基于其指令进行编写的，其规则可以参考下面的表格，当然，在编写Dockerfile时，其格式是需要严格遵循的： 除注释外，第一行必须使用FROM指令所基于的镜像名称；之后使用MAINTAINER指明维护信息；然后就是一系列镜像操作指令，如RUN、 ADD等；最后便是CMD指令来指定启动容器时要运行的命令操作。其中RUN指令可以使用多条，CMD只有最后一条可以生效！ 第二部分 Dockerfile文件组成2.1 注释2.2 FROM命令第三部分 编译Dcokerfile文件]]></content>
      <categories>
        <category>docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群pending_task任务等待集群故障处理]]></title>
    <url>%2F2020%2F10%2F02%2F2021-04-29-Elasticsearch%E9%9B%86%E7%BE%A4pending_task%E4%BB%BB%E5%8A%A1%E7%AD%89%E5%BE%85%E9%9B%86%E7%BE%A4%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 静态分区 第二部分 动态分区 第三部分 两者的比较 第四部分 动态分区使用的问题 参考文献及资料 背景故障现象： 集群出现大量的put-mapping的任务堆积 （1200个左右），该任务属于HIGH级别，比创建索引等任务级别高。所以出现集群4月27号的索引未创建成功。 pending task 反应了master节点尚未执行的集群级别的更改任务（例如：创建索引，更新映射，分配分片）的列表。pending task的任务是分级别的（优先级排序：IMMEDIATE&gt;URGENT&gt;HIGH&gt;NORMAL&gt;LOW&gt;LANGUID）,只有当上一级别的任务执行完毕后才会执行下一级别的任务，这也说明了：当出现HIGH级别以上的pending task任务时，备份和创建索引等一些低级别任务虽然任务对资源占用不多，也将不会执行 put-mapping的原因分析： pendingtask只能由主节点来进行处理，这些任务包括创建索引并将shards分配给节点。任务分优先次序。如果任务的产生比处理速度更快，将会产生堆积。如果存在任务堆积，集群存在较大隐患，需要排查集群的任务，确认原因。 这一条优化建议在上面也提到了，因为创建索引及新加字段都是更新元数据操作，需要 master 节点将新版本的元数据同步到所有节点。 因此在集群规模比较大，写入qps较高的场景下，特别容易出现master更新元数据超时的问题，这可导致 master 节点中有大量的 pending_tasks 任务堆积，从而造成集群不可用，甚至出现集群无主的情况。 动态mapping非常容易引起性能问题，特别是集群比较大的情况下，容易因为大量的mapping更新任务会导致master过载。 并且动态mapping也容易因为脏数据的写入产生错误的字段类型。 我们的做法是完全禁用动态mapping，在索引的mapping设置中增加”dynamic”: “false” 这个选项即可。 集群的索引字段类型都需要预先设计好，数据必须严格按照设计的类型写入，否则会被reject。 优化建议： 1、静态mapping； 2、index创建提前，并且设定好mapping； 其中 priority 字段则表示该 task 的优先级，翻看 ES 的源码可以看到一共有六种优先级： 123456IMMEDIATE((byte) 0),URGENT((byte) 1),HIGH((byte) 2),NORMAL((byte) 3),LOW((byte) 4),LANGUID((byte) 5); 查看分片未分配的原因 其中 index和shard 列出了具体哪个索引的哪个分片未分配成功。reason 字段则列出了哪种原因导致的分片未分配。这里也将所有可能的原因列出来： 1234567891011121314151617181920212223INDEX_CREATED：由于创建索引的API导致未分配。CLUSTER_RECOVERED ：由于完全集群恢复导致未分配。INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。EXISTING_INDEX_RESTORED ：由于恢复到已关闭的索引导致未分配。REPLICA_ADDED：由于显式添加副本分片导致未分配。ALLOCATION_FAILED ：由于分片分配失败导致未分配。NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。REALLOCATED_REPLICA ：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。 detail 字段则列出了更为详细的未分配的原因。下面我会总结下在日常运维工作中常见的几种原因。 如果未分配的分片比较多的话，我们也可以通过下面的API来列出所有未分配的索引和主分片： 新版本： https://github.com/elastic/elasticsearch/pull/48867 参考文献及资料1、记Elasticsearch 因不合理创建type导致集群故障 https://blog.csdn.net/sunjiangangok/article/details/80423852 https://support.cloudbees.com/hc/en-us/articles/115000089811-Elasticsearch-troubleshooting-guide?page=9]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive中静态分区和动态分区总结]]></title>
    <url>%2F2020%2F10%2F02%2F2020-10-02-Hive%E4%B8%AD%E9%9D%99%E6%80%81%E5%88%86%E5%8C%BA%E5%92%8C%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 静态分区 第二部分 动态分区 第三部分 两者的比较 第四部分 动态分区使用的问题 参考文献及资料 背景在Hive中有两种类型的分区：静态分区(Static Partitioning)和动态分区(Dynamic Partitioning)。 静态分区。对于静态分区，从字面就可以理解：表的分区数量和分区值是固定的。 动态分区。会根据数据自动的创建新的分区。 本文会详细介绍两种分区方法、使用场景以及生产中常见问题和解决方法。 第一部分 静态分区静态分区的使用场景主要是分区的数量是确定的。例如人力资源信息表中使用“部门”作为分区字段，通常一段时间是静态不变的。例如： 1234567891011CREATE EXTERNAL TABLE employee_dept ( emp_id INT, emp_name STRING) PARTITIONED BY ( dept_name STRING )location '/user/employee_dept';LOAD DATA LOCAL INPATH 'hr.txt'INTO TABLE employee_deptPARTITION (dept_name='HR'); 上面的外部表以dept_name字段为分区字段，然后导入数据需要指定分区。 第二部分 动态分区通常在生产业务场景中，我们使用的都是灵活的动态分区。例如我们使用时间字段（天、小时）作为分区字段。新的数据写入会自动根据最新的时间创建分区并写入对应的分区。例如下面的例子： 123hive &gt; insert overwrite table order_partition partition (year,month) select order_id, order_date, order_status, substr(order_date,1,4) year, substr(order_date,5,2) month from orders;FAILED: SemanticException [Error 10096]: Dynamic partition strict mode requires at least one static partition column. To turn this off set hive.exec.dynamic.partition.mode=nonstrict 写入报错。这是因为Hive默认配置不启用动态分区，需要使用前开启配置。开启的方式有两种： 在hive服务配置文件中全局配置； 每次交互时候进行配置（只影响本次交互）； 通常我们生产环境使用第二种。 12set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict; 其中参数hive.exec.dynamic.partition.mode表示动态分区的模式。默认是strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。 第三部分 两者的比较两种分区模式都有各自的使用场景，我们总结如下： 静态分区(Static Partitioning) 动态分区（Dynamic Partitioning） 分区创建 数据插入分区之前，需要手动创建每个分区 根据表的输入数据动态创建分区 适用场景 需要提前知道所有分区。适用于分区定义得早且数量少的用例 有很多分区，无法提前预估新分区，动态分区是合适的 另外动态分区的值是MapReduce任务在reduce运行阶段确定的，也就是所有的记录都会distribute by，相同字段(分区字段)的map输出会发到同一个reduce节点去处理，如果数据量大，这是一个很弱的运行性能。而静态分区在编译阶段就确定了，不需要reduce任务处理。所以如果实际业务场景静态分区能解决的，尽量使用静态分区即可。 第四部分 动态分区使用的问题Hive表中分区架构使得数据按照分区分别存储在HDFS文件系统的各个目录中，查询只要针对指定的目录集合进行查询，而不需要全局查找，提高查询性能。 但是分区不是”银弹”，如果分区数据过多，就会在HDFS文件系统中创建大量的目录和文件，对于集群NameNode服务是有性能压力的，NameNode需要将大量元数据信息保留在内存中。另外大分区表在用户查询时候由于分析size太大，也容易造成Metastore服务出现OMM报错。 上面两个现象均在生产环境发生，分别造成NameNode和Metastore不可用。 事实上，Hive为了防止异常生产大量分区，甚至默认动态分区是关闭的。另外对于生成动态分区的数量也做了性能默认限制。 4.1 动态分区创建限制当我们在一个Mapreduce任务（hive写入会编译成mapreduce任务）中创建大量分区的时候，经常会遇到下面的报错信息： 122015-06-15 17:27:44,614 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(171)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row ....Caused by: org.apache.hadoop.hive.ql.metadata.HiveFatalException: [Error 20004]: Fatal error occurred when node tried to create too many dynamic partitions. The maximum number of dynamic partitions is controlled by hive.exec.max.dynamic.partitions and hive.exec.max.dynamic.partitions.pernode. Maximum was set to: 256... 10 more 这个报错就是因为Hive对于动态分区创建的限制，涉及的参数有： 123hive.exec.max.dynamic.partitions = 1000;hive.exec.max.dynamic.partitions.pernode = 100;hive.exec.max.created.files = 10000 hive.exec.max.dynamic.partitions.pernode，参数限制MapReduce任务单个任务(mapper或reducer任务)创建的分区数量为100； hive.exec.max.dynamic.partitions，参数限制单次整体任务创建分区的数量上限为1000个； hive.exec.max.created.files，参数限制所有单次整体map和reduce任务创建的最大文件数量上限为10000个； 以上三个阀值超过就会触发错误，集群会杀死任务。为了解决报错，我们通常将两个参数调大。但是也需要用户对自己的Hive表的分区数量进行合理规划，避免过多的分区。 4.2 特殊分区如果动态分区列输入的值为NULL或空字符串，则Hive将该行将放入一个特殊分区，其名称由参数hive.exec.default.partition.name控制。默认值为__HIVE_DEFAULT_PARTITION__。 用户可以使用（查看表分区）命令进行查看： 1234show partitions 'table名称';# process_date=20160208#process_date=__HIVE_DEFAULT_PARTITION__ 有时候异常生产这些分区数据，需要进行清理。如果使用下面的语句： 1ALTER TABLE Table_Name DROP IF EXISTS PARTITION(process_date='__HIVE_DEFAULT_PARTITION__'); 这时候Hive会报错： 1Error: Error while compiling statement: FAILED: SemanticException Unexpected unknown partitions for (process_date = null) (state=42000,code=40000) 这是Hive一个已知bug（编号：HIVE-11208），在Hive 2.3.0版本修复。 但是有个有修复方法（不建议在生产环境中实施）： 123456-- update the column to be "string"ALTER TABLE test PARTITION COLUMN (p1 string);-- remove the default partitionALTER TABLE test DROP PARTITION (p1 = '__HIVE_DEFAULT_PARTITION__');-- then revert the column back to "int" typeALTER TABLE test PARTITION COLUMN (p1 int); 链接：https://cloudera.ericlin.me/2015/07/how-to-drop-hives-default-partition-__hive_default_partition__-with-int-partition-column/ 4.3 乱码分区字段有时候表分区字段由于处理不当，会出现乱码分区，例如： 1hp_stat_time=r_ready%3D91;r_load%3D351 原因是Hive会自动对一些UTF-8字符编码成Unicode（类似网址中中文字符和一些特殊字符的编码处理）。此处%3D解码后是’=’。可以使用在线转换进行解码：https://www.matools.com/code-convert-utf8。 最后使用解码后的字段即可（注意分号转义）： 1alter table dpdw_traffic_base drop partition(hp_stat_time='r_ready=91\;r_load=351'); 参考文献及资料1、动态分区，链接：https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions 2、Hive Tutorial，链接：https://cwiki.apache.org/confluence/display/Hive/Tutorial 3、Apache Hive 中文手册，链接：https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive中外部表总结和实践]]></title>
    <url>%2F2020%2F10%2F02%2F2021-04-27-Hive%E4%B8%AD%E5%A4%96%E9%83%A8%E8%A1%A8%E6%80%BB%E7%BB%93%E5%92%8C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 静态分区 第二部分 动态分区 第三部分 两者的比较 第四部分 动态分区使用的问题 参考文献及资料 背景参考文献及资料1、动态分区，链接：https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions 2、Hive Tutorial，链接：https://cwiki.apache.org/confluence/display/Hive/Tutorial 3、Apache Hive 中文手册，链接：https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference]]></content>
      <categories>
        <category>Hive</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mysql的两种连接方式总结]]></title>
    <url>%2F2020%2F10%2F02%2F2020-10-01-Mysql%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%BF%9E%E6%8E%A5%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 TCP/IP Socket 第二部分 UNIX Domain Socket 第三部分 Mysql的两种连接方式 参考文献及资料 背景我们在使用Mysql客户端和Mysql交互的时候，如果客户端是远程（非本机）那么底层是通过TCP/IP的Socket方式进行交互。但是如果客户端和数据库在同一台服务器上时，Mysql支持通过UNIX Domain Socket方式交互。 Mysql客户端和Mysql数据库服务通信，不管是本机还是远程，其本质是两个计算机进程之间的通信。根据位置的不同分为：本机进程通信和网络间进程通信。 本机进程通信。Linux通常有：管道、信号量、消息队列、信号、共享内存、UNIX Domain Socket套接字。 网络间进程通信。 TCP/IP Socket 本文将介绍将介绍这两种交互方式。 第一部分 TCP/IP Socket提到通信，那么首先需要解决是身份识别问题。对于本机进程，不同的进程都有操作系统分配的进程号(process ID)作为唯一标识。但是网络间进程通信时候，PID就不能作为唯一标识了，另外操作系统支持的网络协议不同。所以网络间进程通信需要解决唯一身份标识和网络协议识别问题。 这时候出现的TCP/IP协议中，IP层的ip地址可以唯一标识网络计算机身份，传输层的“协议+端口”可以唯一标识进程。这样就有”三元坐标”:(IP地址，协议，端口)，这个坐标可以唯一标识网络中进程。 在网络编程中，TCP/IP和Socket两个概念没有必然的联系。Socket编程接口在设计的时候，也能支持其他的网络协议。所以，socket的出现只是可以更方便的使用TCP/IP协议栈而已，其对TCP/IP进行了抽象封装，形成了几个最基本的函数接口。例如create，listen，accept，connect，read和write等等。 第二部分 UNIX Domain Socket对于本机中进程通信，也是可以使用TCP/IP Socket方式，通过回环地址(loopback)地址 127.0.0.1,但是对于本机这是繁琐的，接着就发展出了Unix domain socket方式，又称IPC(inter-process communication进程间通信) socket。 UNIX domain socket 用于本机进程通信更有效率。不需要经过网络协议栈，不需要打包拆包、计算校验和、维护序号、路由和应答等，只是将应用层数据从一个进程拷贝到另一个进程。UNIX domain socket与 TCP/IP Socket相比较，在同一台主机的传输速度前者是后者的两倍。 UNIX domain socket 与TCP/IP Socket 最明显的不同在于地址格式不同，TCP/IP Socket 的 socket 地址是IP地址加端口号，而UNIX domain socket的地址是一个 socket 类型的文件在文件系统中的路径。 另外下面的命令可以查看当前操作系统中UNIX domain socket通信清单： 1netstat -a -p --unix 第三部分 Mysql的两种连接方式3.1 UNIX Domain Socket方式客户端部署在Mysql本机，配置好环境变量，就可以使用下面的命令登录Mysql。 1234567891011121314[root@mysql ~]# mysql -uroot -P*****Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 1Server version: 5.5.46-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@[(none)]&gt; 这时候就是通过UNIX Domain Socket方式和Mysql进行交互的。只是这时候我们没有指定参数–socket，这个参数指定就是UNIX Domain Socket依赖的socket 类型的文件。Mysql默认这个参数为：–socket=/tmp/mysql.sock。如果安装Mysql时候，配置文件my.cnf中下面配置错误或文件丢失，就会报错。 12[client] socket=/tmp/mysql.sock 报错找不到sock文件： 12[root@mysql ~]# mysql -uroot -P*****ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2) 遇到这种报错的处理方法： 使用find命令查找文件路径，调整配置，使其归位。如果文件不再配置文件指定位置，这时候需要在命令中指定具体的路径，命令如下： 1[root@mysql ~]# mysql -uroot -P***** -S /path/of/mysql.sock 重新创建。可以简单地通过重启Mysql服务重新创建得到它。因为服务在启动时重新创建它。 另外可以通过查看Mysql变量信息来查看这个文件路径配置路径： 12345678mysql&gt; SHOW VARIABLES LIKE 'socket';+---------------+-----------------+| Variable_name | Value |+---------------+-----------------+| socket | /tmp/mysql.sock |+---------------+-----------------+1 row in set (0.00 sec)mysql -uroot -S/tmp/mysql.sock 3.2 TCP/IP Socket方式在本机使用UNIX Domain Socket方式无法登陆时候，还可以使用TCP/IP Socket方式。命令需要指定IP信息，如： 1234567891011121314151617[root@mysql ~]# mysql -h192.1.1.20ERROR 2003 (HY000): Can't connect to MySQL server on '192.1.1.20' (111)[root@mysql ~]# mysql -h192.1.1.20 -P3307Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 3Server version: 5.5.46-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@[(none)]&gt; 那么如果本机中同时指定两个参数时候，Mysql会默认使用TCP/IP Socket的方式连接。 1234567891011121314[root@mysql ~]# mysql -h192.1.1.20 -P3306 -S /path/of/mysql.sockWelcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.5.46-log MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.root@[(none)]&gt; 对于和远程Mysql服务交互，需要指定远程Mysql服务监听IP和端口即可，不再赘述。 参考文献及资料1、Linux进程间通信方式有哪些？，链接：https://zhuanlan.zhihu.com/p/63916424 2、UNIX Domain Socket IPC，链接：http://docs.linuxtone.org/ebooks/C&amp;CPP/c/ch37s04.html]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark任务依赖jar包总结]]></title>
    <url>%2F2020%2F09%2F27%2F2020-09-27-Spark%E4%BB%BB%E5%8A%A1%E4%BE%9D%E8%B5%96jar%E5%8C%85%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景编写了Spark批任务（Java），使用maven打包成jar包，提交到Yarn集群，报jar包冲突的错误。查阅资料了解一下背后的原理。 https://stackoverflow.com/questions/16222748/building-a-fat-jar-using-maven 第一部分 Spark任务加载jar包原理Spark任务运行默认加载的配置从默认的配置文件中获取，如果配置参数用户重新定义，那么参数将会覆盖默认配置文件中加载的配置。 spark-defaults.conf https://exceptionshub.com/add-jars-to-a-spark-job-spark-submit.html 原因是本地的jar包被SPARK_HOME/lib中的jar覆盖。spark程序在提交到yarn时，除了上传用户程序的jar，还会上传SPARK_HOME的lib目录下的所有jar包（参考附录2 )。如果你程序用到的jar与SPARK_HOME/lib下的jar发生冲突，那么默认会优先加载SPARK_HOME/lib下的jar，而不是你程序的jar，所以会发生“ NoSuchMethodError”。 1mvn dependency:tree -Dverbose 参数spark.yarn.jars 官方文档的解释： 1List of libraries containing Spark code to distribute to YARN containers. By default, Spark on YARN will use Spark jars installed locally, but the Spark jars can also be in a world-readable location on HDFS. This allows YARN to cache it on nodes so that it doesn't need to be distributed each time an application runs. To point to jars on HDFS, for example, set this configuration to hdfs:///some/path. Globs are allowed. 参数（1.5.1）： spark.driver.extraClassPath spark.executor.extraClassPath 附加到driver的classpath的额外的classpath实体。 附加到executors的classpath的额外的classpath实体。这个设置存在的主要目的是Spark与旧版本的向后兼容问题。用户一般不用设置这个选项 额外的classpath条目需预先添加到驱动程序 classpath中。 注意 : 在客户端模式下，这一套配置不能通过 SparkConf 直接在应用在应用程序中，因为 JVM 驱动已经启用了。相反，请在配置文件中通过设置 –driver-class-path 选项或者选择默认属性。 目录使用hdfs文件 参数： spark.driver.extraLibraryPath spark.executor.extraLibraryPath 指定启动driver的JVM时用到的库路径 参数：（2.1.0版本后） spark.driver.userClassPathFirst spark.executor.userClassPathFirst (实验性)当在driver中加载类时，是否用户添加的jar比Spark自己的jar优先级高。这个属性可以降低Spark依赖和用户依赖的冲突。它现在还是一个实验性的特征。 （实验）在驱动程序加载类库时，用户添加的 Jar 包是否优先于 Spark 自身的 Jar 包。这个特性可以用来缓解冲突引发的依赖性和用户依赖。目前只是实验功能。这是仅用于集群模式。 参数： spark.yarn.dist.jars 第二部分 解决jar包冲突第四种方式操作：更改Spark的配置信息:SPARK_CLASSPATH, 将第三方的jar文件添加到SPARK_CLASSPATH环境变量中 注意事项：要求Spark应用运行的所有机器上必须存在被添加的第三方jar文件 ;) 1234567A.创建一个保存第三方jar文件的文件夹: 命令：$ mkdir external_jarsB.修改Spark配置信息 命令：$ vim conf/spark-env.sh修改内容：SPARK_CLASSPATH=$SPARK_CLASSPATH:/opt/cdh-5.3.6/spark/external_jars/*C.将依赖的jar文件copy到新建的文件夹中命令：$ cp /opt/cdh-5.3.6/hive/lib/mysql-connector-java-5.1.27-bin.jar ./external_jars/ 备注：（只针对spark on yarn(cluster)模式）spark on yarn(cluster)，如果应用依赖第三方jar文件最终解决方案：将第三方的jar文件copy到${HADOOP_HOME}/share/hadoop/common/lib文件夹中(Hadoop集群中所有机器均要求copy)https://blog.csdn.net/ifenggege/article/details/108327167 参考文献及资料1、Running Spark on YARN，链接：https://spark.apache.org/docs/1.5.1/running-on-yarn.html 2、解决jar包冲突新思路 - maven-shade-plugin，链接：https://zhuanlan.zhihu.com/p/62796806]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤算法总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-13-Yarn%E9%9B%86%E7%BE%A4%E8%B0%83%E5%BA%A6%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 参考文献及资料 背景https://blog.csdn.net/zhanyuanlin/article/details/78799341 https://blog.csdn.net/zhanyuanlin/article/details/78799131 https://blog.csdn.net/u010770993/article/details/70312473 参考文献及资料1、数学之美二十一：布隆过滤器（Bloom Filter）：http://www.google.com.hk/ggblog/googlechinablog/2007/07/bloom-filter_7469.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS小文件治理总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-13-HDFS%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 参考文献及资料 背景https://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html 参考文献及资料]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS小文件治理总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-13-HDFS%E5%B0%8F%E6%96%87%E4%BB%B6%E6%B2%BB%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 回本溯源 第二部分 HDFS大量小文件的危害 第三部分 小文件治理方案总结 第四部分 总结 参考文献及资料 背景企业级Hadoop大数据平台在实际使用过程中，可能大部分会遭遇小文件问题，并体验它的破坏性。HDFS文件系统的 inode 信息和 block 信息以及 block 的位置信息，这些原数据信息均由 NameNode 的内存中维护，这使得 NameNode 对内存的要求非常高，特别是遭遇海量小文件。 例如：京东的 NameNode 内存是 512GB，甚至还有大厂的 NameNode 的机器是 1TB的内存。能力强的大厂，钱就不花在买机器上了，例如字节跳动使用C++重写 NameNode ，这样分配内存和释放内存都由程序控制。但是NameNode天生的架构缺陷，所以元数据的扩展性终是受限于单机物理内存大小。 本篇文章回本溯源，总结小文件的产生原理、对业务平台的危害，最后分析总结了治理方法。 第一部分 回本溯源HDFS基于Google的论文《分布式文件系统》思想实现的，设计目的是解决大文件的读写。 1.1 HDFS存储原理HDFS集群（hadoop 2.0 +）中，有两类服务角色：NameNode、DataNode。文件数据按照固定大小（block size，默认128M）切分后，分布式存储在DataNode节点上。而数据的元数据信息加载在NameNode服务内存中。为防止服务单机会持久化一份在文件中（即fsimage文件，最新的元数据存储在edits log日志中，一般为 64MB，当 edits log 文件大小达到 64MB时，就会将这些元数据追加到 fsimage 文件中）。 每个文件/目录和block元数据信息存储在内存中，内存中分别对应：INodeFile、INodeDirectory、BlockInfo，每个对象大约150-200 bytes。 1.2 检查文件系统1.2.1 命令 fsckHDFS提供了fsck命令用来检查HDFS文件系统的健康状态和Block信息。需要有HDFS的supergroup特权用户组的用户才有执行权限。参考下面的命令： 12345678910111213141516171819202122[root@quickstart cloudera]# hdfs fsck / -blocks -locationsConnecting to namenode via http://quickstart.cloudera:50070FSCK started by admin (auth:KERBEROS_SSL) from /172.17.0.2 for path / at Sun May 09 05:35:33 UTC 2021Status: HEALTHY Total size: 837797031 B Total dirs: 285 Total files: 921 Total symlinks: 0 Total blocks (validated): 916 (avg. block size 914625 B) Minimally replicated blocks: 916 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 1 Average block replication: 1.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 1 Number of racks: 1FSCK ended at Sun May 09 05:35:34 UTC 2021 in 90 millisecondsThe filesystem under path '/' is HEALTHY 其中Total blocks (validated): 916 (avg. block size 914625 B)，表示集群一共有916个Block，平均一个Block存储大小为0.87M。 1.2.2 命令 countHDFS 提供查询目录中文件数量的命令，例如： 12[root@quickstart cloudera]# hdfs dfs -count / 285 923 837797031 / 回显说明，依次为：DIR_COUNT(文件目录数量), FILE_COUNT（文件数量）, CONTENT_SIZE（存储量） FILE_NAME（查询文件目录名） 1.2.3 NameNode WebUI界面NameNode WebUI提供展示界面，显示HDFS文件系统的关键指标信息。例如： 12345Security is on.Safemode is off.1,206 files and directories, 916 blocks = 2,122 total filesystem object(s).Heap Memory used 23.5 MB of 48.38 MB Heap Memory. Max Heap Memory is 48.38 MB.Non Heap Memory used 49.16 MB of 77.85 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB. 目前HDFS文件系统中有1206个文件和文件目录，916个block，一共2122个文件系统对象。 可以jmap命令参看jvm堆栈实际使用： 1234# jmap -histo:live 8204（namenode进程pid ）num #instances #bytes class name#（省略）Total 435970 61136008 实际使用61136008/1024/1024=58.3 M。 1.2.4 元数据内存资源估算NameNode的内存主要由NameSpace和BlocksMap占用，其中NameSpace存储的主要是INodeFile和INodeDirectory对象。BlocksMap存储的主要是BlockInfo对象，所以估算NameNode占用的内存大小也就是估算集群中INodeFile、INodeDirectory和BlockInfo这些对象占用的heap空间。下面是估算NameNode内存数据空间占用资源大小的预估公式。 1Total = 198 ∗ num(directories + Files) + 176 ∗ num(blocks) + 2% ∗ size(JVM Memory Size) 例如测试集群：（1206*198+176*916）/1024/1024+2%*1000=20.38M，和实际值23.5 M误差较小。 1.2.5 堆栈配置建议在NameNode WebUI界面的Summary可以看到文件系统对象（filesystem objects）的统计。下面是NameNode根据文件数量的堆栈大小配置建议。 文件数量(1文件对应1block) 文件系统对象数量（filesystem objects=files+blocks） 参考值（GC_OPTS） 5,000,000 10,000,000 -Xms6G -Xmx6G -XX:NewSize=512M -XX:MaxNewSize=512M 10,000,000 20,000,000 -Xms12G -Xmx12G -XX:NewSize=1G -XX:MaxNewSize=1G 25,000,000 50,000,000 -Xms32G -Xmx32G -XX:NewSize=3G -XX:MaxNewSize=3G 50,000,000 100,000,000 -Xms64G -Xmx64G -XX:NewSize=6G -XX:MaxNewSize=6G 100,000,000 200,000,000 -Xms96G -Xmx96G -XX:NewSize=9G -XX:MaxNewSize=9G 150,000,000 300,000,000 -Xms164G -Xmx164G -XX:NewSize=12G -XX:MaxNewSize=12G 1.3 产生的场景分析在实际生产环境中，很多场景会产生小文件。 1.3.1 MapReduce产生Mapreduce任务中reduce数量设置过多，reduce的个数和输出文件个数一致，从而导致输出大量小文件。 1.3.2 hive产生hive表设置过量分区，每次写入数据会分别落盘到各自分区中，每个分区的数据量越小，对应的分区表文件也就会越小。从而导致产生大量小文件。 1.3.3 实时流任务处理流任务处理数据通常要求短时间数据落盘，例如Spark Streaming 从外部数据源接收数据，每个微批（默认60s）需要落盘一次结果数据到HDFS，如果数据量小，会产生大量小文件落盘文件。 1.3.4 数据自身特性产生除了数据处理产生，还有是由于数据自身特性决定的。例如使用HDFS存储图片、短视频等数据。这些数据本身单体就不大，就会以小文件形式存储。 第二部分 HDFS大量小文件的危害2.1 MapReduce任务消耗大量计算资源MapReduce任务处理HDFS文件的时候回根据数据的Block数量启动对应数量Map task，如果是小文件系统，这会导致任务启动大量的Map task，一个task在Yarn上对应一个CPU，实际线上环境会短时间申请成千个CPU资源，造成集群运行颠簸。可以通过设置map端文件合并及reduce端文件合并来优化。 2.2 NameNode服务过载HDFS作为分布式文件系统的一个优点就是可以横向伸缩扩展，但是由于元数据存储在NameNode中，事实上，当数据量达到一定程度，NameNode服务单机内存资源（普通PC物理机内存通常是256G）反而成为横向扩展的瓶颈。特别是面对小文件系统，可能集群实际存储并不大，但是元数据信息已经使得NameNode服务过载，这时候横向扩容DataNode是无济于事的。 HDFS中元数据的操作均在NameNode服务完成，小文件系统造成服务过载后，元数据更新性能会下降。严重的时候，服务会经常Full GC，如果GC停顿过长甚至会导致服务故障。 如果导致NameNode出现故障，在没有HA保障时，服务启动是一个漫长的过程。服务需要重新将fsImage文件数据加载至服务内存，最新的日志数据editlogs需要回放，最后Checkpoint和DataNode的BlockReport。这个过程当元数据过大时候是个漫长过程。 例如美团的提供案例：当元数据规模达到5亿（Namespace中INode数超过2亿，Block数接近3亿），fsImage文件大小将接近到20GB，加载FsImage数据就需要约14min，Checkpoint需要约6min，再加上其它阶段整个重启过程将持续约50min，极端情况甚至超过60min。 笔者公司HA集群，曾经由于小文件系统导致NameNode服务故障，甚至出现主备元数据未同步，整个恢复过程需要完成先完成主备同步，整整需要8个小时，这个期间HDFS无法对外服务，影响较大。 第三部分 小文件治理方案总结面对HDFS的NameNode内存过载带来的线上问题，Hadoop社区给出治理方案和架构上优化。主要有： 横向扩展NameNode能力，分散单点负载；例如联邦（Federation）。 NameNode元数据调整为外置存储；例如LevelDB最为存储对象。 另外还有美团技术文章《HDFS NameNode内存全景》提到互联网大厂的最佳实践和尝试： 除社区外，业界也在尝试自己的解决方案。Baidu HDFS2[5]将元数据管理通过主从架构的集群形式提供服务，本质上是将原生NameNode管理的Namespace和BlockManagement进行物理拆分。其中Namespace负责管理整个文件系统的目录树及文件到BlockID集合的映射关系，BlockID到DataNode的映射关系是按照一定的规则分到多个服务节点分布式管理，这种方案与Lustre有相似之处（Hash-based Partition）。Taobao HDFS2[6]尝试过采用另外的思路，借助高速存储设备，将元数据通过外存设备进行持久化存储，保持NameNode完全无状态，实现NameNode无限扩展的可能。其它类似的诸多方案不一而足。 尽管社区和业界均对NameNode内存瓶颈有成熟的解决方案，但是不一定适用所有的场景，尤其是中小规模集群。 3.1 联邦HDFS在Hadoop 2.x发行版中引入了联邦（Federation）HDFS功能。联邦HDFS允许集群通过添加多个NameNode来实现扩展，每个NameNode管理一份元数据。架构上解决了横向扩展，但是这不是一个真正的分布式NameNode，仍然存在单点故障风险。具体可以参考美团技术的最佳实践[3]。 3.2 归档文件对于小文件问题，Hadoop自身提供了三种解决方案：Hadoop Archive、 Sequence File 和 CombineFileInputFormat。 3.2.1 Hadoop ArchiveHadoop Archives （HAR files）在 0.18.0版本中引入，目的是为了缓解大量小文件消耗 NameNode 内存的问题。HAR不会减少文件存储大小，而是减少NameNode 的内存资源。例如下图展示了将HDFS文件目录foo中大量小文件file-*归档为bar.har文件。 可以使用下面的命令对HDFS文件进行归档： 1# hadoop archive -archiveName name -p &lt;parent&gt; [-r &lt;replication factor&gt;] &lt;src&gt;* &lt;dest&gt; -archiveName指定创建归档文件的名称，如：foo.har，也就是说需要在归档名称后面添加一个*.har的扩展。 -p 指定需归档的文件的相对路径，如：-p /foo/bar a/b/c e/f/g，/foo/bar是根目录，a/b/c，e/f/g是相对根目录的相对路径。 -r 指定所需的复制因子，如果未被指定，默认为3。 例如下面的命令执行后，将提交一个Mapreduce任务。 1[root@quickstart /]# hadoop archive -archiveName test.har -p /user/test /tmp 使用下面的命令查看归档后的文件： 1234[root@quickstart /]# hadoop fs -lsr har:///tmp/test.harlsr: DEPRECATED: Please use 'ls -R' instead.-rw-r--r-- 3 admin supergroup 0 2021-05-10 23:02 har:///tmp/test.har/file-1(略) 需要注意的是： 归档文件一旦创建就不能改变，要增加或者删除文件，就需要重新创建。 归档不支持压缩数据，类似于Unix中的tar命令。 归档命令不会删除原HDFS文件，需要自行删除。 *.har在HDFS上是一个目录，不是一个文件。 Hive内置了将现有分区中的文件转换为Hadoop Archive (HAR)的支持，具体参数为： 123456# 启用归档hive&gt; set hive.archive.enabled=true;# 创建存档时是否可以设置父目录hive&gt; set hive.archive.har.parentdir.settable=true;# 控制组成存档的文件的大小hive&gt; set har.partfile.size=1099511627776; 实际线上环境，可以对长期保存的hive表以分区颗粒度进行归档，在需要查询的时候进行归档恢复。具体实践案例如下： 1234# hive分区归档ALTER TABLE tablename ARCHIVE PARTITION(ds='2008-04-08', hr='12')# hive归档分区恢复ALTER TABLE tablename UNARCHIVE PARTITION(ds='2008-04-08', hr='12') 数据归档为har后，数据仍然是可以被查询的（但是是不可写的），只是查询会比非归档慢，如果要提高效率需要归档恢复。 3.2.2 Sequence FileSequenceFile是Hadoop API提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中，使用Hadoop的标准Writable接口实现序列化和反序列化。Mapreduce计算的中间结果的落盘就是SequenceFile格式的文件。 线上生产环境中，将SequenceFile格式的文件作为HDFS小文件的容器。即读取小文件然后以Append追加的形式写入”文件容器”。 3.2.3 CombineFileInputFormatHadoop内置提供了一个 CombineFileInputFormat 类来专门处理合并小文件，其核心功能是将HDFS上多个小文件合并到一个 InputSplit中，然后会启用一个map来处理这里面的文件，以此减少Mapreduce整体作业的运行时间，同时也减少了map任务的数量。 这个接口天然具备处理小文件的能力，只要将合并后的小文件落盘即可。 另外对于Hive输入也是支持合并方式读取的，参数配置参考： 1234# 执行Map前进行小文件合并set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormatset mapreduce.input.fileinputformat.split.maxsize=1073741824set mapreduce.input.fileinputformat.split.minsize=1073741824 注意以上Mapreduce和Hive Sql使用CombineFileInputFormat方式并不会缓解NameNode内存管理问题，因为合并的文件并不会持久化保存到磁盘。只是提高Mapreduce和Hive作业的性能。 3.3 更换存储介质技术架构中没有最牛逼的技术，只有最合适的技术。HDFS并不适合小文件，那么可以改变存储介质。用HBase来存储小文件，也是比较常见的选型方案。HBase在设计上主要为了应对快速插入、存储海量数据、单个记录的快速查找以及流式数据处理。适用于存储海量小文件（小于10k）,并支持对文件的低延迟读写。 HBase同一类的文件存储在同一个列族下面，若文件数量太多，同样会导致regionserver内存占用过大，JVM内存过大时gc失败影响业务。根据实际测试，无法有效支持10亿以上的小文件存储。 3.4 客户端写入规范解决小文件问题的最简单方法就是在生成阶段就避免小文件的产生。 3.4.1 Hive 写入优化Hive SQL执行的背后实际是Mapreduce任务，所有优化涉及大量小文件读优化、总结过程小文件优化、输出小文件邮件。本次只介绍最后一种优化场景。主要优化目标就是减少Reduce数量和增加Reduce处理能力，涉及参数有： 123456# 设置reduce的数量set mapred.reduce.tasks = 1;# 每个reduce任务处理的数据量，默认为1Gset hive.exec.reducers.bytes.per.reducer = 1000000000# 每个任务最大的reduce数set hive.exec.reducers.max = 50 还可以对输出数据进行压缩，涉及参数有： 12345678# 开启hive最终输出数据压缩功能set hive.exec.compress.output=true;# 开启mapreduce最终输出数据压缩set mapreduce.output.fileoutputformat.compress=true;# 设置数据输出压缩方式set mapreduce.output.fileoutputformat.compress.codec =org.apache.hadoop.io.compress.SnappyCodec; # 设置mapreduce最终数据输出压缩为块压缩set mapreduce.output.fileoutputformat.compress.type=BLOCK; 另外就是输出结果进行归档，这在前文已介绍。 3.5 合并小文件系统3.5.1 离线合并HDFS提供命令将hdfs多个文件合并后下载到本地文件系统，然后可以将文件重新上传HDFS文件系统。 1hdfs dfs -getmerge hdfs文件1 hdfs文件2 hdfs文件3 输出本地文件名 这显然是低效的。 3.5.2 脚本实现当HDFS文件系统由于疏忽已经受到小文件系统侵扰时，就需要我们被动治理了。通常的做法是检查所有文件系统，并确认哪些文件夹中的小文件过年需要合并。可以通过自定义的脚本或程序。例如通过调用HDFS的sync()方法和append()方法，将小文件和目录每隔一定时间生成一个大文件，或者可以通过写程序来合并这些小文件。 这里推荐一个开源工具File Crush。 1https://github.com/edwardcapriolo/filecrush/ 另外还有文章《分析hdfs文件变化及监控小文件》可以参考。 3.6 HDFS项目解决方案前文小文件治理方案虽然能够解决小文件的问题，但是这些方法都有不足或成本较高。那就需要在架构设计进行根本优化解决，目前 Hadoop 社区已经有很多相应的讨论和架构规划。例如下面的提案： HDFS-7836(将BlocksMap放在堆外) ，未实现； HDFS-8286 (将元数据保存在LevelDB)，已实现； HDFS-8998(一个Block对应多个文件)，未实现； 但是提案进度堪忧呀，大部分还处于讨论或推进一部分就没后续了…….，所以提出问题简单，解决问题难呀。扯远了，本文只挑两个简单看一下。 3.6.1 HDFS-8998目前HDFS版本中一个Block只能对应一个文件。社区和项目组考虑：能否一个Block能对应多个小文件。这就是解决提案：《Small files storage supported inside HDFS》，提案编号：HDFS-8998，提案参考说明文档。 主要的设计目标为： 为小文件设置单独的区域，称为Small file zone，用于小文件创建和写操作; NameNode 保存一个固定大小的 Block List，列表的大小是可以配置的； 当client1第一次向 NameNode 发出写请求时，NameNode 将为client1创建第一个 blockid，并锁定这个Block； 当其他客户端client2向 NameNode 发出写请求时，NameNode 将尝试为其分配未锁定的块（unlocked block），如果没有，并且现有的块数小于Block List，那么 NameNode 则为client2分配创建新的 blockid ，并且同时锁定。 其他客户端Client3...client N类似； 客户端如果获取不到未锁定的块资源，并且也不能新建（Block List资源上限）。这时候需要客户端等待其他客户端释放Block资源。 客户端写数据是将数据追加(appenging)到块上； 当客户端的读写（OutputStream）关闭，被客户端占用的Block将被释放； 当某个块被写满，也会分配新的一个块给客户端。这个写满的Block将从Block List中移除，同时会添加新的Block资源进行资源List。 新的设计中一个 Block 将包含多个文件。需要新的文件操作设计： 读取，读写小文件和平常读取hdfs文件类似。 删除，新的设计小文件是 Block 的一部分（segment），所以删除操作不能直接删除一个 Block。删除操作调整为：从 NameNode 中的 BlocksMap 删除 INode；然后当这个块中被删除的数据达到一定的阈值（可配置） ，对应的块将会被重写。 append 和 truncate，对小文件的 truncate 和 append 是不支持的，因为这些操作代价非常高，而且是不常用的。会增加后台进程对Block的小文件segment进行合并（segment合并触发数量可配置） 高可用：继承原架构的副本机制； 3.6.2 HDFS-8286提案：《Scaling out the namespace using KV store》，提案编号：HDFS-8286。该提案目标调整元数据的存储形式，从内存调整外置存储( KV存储系统)。现HDFS中以层次结构的形式来管理文件和目录，文件和目录表示为inode 对象。调整为KV存储，核心解决的问题是设计合适数据结构将元数据以KV格式存储。 详细设计就不展开了，可以参考提示设计说明文档。 3.6.3 Hadoop Ozone项目如果线上的业务数据是非结构化的小数据对象，例如海量图片（如银行业务保存的文件影像数据）、音频、小视频等。这种类型数据可以有适合的存储方式，对象存储。而Hadoop生态圈也正好有个项目。 Ozone 是 Hortonworks 基于 HDFS 实现的对象存储，OZone与HDFS有着很深的关系，在设计上也对HDFS存在的不足做了很多改进，使用HDFS的生态系统可以无缝切换到OZone，参考提案: HDFS-7240。 目前项目已经成为 Apache Hadoop的子项目。已经告别alpha版本阶段，最新的Release 1.1.0 版本已发布。 Ozone 和HDFS相同，也是采用 Master/Slave 架构，但是对管理服务namespace和BlockManager进行拆分，将元数据管理分成两个，一个是 Ozone Manager 作为对象存储元数据服务，另一个是 StorageContainerManager，作为存储容器管理服务。 另外Ozone Manager和StorageContainerManager的元数据都是使用RocksDB 进行单独存储，而不是放在NameNode内存中，架构上不再被堆内存限制，可以横向扩展。 第四部分 总结 技术没有银弹，只有合适的技术 实际生产环境面对HDFS小文件问题，需要提前管控业务写入，优化写入的客户端程序。为业务数据选择合适的数据存储方式，因地适宜。 追根溯源 当我们面多小文件的问题时，需要检查业务数据处理流，定位小文件产生的根因。 量力而行 需要评估企业大数据团队的技术能力。如果没有能力对原架构进行二次开发优化，就需要编写一些自定义程序来处理小文件。尽量摸透开源架构的特性，做好参数优化。 参考文献及资料[1] HDFS NameNode内存全景，链接：https://tech.meituan.com/2016/08/26/namenode.html [2] The Small Files Problem，链接：https://blog.cloudera.com/the-small-files-problem/ [3] HDFS Federation在美团点评的应用与改进，链接：https://tech.meituan.com/2017/04/14/hdfs-federation.html [4]Introducing Apache Hadoop Ozone: An Object Store for Apache Hadoop,链接：https://blog.cloudera.com/introducing-apache-hadoop-ozone-object-store-apache-hadoop/]]></content>
      <categories>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤算法总结]]></title>
    <url>%2F2020%2F09%2F12%2F2020-09-12-%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 第二部分 第三部分 参考文献及资料 背景在数据处理中，我们经常有这样的需求，判断某个元素是否在一个指定集合中。最朴素的处理方法是首先存储指定集合中数据，然后查找集合中数据，如有数据和查找元素相等即归属于该集合。 在数学中，如果只利用集合的定义属性，查找只能通过穷举遍历。如果集合元素较大，势必会影响查找的性能。 这时候对于集合数据类型使用特殊的数据架构实现，这就是Hash表（散列表）。集合数据结构由三个部分组成： 数据坐标集合B。 原始数据集合A。 Hash Function。hash函数是原始集合A到坐标集合的映射。 在这种数据结构下，判断元素是否属于指定集合，只需要计算该元素在hash函数作用下的坐标值。而这个坐标设置为数组的坐标，通过数组坐标就可以获取这个地址空间存储的值，最后判断是否和元素相等。 上面是最朴素的原理，在实际中由于hash函数的特点，存在集合A中不同值在hash函数映射下，坐标值可能是相同的，这就是hash冲突。具体细节后续介绍。 Hash表的实现本质思想是“空间换取时间”，对于特别大的集合，这种换取需要海量的内存存储空间。例如经常列举的案例就是垃圾邮件过滤场景。 引用自吴军的《数学之美》。Yahoo, Hotmail 和 Gmail 等公众电子邮件提供商，需要过滤垃圾邮件。一个办法就是记录下那些发垃圾邮件的 email 地址。由于那些发送者不停地在注册新的地址，全世界少说也有几十亿个发垃圾邮件的地址，将他们都存起来则需要大量的存储。如果用哈希表，每存储一亿 个 email 地址， 就需要 1.6 GB 的内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹， 然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6 GB内存资源）。因此存储几十亿个邮件地址可能需要上百 GB 的内存。 那么我们重新分析一下我们业务场景，实际核心需求是：判断元素是否重复，并不需要存储集合中具体数据。 数据坐标集合B，在数学中我们有很多实现方式（空间坐标等）。但是在计算机科学中，我们需要使用基础数据结构和运算方式。布隆（Burton Howard Bloom）在1970年提出布隆过滤器算法，算法中使用位阵列（Bit Array）作为坐标集合。接下来我们将详细介绍。 第一部分 Hash函数1.1 Hash函数定义Hash函数，通常音译为哈希函数（也有翻译成：散列函数）。Hash函数首先是数学意义上的函数：$$Hash\ Function\ F：A-&gt;B$$其中定义域集合A是不等长的字符串集合，而值域集合B中字符串是固定长度。 理论上满足这样的函数的海量的，我们在挑选好的Hash函数时，通常有下面的标准： Hash函数在计算值域的时候是高效的（对于长度为n的字符串计算时间复杂度应为O(n)）。 确定性。对于任何给定的输入，哈希函数必须始终给出相同的结果。即函数值的确定性。 Hash碰撞概率低。由于值域集合中字符串是固定长度的，那么肯定是一个有限集合。例如SHA256算法值域大小（集合的势）为2^256。在我们进行2^256+1次输入时，必然会发生一次碰撞（$$x!=y,F(x)=F(y)$$）。所以选取的Hash函数针对具体场景，需要具有较低的碰撞概率。 隐蔽性。通俗的讲就是不能通过函数值F(x)，反向计算出x。这就是计算理论和密码学中单向函数概念。由于这个特性Hash函数大量应用于加密场景。 值域集合分布均匀。谈到分布那么值域空间就引入了距离的概念。通俗的讲就是点与点之间打散在空间中，没有聚集现象。 Hash函数将一个空间A中的数据映射到另一个空间B（坐标空间）中数据，通常集合B小于集合A（集合的势）。数据空间A定义成Hash表，在数据初始化和插入过程需要计算Hash坐标，并存储。这就完成了将计算时间（或计算消耗）转换成存储空间的思想。 1.2 Hash函数种类通常按照Hash函数的实现原理分为：加法Hash、位运算Hash、乘法Hash、除法Hash、查表Hash、混合Hash。 1.2.1 加法HashHash将字符串字符相加并处理后形成结果。下面案例同余质数。 123456789101112public static void main(String[] args) &#123; System.out.println(additiveHash("Secure Hash Algorit",19)); // 输出3&#125;public static int additiveHash(String key, int prime)&#123; int hash,i; for(hash=key.length(),i=0;i&lt;key.length();i++)&#123; hash+=key.charAt(i); &#125; return hash%prime;&#125; 1.2.2 位运算Hash这类型Hash函数通过利用各种位运算（常见的是移位和异或等）来充分的变换输入元素。 1234567891011public static void main(String[] args) &#123; System.out.println(rotatingHash("Secure Hash Algorit",19)); //6&#125;public static int rotatingHash(String key,int prime)&#123; int hash,i; for(hash = key.length(),i=0;i&lt;key.length();i++)&#123; hash=(hash&lt;&lt;1)^(hash&gt;&gt;10)^key.charAt(i)&amp;key.charAt(i); &#125; return hash%prime;&#125; 1.2.3 乘法Hash这种类型的Hash函数利用了乘法的不相关性. 123456789101112public static void main(String[] args) &#123; System.out.println(bernstein("Secure Hash Algorit")); //361558494&#125; public static int bernstein(String key)&#123; int hash=0; int i; for(i=0;i&lt;key.length();i++)&#123; hash=hash*10+key.charAt(i); &#125; return hash;&#125; 1.2.4 除法Hash因为除法太慢，这种方式几乎找不到真正的应用。 1.2.5 查表Hash查表Hash中有名的例子有：Universal Hashing和Zobrist Hashing。他们的表格都是随机生成的。 1.2.6 混合Hash混合Hash算法利用了以上各种方式。各种常见的Hash算法，比如MD5、Tiger都属于这个范围。它们一般很少在面向查找的Hash函数里面使用。 1.3 Hash函数应用Hash函数主要应用有： 安全加密 密钥加密通常使用MD5和SHA系列算法。SHA系列有五个算法，分别是 SHA-1、SHA-224、SHA-256、SHA-384，和SHA-512。后四者有时并称为 SHA-2。SHA-1在许多安全协定中广为使用，包括 TLS/SSL 等，是 MD5 的后继者。 SHA-256可能是所有加密哈希函数中最著名的，因为它已在区块链技术中广泛使用。中本聪的原始比特币协议中使用了它。 唯一标识 文件之类的二进制数据做 md5 处理，作为唯一标识，这样判定重复文件的时候更快捷。 数据校验 比如从网上下载的很多文件（尤其是P2P站点资源），都会包含一个 MD5 值，用于校验下载数据的完整性，避免数据在中途被劫持篡改。 分布式缓存 分布式缓存和其他机器或数据库的分布式不一样，因为每台机器存放的缓存数据不一致，每当缓存机器扩容时，需要对缓存存放机器进行重新索引（或者部分重新索引），这里应用到的也是哈希算法的思想。 负载均衡 对于同一个客户端上的请求，尤其是已登录用户的请求，需要将其会话请求都路由到同一台机器，以保证数据的一致性，这可以借助哈希算法来实现，通过用户 ID 尾号对总机器数取模（取多少位可以根据机器数定），将结果值作为机器编号。 第二部分 布隆过滤2.1 原理前文我们讨论了“判断某个元素是否在一个指定集合中”问题的实现思路。在数据量较小的情况下，我们可以使用经典的HashMap数据结构。对于大量数据场景下，布隆（Burton Howard Bloom）在1970年提出布隆过滤器算法。 2.2 Java实现2.2 布隆过滤的应用Google 著名的分布式数据库 Bigtable 使用了布隆过滤器来查找不存在的行或列，以减少磁盘查找的IO次数［3］。 Squid 网页代理缓存服务器在 cache digests 中使用了也布隆过滤器［4］。 Venti 文档存储系统也采用布隆过滤器来检测先前存储的数据［5］。 SPIN 模型检测器也使用布隆过滤器在大规模验证问题时跟踪可达状态空间［6］。 Google Chrome浏览器使用了布隆过滤器加速安全浏览服务［7］。 参考文献及资料1、数学之美二十一：布隆过滤器（Bloom Filter）：http://www.google.com.hk/ggblog/googlechinablog/2007/07/bloom-filter_7469.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven项目中读取properties配置文件总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-Maven%E9%A1%B9%E7%9B%AE%E4%B8%AD%E8%AF%BB%E5%8F%96properties%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 读取properties配置文件方法 参考文献及资料 背景在java开发中，通常将配置信息存储在特定的配置文件中，而不是内嵌在程序代码中。即代码可配置化。properties文件以key=value键值对形式表达，结构比较简单，但是难以表达层次，适合小型项目。本文总结汇总了读取properties配置文件方法。 第一部分 读取properties配置文件方法为了讲解方便，我们在maven项目的资源目录resources中创建配置文件application.properties。文件内容为： 12username=apppassword=password123 1.1 基于ClassLoder的getResourceAsStream方法读取配置文件本方法基于ClassLoder的getResourceAsStream方法，通过类加载器来定位资源，返回InputStream后用Properties对象进行加载。 123456789public class configRead &#123; public static void main(String[] args) throws IOException &#123; InputStream in = configRead.class .getClassLoader().getResourceAsStream("application.properties"); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); &#125;&#125; 1.2 基于getResourceAsStream()方法读取配置文件利用class的getResourceAsStream方法来定位资源文件，并且直接返回InputStream对象，然后通过Properties进行加载。 12345678910public class configRead &#123; public static void main(String[] args) throws IOException &#123; InputStream In =configRead.class .getResourceAsStream("application.properties"); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); //app &#125;&#125; 1.3 基于ClassLoader类的getSystemResourceAsStream()静态方法读取配置文件使用ClassLoader的getSystemResourceAsStream()静态方法来定位资源，并且返回InputStream，最后用Properties来加载。 123456789public class configRead &#123; public static void main(String[] args) throws IOException &#123; InputStream in = ClassLoader .getSystemResourceAsStream("application.properties"); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); &#125;&#125; 1.4 基于FileInputStream读取配置文件这种方法通过类的路径来定位properties文件资源的路径，然后通过FileInputStream读取流，最后通过java.util.Properties类的load()方法来加载数据。 1234567891011121314public class configRead &#123; public static void main(String[] args) throws IOException &#123; URL url = configRead.class.getClassLoader() .getResource("application.properties"); if (url != null) &#123; String fileName = url.getFile(); InputStream in = new BufferedInputStream( new FileInputStream(fileName)); Properties properties = new Properties(); properties.load(in); System.out.println(properties.getProperty("username")); &#125; &#125;&#125; 1.5 基于ResourceBundle读取配置文件利用ResourceBundle来读取properties文件。 12345678public class configRead &#123; public static void main(String[] args) throws IOException &#123; ResourceBundle resourceBundle = ResourceBundle .getBundle("application.properties", locale1); System.out.println(resourceBundle.getString("username")); &#125;&#125; 1.6 基于PropertyResourceBundle读取配置文件PropertyResourceBundle是ResourceBundle的子类，同样我们也可以利用PropertyResourceBundle来加载配置文件的数据，具体的示例如下： 12345678910public class configRead &#123; public static void main(String[] args) throws IOException &#123; URL url = configRead.class.getClassLoader().getResource("application.properties"); if (url != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(url.getFile())); ResourceBundle resourceBundle = new PropertyResourceBundle(in); System.out.println(resourceBundle.getString("username")); &#125; &#125;&#125; 参考文献及资料1、Properties files，链接：https://commons.apache.org/proper/commons-configuration/userguide/howto_properties.html]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Maven项目中resources配置总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-Maven%E9%A1%B9%E7%9B%AE%E4%B8%ADresources%E9%85%8D%E7%BD%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 基本配置介绍 第二部分 具体配置和注意事项 第三部分 读取resources资源 参考文献及资料 背景通常Maven项目的文件目录结构如下： 1234567891011121314151617181920212223242526# Maven项目的标准目录结构src main java #源文件 resources #资源文件 filters #资源过滤文件 config #配置文件 scripts #脚本文件 webapp #web应用文件 test java #测试源文件 resources #测试资源文件 filters #测试资源过滤文件 it #集成测试 assembly #assembly descriptors site #Sitetarget generated-sources classes generated-test-sources test-classes xxx.jarpom.xmlLICENSE.txtNOTICE.txtREADME.txt 其中src/main/resources和src/test/resources是资源文件目录。本文将详细介绍资源文件相关的配置。 第一部分 基本配置介绍我们在使用Maven组件来构建项目的时候，通常将配置文件放在资源文件目录下。针对这个目录，在pom.xml文件进行了定义，我们首先看一个案例： 123456789101112131415161718&lt;build&gt;&lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;application.properties&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;excludes&gt; &lt;exclude&gt;application.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt;&lt;/resources&gt;&lt;/build&gt; 标签&lt;directory&gt;指定资源文件目录； 标签&lt;include&gt;指定资源文件目录中，哪些文件被打包。 标签&lt;excludes&gt;指定资源文件目录中，哪些文件不被打包。 特别的，标签&lt;filtering&gt;是一个bool值，默认值为false。在maven资源文件中，支持使用变量placeholder，例如资源文件： 123# application.propertiesapplication.user=$&#123;username&#125;application.password=$&#123;password&#125; 文件中使用${keyword}占位符来标识变量。这时候可以在pom.xml文件中定义变量的取值： 1234&lt;properties&gt; &lt;username&gt;mysql&lt;/username&gt; &lt;password&gt;password123&lt;/password&gt;&lt;/properties&gt; 如果需要对配置文件中变量进行替换实际值，就需要开启&lt;filtering&gt;，该值设置为true。 第二部分 具体配置和注意事项2.1 案例说明根据上面的介绍，最开始例子中有两段resource的配置描述，分别的含义为： 第一个配置的含义是：在配置文件目录src/main/resources过滤掉其他文件，只保留application.properties文件。并且开启filtering变量替换属性。 第二个配置的含义是：在配置文件目录src/main/resources过滤掉application.properties文件，其他文件均保留。并且关闭filtering变量替换属性。 需要特别注意的是，这里两个&lt;resources&gt;都是对资源目录&lt;src/main/resources&gt;的配置定义，一个是保留application.properties，一个是去除application.properties。这样两个配置会不会冲突？实际上两个配置是兼容。最后是取两个配置分别过滤的文件集合的并集。 可以看一下例子，资源目录src/main/resources里面有三个文件： 123application.ymlapplication.propertiesapplication.xml 编译后，target/classes路径中三个配置文件都是有的。第一配置文件过滤后文件集合为{application.properties}，第二个配置过滤后的集合为{application.yml,application.xml},最后取并集就得到了最后编译结果。 2.2 正则过滤在对资源目录中文件进行过滤时，还支持正则表达式。例如： 1&lt;include&gt;**/*.xml&lt;/include&gt; 这个表达式表示包含了资源目录下面所有xml文件（以及子目录下面）。 2.3 变量占位符这里主要指的是&lt;filtering&gt;的功能。例如下面的xml文件定义了一个研发&lt;profile&gt;。 12345678910111213&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;resource.delimiter&gt;$&#123;&#125;&lt;/resource.delimiter&gt; &lt;username&gt;mysql&lt;/username&gt; &lt;password&gt;password123&lt;/password&gt; &lt;/properties&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;/profiles&gt; 配置中定义的username和password两个变量的值。使用package -P dev编译后，配置文件中占位符变量被替换： 12application.user=mysqlapplication.password=password123 需要注意的是这里增加了&lt;resource.delimiter&gt;标签配置，定义了占位符的格式。有些时候其他依赖包的pom文件也会指定占位符的格式，就会造成格式不统一。例如：spring boot把默认的占位符号${}改成了@var@。所以建议进行配置，否则容易环境”污染”。 2.4 关于一个错误观点的说明有很多关于这个主题的文章（例如CSND）中，认为同一个&lt;resource&gt;中，若是&lt;include&gt;和&lt;exclude&gt;都存在的话，那就发生冲突了，这时会以&lt;exclude&gt;为准。 关于这个论点，笔者实际做了实验，同一个&lt;resource&gt;中，同时配置了&lt;include&gt;和&lt;exclude&gt;。 1234567891011121314&lt;build&gt;&lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;includes&gt; &lt;include&gt;application.properties&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;application.properties&lt;/exclude&gt; &lt;/excludes&gt; &lt;/resource&gt;&lt;/resources&gt;&lt;/build&gt; 编译结果，配置文件没有打包进入target/classes。说明这个论点是有问题的。说明在同一个resource中两种配置是取交集的。 2.5 子目录资源目录也是支持子目录的。即可以在资源目录下面创建子目录，在打包过程中会保留子目录结构。例如： 123456resources -test --app.xml -application.yml -application.properties -application.xml 在项目编译后，如果子目录中资源文件被保留，那么子目录的结构也是保留的。例如： 1234567target -classes --test ---app.xml -application.yml -application.properties -application.xml 第二部分 读取resources资源例如我们的配置文件properties类型的配置文件，可以使用下面的语句进行读取： 方法1，从编译后的整个classes目录下去找； 1InputStream is = this.getClass().getResourceAsStream("/" +application.properties); 方法2，ClassLoader从整个classes目录找； 1InputStream is = this.getClass().getClassLoader().getResourceAsStream(application.properties); 读取使用Java的工具包java.util.Properties： 12345678import java.util.Properties;Properties properties = new Properties();InputStream is = this.getClass().getClassLoader().getResourceAsStream(application.properties);properties.load(is)//获取配置文件中name的配置值System.out.println(properties.get(getProperty("name"))) 其他类型的配置文件读取读者可以执行查找资料。 参考文献及资料1、Maven Resources Plugin，链接：https://maven.apache.org/components/plugins-archives/maven-resources-plugin-2.6/ 2、Maven资源过滤的配置，链接：http://c.biancheng.net/view/5285.html]]></content>
      <categories>
        <category>Maven</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统三种IP区别总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-10-01-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B8%89%E7%A7%8DIP%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 三种IP介绍 第二部分 localhost 参考文献及资料 背景在我们配置服务监听地址的时候，会面对服务器上三种”IP“: 本机IP。例如：192.168.1.1、8.8.8.8； 0.0.0.0地址； 127.0.0.1地址； 那么这三种在使用上有哪些区别呢？本文将总结介绍。 第一部分 三种IP介绍1.1 本机IP对于linux系统，我们输入ifconfig命令会回显一个eth0网卡信息： 12345678eth0 Link encap:Ethernet HWaddr 98:3f:9f:18:25:97 inet addr:192.168.1.3 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::519e:af46:9443:4f0d/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:431456 errors:0 dropped:0 overruns:0 frame:0 TX packets:248421 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:405352090 (405.3 MB) TX bytes:35380000 (35.3 MB) 这就是有线网卡的信息。如果有无线网卡还会有一个wlan网卡信息。这是局域网给计算机分配的局域网唯一的IP。如果计算机直接接入互联网，会分配一个公网IP（互联网唯一IP）。 这个IP用于和局域网或互联网中其他机器通信的唯一IP，即网络中唯一标识。 1.2 0.0.0.0地址0.0.0.0表示“本地计算机上的所有IP地址”（所有IPv4地址）。因此，如果服务器有两个IP地址:192.168.1.1和8.1.2.1。如果这时候如果我们在服务器上开启一个http服务（端口8080），配置文件中监听地址为：0.0.0.0。那么我们可以通过下面两个url访问服务： http://192.168.1.1:8080 http://8.1.2.1:8080 但是我们如果监听地址配置成192.168.1.1，那么http://8.1.2.1:8080就无法访问服务了。 1.3 127.0.0.1地址对于linux系统，我们输入ifconfig命令会回显一个lo网卡信息： 12345678lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:415 errors:0 dropped:0 overruns:0 frame:0 TX packets:415 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:37794 (37.7 KB) TX bytes:37794 (37.7 KB) 127.0.0.1地址分配给 loopback 接口，也称为回环地址(loopback address)。用来测试本机的 TCP/IP 协议栈。loopback是一个特殊的网络接口(可理解成虚拟网卡)，用于本机中各个应用之间的网络交互。只要操作系统的网络组件是正常的，loopback 就能工作。 如果我们服务监听的地址配置成127.0.0.1，那么只能本机服务能访问了。 第三部分 localhostlocalhost是一个域名，而不是一个ip地址。localhost域名配置在本地DNS中，即/etc/hosts文件中定义，例如： 1127.0.0.1 localhost 例如我们ping localhost，就会解析成127.0.0.1地址。 12root@deeplearning:# ping localhostPING localhost (127.0.0.1) 56(84) bytes of data. 如果我们服务监听的地址配置成localhost，那么会根据/etc/hosts中的配置，解析成127.0.0.1地址。这时候也只能本机服务能访问了。 参考文献及资料1、0.0.0.0地址，链接：https://en.wikipedia.org/wiki/0.0.0.0 2、IP地址，链接：https://zh.wikipedia.org/zh/IP%E5%9C%B0%E5%9D%80]]></content>
      <categories>
        <category>IP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java中定时任务]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-17-Java%E4%B8%AD%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景参考文献及资料1、https://www.cnblogs.com/wenbronk/p/6433178.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[配置kerberos认证的Kafka集群交互介绍]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-%E9%85%8D%E7%BD%AEkerberos%E8%AE%A4%E8%AF%81%E7%9A%84Kafka%E9%9B%86%E7%BE%A4%E4%BA%A4%E4%BA%92%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景https://docs.spring.io/spring-kafka/api/org/springframework/kafka/core/KafkaTemplate.html https://blog.csdn.net/justry_deng/article/details/88387898 Spark Kafka Consumer in secure( Kerberos) enviornment Raw SparkKafkaIntegration.md Sample Applicationusing direct stream1234567891011121314151617181920212223242526272829303132import kafka.serializer.StringDecoder;import org.apache.spark.SparkConfimport org.apache.spark.streaming._import org.apache.spark.streaming.kafka._object SparkKafkaConsumer2 &#123; def main(args: Array[String]) &#123; // TODO: Print out line in log of authenticated user val Array(brokerlist, group, topics, numThreads) = args var kafkaParams = Map( "bootstrap.servers"-&gt;"rks253secure.hdp.local:6667", "key.deserializer" -&gt;"org.apache.kafka.common.serialization.StringDeserializer", "value.deserializer" -&gt;"org.apache.kafka.common.serialization.StringDeserializer", "group.id"-&gt; "test", "security.protocol"-&gt;"PLAINTEXTSASL", "auto.offset.reset"-&gt; "smallest" ) val sparkConf = new SparkConf().setAppName("KafkaWordCount") val ssc = new StreamingContext(sparkConf, Seconds(100)) val kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder] (ssc, kafkaParams, Set(topics)) // TODO: change to be a variable kafkaStream.saveAsTextFiles("/tmp/streaming_output") ssc.start() ssc.awaitTermination() &#125;&#125; using createStream1234567891011121314151617181920212223242526272829303132333435import kafka.serializer.StringDecoderimport org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.Secondsimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.kafka.KafkaUtilsobject Kafka_Word_Count &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName("KafkaWordCount") .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .set("spark.driver.allowMultipleContexts", "true") val ssc = new StreamingContext(conf, Seconds(3)) val groupID = "test" val numThreads = "2" val topic = "kafkatopic" val topicMap = topic.split(",").map((_, numThreads.toInt)).toMap val kafkaParams = Map[String, String]( "zookeeper.connect" -&gt; "rks253secure.hdp.local:2181", "group.id" -&gt; groupID, "zookeeper.connection.timeout.ms" -&gt; "10000", "security.protocol"-&gt;"PLAINTEXTSASL" ) val lines = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicMap, StorageLevel.MEMORY_AND_DISK_SER_2).map(_._2) lines.print() ssc.start() ssc.awaitTermination() &#125;&#125; kafka_jaas.conf (for spark local mode)123456789101112131415161718192021222324KafkaServer &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/kafka.service.keytab" storeKey=true useTicketCache=false serviceName="kafka" principal="kafka/rks253secure.hdp.local@EXAMPLE.COM"; &#125;; KafkaClient &#123; com.sun.security.auth.module.Krb5LoginModule required useTicketCache=true renewTicket=true serviceName="kafka"; &#125;; Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/kafka.service.keytab" storeKey=true useTicketCache=false serviceName="zookeeper" principal="kafka/rks253secure.hdp.local@EXAMPLE.COM"; &#125;; kafka_jaas.conf (for spark yarn client mode)12345678910111213141516KafkaClient &#123;com.sun.security.auth.module.Krb5LoginModule requireduseKeyTab=truekeyTab="/etc/security/keytabs/kafka.service.keytab"serviceName="kafka"principal="kafka/rks253secure.hdp.local@EXAMPLE.COM";&#125;;Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab="/etc/security/keytabs/kafka.service.keytab" storeKey=true useTicketCache=false serviceName="zookeeper" principal="kafka/rks253secure.hdp.local@EXAMPLE.COM";&#125;; Spark Submit commandkinit from kafka user.. 12spark-submit --files /etc/kafka/conf/kafka_jaas.conf,/etc/security/keytabs/kafka.service.keytab --conf "spark.executor.extraJavaOptions=-Djava.security.auth.login.config=/etc/kafka/conf/kafka_jaas.conf" --driver-java-options "-Djava.security.auth.login.config=/etc/kafka/conf/kafka_jaas.conf" --class SparkKafkaConsumer2 --master local[2] /tmp/SparkKafkaSampleApp-1.0-SNAPSHOT-jar-with-dependencies.jar "rks253secure.hdp.local:6667" test kafkatopic 1 参考文献及资料1、]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring框架下Kafka交互总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-10-01-Java%E4%B8%AD%E6%97%A5%E5%BF%97%E5%A4%84%E7%90%86%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景https://www.cnblogs.com/qlqwjy/p/9275415.html参考文献及资料1、Spring for Apache Kafka，链接：https://spring.io/projects/spring-kafka]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[提交Spark任务到kerberos安全集群]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-%E6%8F%90%E4%BA%A4Spark%E4%BB%BB%E5%8A%A1%E5%88%B0kerberos%E5%AE%89%E5%85%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景在生产Hadoop集群上，通常配置了kerberos认证服务，需要通过认证后，才能和集群中其他服务进行交互。例如像安全集群提交Spark任务。 第一部分 实现方式1123UserGroupInformation.setConfiguration(SparkHadoopUtil.get().newConfiguration(sparkConfiguration)); Credentials credentials = UserGroupInformation.getLoginUser().getCredentials(); SparkHadoopUtil.get().addCurrentUserCredentials(credentials); 第二部分 实现方式212345SparkConf sparkConfiguration = new SparkConf();sparkConfiguration.set("spark.hadoop.hadoop.security.authentication", "kerberos");sparkConfiguration.set("spark.hadoop.hadoop.security.authorization", "true");sparkConfiguration.set("spark.hadoop.dfs.namenode.kerberos.principal","hdfs/_HOST@EXAMPLE.COM");sparkConfiguration.set("spark.hadoop.yarn.resourcemanager.principal", "yarn/&lt;resource-manager-host-name&gt;@EXAMPLE.COM"); 参考文献及资料]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring框架下Kafka交互总结]]></title>
    <url>%2F2020%2F09%2F03%2F2021-03-13-Java%E4%B8%AD%E6%97%A5%E5%BF%97%E4%BD%93%E7%B3%BB%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景https://www.cnblogs.com/qlqwjy/p/9275415.html参考文献及资料1、Spring for Apache Kafka，链接：https://spring.io/projects/spring-kafka]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring框架下Kafka交互总结]]></title>
    <url>%2F2020%2F09%2F03%2F2020-09-03-Spring%E6%A1%86%E6%9E%B6%E4%B8%8BKafka%E4%BA%A4%E4%BA%92%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景Java中与Kafka交互的API类包有： Kafka client Spring for Apache Kafka Spring Integration Kafka Spring Cloud stream binder Kafka 本文主要介绍Spring for Apache Kafka的使用。 第一部分 项目配置1.1 依赖将spring-kafka依赖项添加到项目中的pom.xml中： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.3.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 需要注意的是：Apache Kafka的Spring基于Java kafka-clientsjar，需要小心依赖兼容性。以下是兼容性对照表： Spring for Apache Kafka Version Spring Integration for Apache Kafka Version kafka-clients Spring Boot 2.6.0 5.3.x or 5.4.0-SNAPSHOT (pre-release) 2.6.0 2.3.x or 2.4.0-SNAPSHOT (pre-release) 2.5.x 3.3.x 2.5.0 2.3.x 2.4.x 3.2.x 2.4.1 2.2.x 2.3.x 3.2.x 2.3.1 2.2.x 2.2.x 3.1.x 2.0.1, 2.1.x, 2.2.x 2.1.x 2.1.x 3.0.x 1.0.2 2.0.x (End of Life) 1.3.x 2.3.x 0.11.0.x, 1.0.x 1.5.x (End of Life) 1.2 Kafka配置在src/mian/java/resources中创建配置文件application.yml: 12345678910111213server: port: 9000spring: kafka: consumer: bootstrap-servers: localhost:9092 group-id: group_id auto-offset-reset: earliest key-deserializer: org.apache.kafka.common.serialization.StringDeserializer value-deserializer: org.apache.kafka.common.serialization.StringDeserializer producer: bootstrap-servers: localhost:9092 key-serializer: org.apache.kafka.common.serialization.StringSerializer value-serializer: org.apache.kafka.common.serialization.StringSerializer 第二部分 Kafka生产消费2.1 Kafka生产者Spring for Apache Kafka 2.2 Kafka消费者参考文献及资料1、Spring for Apache Kafka，链接：https://spring.io/projects/spring-kafka]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Intellij IDEA编辑器中Java代码块的快捷方式]]></title>
    <url>%2F2020%2F08%2F15%2F2020-08-15-Intellij%20IDEA%E7%BC%96%E8%BE%91%E5%99%A8%E4%B8%ADJava%E4%BB%A3%E7%A0%81%E5%9D%97%E7%9A%84%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 常用快捷键 参考文献及资料 背景记录常用的ideal快捷输入； 第一部分 常用快捷键1.1 mian主函数快捷键psvm 123public static void main(String[] args) &#123; &#125; 1.2 输出快捷键sout 1System.out.println(); 1.3 循环遍历快捷键iter 123for (String arg : args) &#123; &#125; 1.4 生成公共静态finalpsf 1public static final 1.5 生成公共静态 final intpsfi 1public static final int 1.6 生成公共静态final Stringpsfs 1public static final String 参考文献及资料1、IntelliJ IDEA，链接：https://www.jetbrains.com/idea/]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka系列文章(第三篇 Kafka可视化管理界面)]]></title>
    <url>%2F2020%2F05%2F12%2F2020-05-11-Kafka%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%89%E7%AF%87Kafka%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AE%A1%E7%90%86%E7%95%8C%E9%9D%A2%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 kafka-manager安装 第二部分 kafka-manager配置 第三部分 kafka-manager管理 第四部分 总结 参考文献及资料 背景在Kafka的监控系统中有很多优秀的开源监控系统。比如Kafka-manager，open-faclcon，zabbix等主流监控工具均可直接监控kafka。Kafka集群性能监控可以从消息网络传输，消息传输流量，请求次数等指标来衡量集群性能。这些指标数据可以通过访问kafka集群的JMX接口获取。Kafka-manager工具由Yahoo研发的Kafka管理和监控工具，并在github上开源。 对于非加密Kafka集群配置Kafka manager，目前互联网也有大量的资料。而对于加密集群（特别是云端集群还配置了域名方式），参考材料较为匮乏。本文针对云端加密Kafka集群配置Kafka Manager进行详细介绍，供大家参考。 第一部分 kafka-manager安装1.1 版本选择版本使用cmak-3.0.0.0版本，依赖java11（使用openjdk-11+28_linux-x64_bin.tar.gz）。使用已经编译好的介质包cmak-3.0.0.0.zip。假设安装目录为/dmqs。 1.2 介质部署1.2.1 部署cmak上传cmak-3.0.0.0.zip至安装目录/dmqs,使用命令解压： 1f-itdw-4c8g-100g-11:/dmqs # unzip cmak-3.0.0.0.zip 1.2.2 部署java上传openjdk-11+28_linux-x64_bin.tar.gz介质到/dmqs/cmak-3.0.0.0路径： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # tar -zxvf openjdk-11+28_linux-x64_bin.tar.gz 重命名java路径名： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # mv jdk11 jdk 1.3 配置文件准备1.3.1 配置application.conf文件备份文件并修改： 12f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # cp application.conf application.conf.bakf-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # vi application.conf 调整afka-manager.zkhosts参数项的配置信息： 1kafka-manager.zkhosts="84.10.228.50:2181,84.10.228.55:2181,84.10.228.56:2181" 1.3.2 加密集群配置jaas文件如果是加密集群需要准备jaas文件，文件名为：kafka_server_jaas.conf。 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # touch kafka_server_jaas.conf 文件内容如下： 1234567891011KafkaClient &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";&#125;;Client &#123; org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret";&#125;; 上面配置中KafkaClient为和kafka通信配置；Client为和zookeeper通信配置。 1.3.3 配置consumer.properties 首先备份： 12f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # cp consumer.properties consumer.properties.bakf-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/conf # vi consumer.properties 配置文件调整为： 12345678910111213#security.protocol=PLAINTEXT#key.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer#value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeseriazerbootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093security.protocol=SASL_SSLsasl.mechanism=SCRAM-SHA-512ssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=itdw123 ssl.keystore.password=itdw123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.key.password=itdw123ssl.endpoint.identification.algorithm= 其中注释部分为源配置文件内容。 1.4 准备ca信任证书对于已经配置为域名方式的Kafka集群需要配置域名信任证书。 创建InstallCert.java，java程序文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185/* * Copyright 2006 Sun Microsystems, Inc. All Rights Reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * - Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * - Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * - Neither the name of Sun Microsystems nor the names of its * contributors may be used to endorse or promote products derived * from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ import java.io.*;import java.net.URL; import java.security.*;import java.security.cert.*; import javax.net.ssl.*; public class InstallCert &#123; public static void main(String[] args) throws Exception &#123; String host; int port; char[] passphrase; if ((args.length == 1) || (args.length == 2)) &#123; String[] c = args[0].split(":"); host = c[0]; port = (c.length == 1) ? 443 : Integer.parseInt(c[1]); String p = (args.length == 1) ? "changeit" : args[1]; passphrase = p.toCharArray(); &#125; else &#123; System.out.println("Usage: java InstallCert &lt;host&gt;[:port] [passphrase]"); return; &#125; File file = new File("jssecacerts"); if (file.isFile() == false) &#123; char SEP = File.separatorChar; File dir = new File(System.getProperty("java.home") + SEP + "lib" + SEP + "security"); file = new File(dir, "jssecacerts"); if (file.isFile() == false) &#123; file = new File(dir, "cacerts"); &#125; &#125; System.out.println("Loading KeyStore " + file + "..."); InputStream in = new FileInputStream(file); KeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType()); ks.load(in, passphrase); in.close(); SSLContext context = SSLContext.getInstance("TLS"); TrustManagerFactory tmf = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm()); tmf.init(ks); X509TrustManager defaultTrustManager = (X509TrustManager)tmf.getTrustManagers()[0]; SavingTrustManager tm = new SavingTrustManager(defaultTrustManager); context.init(null, new TrustManager[] &#123;tm&#125;, null); SSLSocketFactory factory = context.getSocketFactory(); System.out.println("Opening connection to " + host + ":" + port + "..."); SSLSocket socket = (SSLSocket)factory.createSocket(host, port); socket.setSoTimeout(10000); try &#123; System.out.println("Starting SSL handshake..."); socket.startHandshake(); socket.close(); System.out.println(); System.out.println("No errors, certificate is already trusted"); &#125; catch (SSLException e) &#123; System.out.println(); e.printStackTrace(System.out); &#125; X509Certificate[] chain = tm.chain; if (chain == null) &#123; System.out.println("Could not obtain server certificate chain"); return; &#125; BufferedReader reader = new BufferedReader(new InputStreamReader(System.in)); System.out.println(); System.out.println("Server sent " + chain.length + " certificate(s):"); System.out.println(); MessageDigest sha1 = MessageDigest.getInstance("SHA1"); MessageDigest md5 = MessageDigest.getInstance("MD5"); for (int i = 0; i &lt; chain.length; i++) &#123; X509Certificate cert = chain[i]; System.out.println (" " + (i + 1) + " Subject " + cert.getSubjectDN()); System.out.println(" Issuer " + cert.getIssuerDN()); sha1.update(cert.getEncoded()); System.out.println(" sha1 " + toHexString(sha1.digest())); md5.update(cert.getEncoded()); System.out.println(" md5 " + toHexString(md5.digest())); System.out.println(); &#125; System.out.println("Enter certificate to add to trusted keystore or 'q' to quit: [1]"); String line = reader.readLine().trim(); int k; try &#123; k = (line.length() == 0) ? 0 : Integer.parseInt(line) - 1; &#125; catch (NumberFormatException e) &#123; System.out.println("KeyStore not changed"); return; &#125; X509Certificate cert = chain[k]; String alias = host + "-" + (k + 1); ks.setCertificateEntry(alias, cert); OutputStream out = new FileOutputStream("jssecacerts"); ks.store(out, passphrase); out.close(); System.out.println(); System.out.println(cert); System.out.println(); System.out.println ("Added certificate to keystore 'jssecacerts' using alias '" + alias + "'"); &#125; private static final char[] HEXDIGITS = "0123456789abcdef".toCharArray(); private static String toHexString(byte[] bytes) &#123; StringBuilder sb = new StringBuilder(bytes.length * 3); for (int b : bytes) &#123; b &amp;= 0xff; sb.append(HEXDIGITS[b &gt;&gt; 4]); sb.append(HEXDIGITS[b &amp; 15]); sb.append(' '); &#125; return sb.toString(); &#125; private static class SavingTrustManager implements X509TrustManager &#123; private final X509TrustManager tm; private X509Certificate[] chain; SavingTrustManager(X509TrustManager tm) &#123; this.tm = tm; &#125; public X509Certificate[] getAcceptedIssuers() &#123; throw new UnsupportedOperationException(); &#125; public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException &#123; throw new UnsupportedOperationException(); &#125; public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException &#123; this.chain = chain; tm.checkServerTrusted(chain, authType); &#125; &#125;&#125; 上传至目的目录,并编译： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/javac InstallCert.java 编译后生成下面的文件： 1234f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # ll-rw-r--r-- 1 dmqs dmqs 975 May 28 02:23 InstallCert$SavingTrustManager.class-rw-r--r-- 1 dmqs dmqs 6126 May 28 02:23 InstallCert.class-rw-r--r-- 1 dmqs dmqs 6884 May 28 02:21 InstallCert.java 添加域名（kafka集群配置为域名方式）到jssecacerts文件中： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node1:9093 这时在当前目录就生成了jssecacerts文件。如果集群是多节点，需要将其他节点域名信息追加到这个文件中。执行命令即为： 12f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node2:9093f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # /dmqs/cmak-3.0.0.0/jdk/bin/java InstallCert kafka.itdw.node3:9093 这样就生成了集群所有的节点域名的信任证书。 最后将jssecacerts文件拷贝至jdk/lib/security： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0 # cp jssecacerts jdk/lib/security 完成所有配置的准备。 1.5 服务启动完成配置文件准备后，使用下面的命令启动Kafka-manager服务： 1f-itdw-4c8g-100g-11:/dmqs/cmak-3.0.0.0/bin # ./cmak -java-home ../jdk -Djava.security.auth.login.config=../conf/kafka_server_jaas.conf -Dapplication.home=/dmqs/cmak-3.0.0.0 &gt; /dev/null 2&gt;&amp;1 &amp; 其中参数命令说明如下： 参数-java-home指定服务启动的java依赖环境目录； 参数-Djava.security.auth.login.config指定和kafka和zookeeper交互的jaas文件路径； 参数-Dapplication.home指定了应用的主目录； 参数-Dhttp.port=8888指定了应用的监听端口，默认9000； 参数-Dconfig.file=../conf/application.conf指定了应用的应用配置文件； 启动后应用目录下面生成logs目录，作为日志存放目录。启动命令不指定端口的情况下，默认监听9000端口。 1.6 自动化脚本为了提高服务运维管理，对服务启停进行自动化管理。 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bash -e RETVAL=0cmak="/dmqs/cmak-3.0.0.0/bin/cmak"start() &#123; $cmak -java-home ../jdk -Djava.security.auth.login.config=../conf/kafka_server_jaas.conf -Dapplication.home=/dmqs/cmak-3.0.0.0 &gt;/dev/null 2&gt;&amp;1 &amp; RETVAL=$? [ $RETVAL -eq 0 ] &amp;&amp; echo "Start Kafka Manager Success!" ||echo "Start Kafka Manager failed!" return $RETVAL&#125; stop() &#123; CMAKPID=$(ps -ef|grep cmak|grep -v grep| awk '&#123;print $2&#125;') if [[ -a /dmqs/cmak-3.0.0.0/RUNNING_PID ]] then rm /dmqs/cmak-3.0.0.0/RUNNING_PID &amp;&amp; echo -e "\n已删除文件:RUNNING_PID\n" &amp;&amp; kill -9 $CMAKPID &gt;/dev/null 2&gt;&amp;1 &amp; RETVAL=$? else kill -9 $CMAKPID &gt;/dev/null 2&gt;&amp;1 &amp; RETVAL=$? fi; [ $? -eq 0 ] &amp;&amp; echo "Stop Kafka Manager Success!" ||echo "Stop Kafka Manager failed!" return $RETVAL&#125;case "$1" in start) start ;; stop) stop ;; restart) sh $0 stop sh $0 start ;; *) echo "Format error!" echo $"Usage: $0 &#123;start|stop|restart&#125;" exit 1 ;;esacexit $RETVAL 对于启动命令，可以自定义修改。 第二部分 kafka-manager配置2.1 创建新集群管理创建新的管理集群，需要填入下面的信息： Cluster Name 集群名称； Cluster Zookeeper Hosts 配置kafka集群背后的zookeeper集群的信息。例如：192.168.1.1:2181； Kafka Version Kafka的版本信息； Enable JMX Polling (Set JMX_PORT env variable before starting kafka server) 是否启用集群的监控组件。 Security Protocol 安全协议。目前支持：SSL、SASL_PLAINTEXT、SASL_SSL、PLAINTEXT SASL Mechanism (only applies to SASL based security) SASL的权限管理协议：DEFAULT、PLAIN、GSSAPI、SCRAM-SHA-256、SCRAM-SHA-512 SASL JAAS Config (only applies to SASL based security) SASL的用户配置信息。例如： 1org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret"; 需要注意的是配置以分号结束，否则会报错。 第三部分 kafka-manager管理Kafka Manager服务启动后，默认监听9000端口，所以服务URL地址为：http://102.168.1.1:9000。目前组件支持的管理功能有： 管理多个集群 轻松检查集群状态（主题，使用者，偏移量，代理，副本分发，分区分发） 运行首选副本选择 生成带有选项的分区分配，以选择要使用的代理 运行分区的重新分配（基于生成的分配） 使用可选的主题配置创建主题（0.8.1.1与0.8.2+具有不同的配置） 删除主题（仅在0.8.2+上受支持，并记住在代理配置中设置delete.topic.enable = true） 现在，主题列表指示标记为删除的主题（仅在0.8.2+上受支持） 批量生成多个主题的分区分配，并可以选择要使用的代理 批量运行分区的多个主题的重新分配 将分区添加到现有主题 更新现有主题的配置 （可选）为代理级别和主题级别的度量启用JMX轮询。 （可选）过滤出在Zookeeper中没有id / owner /＆offsets /目录的使用者。 对于具体的组件使用，可以参文献中的[3]。 参考文献及资料1、kafka-manager项目地址，链接：https://github.com/yahoo/kafka-manager 2、kafka-manager项目下载地址，链接：https://blog.wolfogre.com/posts/kafka-manager-download/ 3、Apache Kafka集群管理工具CMAK(Cluster Manager for Apache Kafka)从安装启动到配置使用，链接：http://www.luyixian.cn/news_show_324464.aspx]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统远程执行shell命令]]></title>
    <url>%2F2020%2F05%2F07%2F2020-05-07-Linux%E7%B3%BB%E7%BB%9F%E8%BF%9C%E7%A8%8B%E6%89%A7%E8%A1%8Cshell%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 远程执行shell命令 参考文献及资料 背景第一部分 远程执行shell命令第二部分 Python实现远程执行shell命令参考文献及资料1、iptables命令详解，链接]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中的shell解释器介绍]]></title>
    <url>%2F2020%2F04%2F21%2F2020-04-21-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84shell%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 shell解释器种类 第二部分 shell执行命令模式 第三部分 shell命令的类型 第四部分 shell初始化和调用 第五部分 启动文件介绍 第六部分 生产运维注意事项 参考文献及资料 背景shell是用C语言编写的程序，用户用它来和Linux进行交互。我们通常说的 shell 都是指 shell 脚本，但 shell 和 shell script 是两个不同的概念，这是后话。 第一部分 shell解释器种类1.1 shell常见解释器类型Linux shell常用类型有：bash、ksh、csh、zsh、ash等。可以通过下面的命令查看本机操作系统支持的shell类型（下面是ubuntu系统）： 123456root@deeplearning:/# cat /etc/shells# /etc/shells: valid login shells/bin/sh/bin/dash/bin/bash/bin/rbash 使用下面的命令查看当前（ubuntu）正在使用的shell： 12root@deeplearning:/# echo $SHELL/bin/bash 这里SHELL是一个环境变量，用于记录当前用户使用的shell类型。在当前shell环境中，可以打开其他的shell环境（子shell）： 12root@deeplearning:/# /bin/dash# 上面命令我们打开了dash，如果要退出，直接使用exit命令即可退出子shell。 1.2 常用shell详细介绍1.2.1 ashash Shell是由Kenneth Almquist编写的，是Linux 中占用系统资源最少的一个小Shell。但是它只包含24个内部命令，因而使用起来很不方便。 1.2.2 bashbash是Linux系统默认使用的Shell，它由Brian Fox 和Chet Ramey共同完成，是BourneAgain Shell的缩写，内部命令一共有40 个。Linux 使用它作为默认的Shell是因为它具有以下特色： 可以使用类似DOS下面的doskey的功能，用上下方向键查阅和快速输入并修改命令。 自动通过查找匹配的方式，给出以某字串开头的命令。 包含了自身的帮助功能，你只要在提示符下面键入help就可以得到相关的帮助信息。 1.2.3 kshksh是Korn Shell的缩写，由Eric Gisin编写，共有42 条内部命令。该Shell最大的优点是几乎和商业发行版的ksh 完全相容，这样就可以在不用花钱购买商业版本的情况下尝试商业版本的性能了。 1.2.4 cshcsh 是Linux 比较大的内核，它由以William Joy 为代表的共计47 位作者编成，共有52个内部命令。该Shell其实是指向/bin/tcsh这样的一个Shell，也就是说，csh其实就是tcsh。 1.2.5 zchzch是Linux 最大的Shell之一，由Paul Falstad完成，共有84 个内部命令。如果只是一般的用途，没有必要安装这样的Shell 第二部分 Shell执行命令的模式Shell按照是否交互，分为交互式和非交互两种模式。这就像Python等解释型语言，有交互式和非交互式。 2.1 交互式(Interactive Shell)交互式（Interactive）：解释执行用户的命令，用户输入一条命令，Shell就解释执行一条。 2.2 非交互式(Non Interactive Shell)非交互式即批处理（batch）。用户提前编写好Shell脚本，文件中存储多条命令。Shell读取并执行文件中命令，直到读到文件的结束EOF，Shell终止。另外对于使用管道连接的多个命令也算批处理。 可以使用打印$-来判别当前交互模式。echo $-，输出himBH为交互式，输出hB表示非交互式。 另外按照是否需要登录（使用用户名/密钥），分为登录式和非登录式。 2.3 登录式(Login Shell)需要用户名、密码登录后才能进入的shell（或者通过--login选项生成的shell）。 2.4 非登录式(Non Login Shell)不需要输入用户名和密码即可打开的Shell。例如在当前shell交互命令行中，执行bash就会开启一个新的shell（子shell）环境，这就是一个非登录式的shell。 2.5 shell的退出执行exit命令，退出一个shell（登录或非登录shell）。执行logout命令，退出登录shell（不能退出非登录shell）。 第三部分 shell命令的类型3.1 内部命令内部命令内置于Shell源码中，即存在于内存中，一般比较简短，代码量很少，执行起来速度快，经常会使用，比如cd、echo。它与shell本身处在同一进程内（就写在Shell这个程序里面）,当打开Shell时，操作系统会将Shell程序放入内存 。 类似Python的程序中的内置函数（Build-in Function），Python解析器初始化化就会加载这些函数。 3.2 外部命令外部命令一般功能比较强大，包含的代码量也较大，所以在系统加载时并不随系统一起被加载到内存中，而是在需要时才调用，它们是存在于文件系统中某个目录下的单独的程序，当执行外部命令时，会到文件系统中文件的目录中寻找，例如 cp、rm、ifconfig。 3.3 查看命令类型对于一个命令是否是内部或者外部命令，可以使用type命令来检测。 1234root@vultr:~# type cd cd is a shell builtinroot@vultr:~# type cpcp is /bin/cp 其中builtin就是指是内部命令，类似Python中builtin包。另外type本身也是一个内部命令： 12root@vultr:~# type typetype is a shell builtin 第四部 shell的初始化和调用当shell被调用时，会读取一些初始化启动文件。主要作用是为shell本身或用户设定运行环境，包含一些函数、 变量、别名等等。 shell有两种类型的初始化文件： 系统级启动文件。这些包含适用于系统上所有用户的全局配置，通常位于/etc目录中。 包括：/etc/profiles和/etc/bashrc或/etc/bash.bashrc(不同操作系统差异) 。 用户级启动文件 。这些存储配置适用于系统上的单个用户，通常位于用户主目录中的点文件（使用la命令查看）。 包括： .profiles ， .bash_profile ， .bashrc和.bash_login。 shell可以以三种模式被调用，分别是：交互式登录、非登录交互式、非交互式。 4.1 交互式登录shell用户成功登录系统后，使用/bin/login登录，随后读取/etc/passwd文件，获取用户凭证后调用shell。/etc/passwd文件中配置了用户默认的shell类型（每行最后）。 123root@VM-0-5-ubuntu:~# cat /etc/passwdroot:x:0:0:root:/root:/bin/bash...... 然后这个登录shell 将查找几个不同的启动文件来处理其中的命令（它的作用是初始化linux系统相关配置）。例如 bash shell 处理文件的顺序如下： 系统登录后，shell首先执行/etc/profile文件中的命令(系统级)。设置这个文件后，可以为系统内所有的用户建立默认的特征（不同版本的Linux此文件放置路径有区别）。 如果是超级用户则提示符用#，如果是普通用户则提示符用$. 当某个用户登录后，shell依次查找~/.bash_profile、~/.bash_login、~/.profile这几个文件。 其中~/.bash_profile、~/.bash_login和 ~/.profile文件往往只存在一个，这与Linux的发行版本有关。ubuntu则为 ~/.profile 如果用户级有与系统级/etc/profile相同的环境变量，将会重新更新系统级的值。 对于Centos系统，加载的是.bash_profile。另外还会加载~/.bashrc，~/.bashrc文件中还会调用文件：/etc/bashrc。 当用户注销时，bash执行文件~/.bash_logout中的命令，这个文件包含了退出会话时执行的清理命令和退出等，如：exit退出。 4.2 交互式非登录Shell这种情况下调用时，它将拷贝父shell的环境，并读取相应用户级的~/.bashrc配置文件。交互式非登录shell 就是指你在当前图形界面中打开“终端”出来的那种窗口程序，和登录shell相比，它是“非登录”的，你并不需要输入用户名和码；和非交互式shell相比，这是“交互式”的，就像你说的那它：你输入什么，它就解释出什么。 4.3 非交互式Shell当执行脚本时，则调用非交互式shell。在这种模式下，它将处理所运行的脚本中的命令、函数等操作，不需要进行交互式输入（除非脚本需要交互式输入）。使用的环境继承自父shell。 第五部分 启动文件介绍5.1 系统级启动文件 /etc/profile，文件保存了登录时系统级环境配置和启动程序。如果你想配置对于所有用户的环境生效，可以加入此文件。 /etc/bashrc（ubuntu为 /etc/bash.bashrc），包含应用于所有用户的系统级函数、变量、别名等配置信息。 5.2 用户级启动文件在/home/用户名该目录下面一般有下面文件（存在操作系统差异）： 1234567.bash_logout # 用户登出shell是加载# 下面三个是用户级启动文件.profile.bash_profile.bash_login.bashrc # Centos操作系统用户级启动文件 第六部分 生产运维注意事项6.1 su命令注意事项在Linux系统使用中，很多用户无法区分su 用户名和su - 用户名两个命令的区别。甚至不懂差异，经常混用，非常危险，特别是生产运维中使用。两个命令的区别我们举个栗子说明一下。假如当前是root用户，su guest执行后，只是切换了用户身份由root切换成guest，但是shell环境仍然继承了root用户的shell环境。但是su - guest命令不仅切换了用户身份，而且shell环境也切换成guest登录后的shell环境。 事实上，下面三个命令形式是等价的： 1234# ubuntu用户切换到guest用户（使用登录方式）ubuntu@VM-0-5-ubuntu:~$ su - guestubuntu@VM-0-5-ubuntu:~$ su -l guestubuntu@VM-0-5-ubuntu:~$ su --login guest 另外这个差异还可以通过命令执行后的用户目录更为形象的感知：su guest执行后，工作目录并没有切换，而su - guest执行后工作目录切换为/home/guest。 对于生产环境，应该统一使用su - 用户名的方式，避免用户shell的继承，造成对切换用户后环境变量的变化（通常应用用户会有特殊的环境变量）。 6.2 crontab中的shell首先需要注意的是：crontab中的shell脚本，既不是交互式shell，也不是登录shell。所以不会加载启动配置文件。所以环境变量需要自行加载，不能想当然脚本在用户shell环境能执行，部署crontab也能执行。 另外下面是crontab的PATH值： 12ubuntu@VM-0-5-ubuntu:~$ grep PATH /etc/crontabPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin 而操作系统用户的PATH值如下，也是有差异的。 12root@VM-0-5-ubuntu:~# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 可以按照下面的方式解决： 可以把shebang改为#!/bin/bash -l让脚本用登录Shell来解释执行，这个时候，执行脚本要采用路径执行的方式 调用Bash解释器，加-l参数，即 /bin/bash -l shell脚本 参考文献及资料1、GNU Bash，链接：https://www.gnu.org/software/bash/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux系统中环境变量详解]]></title>
    <url>%2F2020%2F04%2F21%2F2020-04-21-Linux%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 参考文献及资料 背景第一部分 环境变量分类1.1 生命周期永久环境变量 临时环境变量 1.2 作用域系统级环境变量 用户环境变量 参考文献及资料1、iptables命令详解，链接：https://wangchujiang.com/linux-command/c/iptables.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何禁止云服务器端口被嗅探]]></title>
    <url>%2F2020%2F04%2F11%2F2020-04-14-%E5%A6%82%E4%BD%95%E7%A6%81%E6%AD%A2%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E5%8F%A3%E8%A2%AB%E5%97%85%E6%8E%A2%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 实施 参考文献及资料 背景为了保证云服务器的安全，特别是一些匿名服务器嗅探你的服务端口，然后对你的端口进行封禁。为了保证云机器上服务安全，可以使用linux自带的IPTABLES工具进行防护，相当于操作系统级别的防火墙。 第一部分 实施使用下面的命令只允许222.69.213.94ip对40000端口进行访问，其他ip对该端口的访问全部拒绝。 1iptables -A INPUT -s 218.92.0.202 -p tcp --dport 11111 -j ACCEPT 避免外界对该端口进行嗅探，甄别该端口功能后，进行封禁端口。 参考文献及资料1、iptables命令详解，链接：https://wangchujiang.com/linux-command/c/iptables.html]]></content>
      <categories>
        <category>路由器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[小米路由器MT工具箱配置]]></title>
    <url>%2F2020%2F03%2F22%2F2020-03-22-%E5%B0%8F%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8MT%E5%B7%A5%E5%85%B7%E7%AE%B1%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 装备工作 第二部分 配置MT工具箱 参考文献及资料 背景家庭使用的小米路由器，需要折腾一下，开始全局小飞机。 第一部分 准备工作首先需要小米路由器准备好下面的配置前提： 安装了开发版的rom，获得root权限； 路由器开启了ssh； 路由器默认的IP地址为“192.168.31.1”； 具体可以参考文献中文章《小米路由器安装MT工具箱》。 第二部分 配置MT工具箱使用SSH登录小米路由器，执行下面的安装命令： 1curl -s -k https://beta.misstar.com/download/$(uname -m)/mtinstall -o /tmp/mtinstall &amp;&amp; chmod +x /tmp/mtinstall &amp;&amp; /tmp/mtinstall 选择网络安装（选项2），按照提示配置用户和密钥。提示安装Misstar tools 3.0beta版成功： 1http://192.168.31.1:1314/ 但是提示页面url并不能打开。事实上监听的端口是1024。 12root@XiaoQiang:~# netstat -ant|grep 1024tcp 0 0 :::1024 :::* LISTEN 0 0 使用下面的url进入MT工具箱： 1http://192.168.31.1:1024 然后部署你的小飞机吧。新建节点等不再赘述。 参考文献及资料1、小米路由器安装MT工具箱，链接：https://whrr.blog/2019/01/06/install-mt-tools-on-a-xiaomi-router/]]></content>
      <categories>
        <category>路由器</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch集群升级指引]]></title>
    <url>%2F2020%2F03%2F07%2F2020-03-07-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7%E6%8C%87%E5%BC%95%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 版本升级指引 第二部分 升级方法和具体步骤 总结 参考文献及资料 背景Elasticsearch集群的版本升级是一项重要的集群维护工作。本篇文章参考官方文档，将详细介绍相关细节。 第一部分 版本升级指引1.1 同步升级Elastic Stack组件对于Elasticsearch的生态圈组件需要同步升级，具体配套版本可以参考官方提供的升级指南。 https://www.elastic.co/cn/products/upgrade_guide 1.2 索引兼容性Elasticsearch对于老版本的索引（index）兼容性如下： Elasticsearch 6.x兼容Elasticsearch 5.x中创建的索引，但不兼容Elasticsearch 2.x或更旧版本的索引。 Elasticsearch 5.x兼容Elasticsearch 2.x中创建的索引，但不不兼容Elasticsearch1.x或更旧版本的索引。 如果升级过程中遇到索引不兼容场景，升级后集群将无法正常启动。 1.3 版本升级路线Elasticsearch版本升级具体路线总结如下： 序号 原版本 升级目标版本 支持的升级类型 1 5.x 5.y 滚动升级（其中 y &gt; x） 2 5.6 6.x 滚动升级 3 5.0-5.5 6.x 集群重启 4 &lt;5.x 6.x reindex升级 5 6.x 6.y 滚动升级（其中 y &gt; x） 6 1.x 5.x reindex升级 7 2.x 2.y 滚动升级（其中 y &gt; x） 8 2.x 5.x 集群重启 9 5.0.0 pre GA 5.x 集群重启 10 5.x 5.y 滚动升级（其中 y &gt; x） 关于Elasticsearch的版本序列需要特别说明一下。Elasticsearch版本序列不是连续递增的，从2.4.x版本后直接跳跃到5.0.x。所以对于5.x版本，如果按照严格顺序递增编号，应该是3.x。之所以没有连续编号，主要是为了保持ELK（Elasticsearch 、 Logstash 、 Kibana）整体版本的统一。 其中第4种情况，小于5.x其实就是2.x和1.x。由于6.x对于更低版本的索引不兼容，所以需要对原集群的中索引实施reindex。方案分别为： 1.3.1 2.x升级到6.x按照上面的升级路线有两种升级方案： 方案1：先由2.x升级到5.6版本（reindex升级索引版本），然后由5.6升级到6.x（滚动升级）； 方案2：创建全新的6.x集群，然后将旧集群中的索引数据远程reindex到新集群中； 1.3.2 1.x升级到6.x同样有两个方案： 方案1：先由1.x升级到2.4.x版本（reindex升级索引版本），最后按照上面2.x升级到6.x的方案实施； 方案2：创建全新的6.x集群，然后将旧集群中的索引reindex到新集群中； 第二部分 升级方法和具体步骤集群升级路线中，针对不同的版本之间升级，一共有三种升级方案：滚动升级、集群重启、reindex。下面将分别介绍。 2.1 滚动升级所谓滚动升级指的是集群中节点逐个将版本升级至目标（高）版本，升级期间集群保持对外服务不中断。这种升级方案都是针对同一个大版本内的升级，即x.y升级到x.z（z&gt;y）。特别的，5.6升级到6.x也是支持使用滚动升级方式的。 https://www.elastic.co/guide/en/elasticsearch/reference/current/rolling-upgrades.html 通常滚动升级的步骤如下： 第1步 禁用副本分片（shards）分配在下宕升级节点前，需要提前禁止副本分片的分配。 节点下宕后，副本分配进程会等待index.unassigned.node_left.delayed_timeout（默认情况下为1分钟），然后再开始将该节点上的分片复制到群集中的其他节点，这会导致大量I/O。由于节点很快将重新启动，所以并不需要重新分配。 API命令如下： 123456PUT _cluster/settings&#123; "persistent": &#123; "cluster.routing.allocation.enable": "primaries" &#125;&#125; 第2步 执行同步刷新 重启集群时如果translog过大，日志回放恢复数据耗时较长，建议手动同步刷新，减少translog。 注意：这个过程较为缓慢。 1POST _flush/synced 第3步 停止机器学习作业如果集群中运行了机器学习任务，需要停止任务运行。 参考：https://www.elastic.co/guide/en/elastic-stack-overview/6.8/stopping-ml.html 第4部 下宕待升级节点并安装主版本和插件对升级节点实施下宕，开始文件系统的升级。 第5步 启动节点启动节点，并用下面的API检查节点是否加入集群。 1GET _cat/nodes 第6步 重启分片分配节点加入集群后，设置启用分片分配开始使用该节点。 123456PUT _cluster/settings&#123; "persistent": &#123; "cluster.routing.allocation.enable": null &#125;&#125; 在升级下一个节点前，等待集群分片完成。可以通过下面的API检查集群状态： 1GET _cat/health?v 等待集群的状态由red变成yellow，再到green。说明集群完成所有主分片和副分片的分配。 第7步 重复升级其他节点重复滚动升级集群其他节点。 第8步 重启机器学习任务如果集群中有机器学习任务，需要从新启动。 2.2 集群整体重启集群整体重启指的是升级前将集群所有节点均下宕，集群停止对外服务，待所有节点完成升级后，整体启动集群，恢复对外服务。例如：5.6之前的版本升级到6.x需要重启集群实施升级。 https://www.elastic.co/guide/en/elasticsearch/reference/current/restart-upgrade.html 集群重启升级步骤和滚动方式相似，主要步骤如下： 第1步 禁用副本分片（shards）分配下宕升级节点前需要，提前禁止副本分片的分配。（参考滚动升级） 第2步 停止不必要的索引并执行同步刷新参考滚动升级。 第3步 停止机器学习作业参考滚动升级 第4部 下宕所有节点并安装主版本和插件对集群所有节点实施下宕，开始文件系统版本升级。 第5步 启动节点并等待集群状态为yellow启动所有节点，并用下面的API检查所有节点是否加入集群。 1GET _cat/nodes 第6步 重启分片分配节点加入集群后，设置启用分片分配开始使用该节点。 123456PUT _cluster/settings&#123; "persistent": &#123; "cluster.routing.allocation.enable": null &#125;&#125; 在升级下一个节点前，等待集群分片完成。可以通过下面的API检查集群状态： 1GET _cat/health?v 等待集群的状态由yellow变为green。说明集群完成所有主分片和副分片的分配。 第7步 重启机器学习任务参考滚动升级 2.3 reindexElasticsearch中相邻版本的index具有兼容性，但是跨度较大的版本不再向下兼容。在上文（1.2 索引兼容性）中已做介绍。而在ElasticSearch中，索引的field设置是不能被修改的，如果要修改一个field，那么应该重新按照新的mapping，建立一个index，然后将数据批量查询出来，重新用bulk api写入新index中。 批量查询的时候，建议采用scroll api，并且采用多线程并发的方式来reindex数据，每次scroll就查询指定日期的一段数据，交给一个线程即可。 第1步 搭建新版本集群申请服务器资源，搭建全新版本的ElasticSearch集群。将对外服务全部指向新集群。 第2步 将老集群中数据reindex到新集群在老集群上使用reindex API将老集群中index历史数据逐步迁移至新集群。 如果集群数据量较大，迁移过程是一个很缓慢的过程。 API案例（下面是简单的配置）： 12345678910111213141516171819202122POST _reindex&#123; "source": &#123; "remote": &#123; "host": "http://otherhost:9200", "username": "user", "password": "pass" &#125;, "index": "source", "query": &#123; "match": &#123; "test": "data" &#125; &#125; &#125;, "dest": &#123; "index": "dest" &#125;&#125;//host为远程集群（新集群）的地址。//username和password针对安全集群的密钥验证。//"index": "source"为旧集群中index名，dest的所对应的是新集群目标index名。 迁移完成后，可以对旧集群中数据实施清理。清理完成后根据情况需要，旧节点可以离线升级文件系统，最后作为全新的节点加入新集群。 如果旧集群中历史数据不重要，可以删除数据后，搭建全新的集群。 2.4 分步升级对于跨度较大的版本升级，如果不采用新建集群再实施reindex方式，那么就需要分步升级。例如A、B、C依次为三个版本，版本级别A&lt;B&lt;C，其中index数据B兼容A，C兼容B，但是C不兼容A。这种情况需要分步升级： A升级到B，使用滚动升级或者集群整体重启方式。 对于B版本的集群，将A版本的所有数据reindex到B版本。这个过程较为耗时。 等到集群中所有历史index（新建的index自然是B版本）均为B版本后，升级集群版本到C版本。 如果index数据是时间序列类的数据，可以不实施reindex，等到历史数据生命周期结束后（集群中不在有A版本的index数据），再从B版本升级到C版本。 总结（1）一般Elasticsearch大版本之间跨度升级需要重启整体集群。 （2）部分ElasticSearch大版本间index并不兼容，需要对数据重索引（reindex）。 （3）大版本中的小版本升级，通常只需要滚动重启方式即可。 参考材料1、Elasticsearch官网 链接：https://www.elastic.co/cn/]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PySpark和Kafka交互介绍]]></title>
    <url>%2F2020%2F03%2F02%2F2020-03-02-Pyspark%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Pyspark%E5%92%8CKafka%E4%BA%A4%E4%BA%92%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Receiver接口模式 第二部分 Direct接口模式 第三部分 PySpark和Kafka交互 第四部分 任务提交 参考文献及资料 背景Apache Kafka项目是大数据处理中重要的消息引擎。Spark Streaming作为重要的流处理计算框架通常和Kafka结合使用。Spark Streaming（新旧版本）支持Kafka的编程模型有两种：Receiver模式和Direct模式。Direct模式在Apache Spark 1.3中最新引入，下面是版本演进过程（截止Spark 2.1）。 本文主要介绍Spark Streaming和Kafka进行交互模式和PySpark实现。 0.0 Kafka的Consumer API在正式介绍前，我们首先介绍一下Kafka Consumer API。Kafka在0.9版本前提供了两种版本的Consumer：高级版和简单版。 高级版。消费者客户端不需要去管理偏移量（offset）状态，而是由Zookeeper管理。消费者中断重启后，可以根据上一次记录在Zookeeper中的offset状态信息，继续获取数据（默认为1分钟更新一次Zookeeper中的offset）。通俗的说就是全auto，最适合小白，哈哈。 简单版。消费者客户端需要自行管理offset状态信息（持久化为文件、数据库或者内存等）。 两个客户端的主要差异就是：谁来负责管理offset状态。但是高级的不一定就好，需要具体架构场景具体分析，适合才是最好的，哈哈（扯远了）。 Kafka项目组在0.8.1版本开始重写生产者API，在0.9版本完成新版本API的发版。新版的Consumer API使用原生Java编写（居然不用scala了，啧啧）。当然新版本API也不依赖于Zookeeper。详细细节可以参考官方文章：《Introducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client》。 ##第一部分 Receiver模式 1.1 原始方式最开始Kafka和Spark Streaming集成是通过Receiver。即在每个Spark Executor中实例化Apache Kafka”高级版” Consumer API，启动一个Receiver，实现异步消费Kafka消息，并存储在执行程序的内存中，同时更新在Zookeeper中偏移量（offset）。接着Spark Streaming Driver启动作业（jobs）处理已经接受到的缓存数据。 这种方式是朴素和高效的，但是不具备容错性，遇到故障或者Executor重启任务，均会导致缓存消息的丢失和offset状态的滞后。对于一些重要业务数据处理场景，这是无法容忍的架构缺陷。 1.2 预写日志（WAL）模式为了保证容错性，最简单的架构思路就是”存储状态”。Spark项目组引入了预写日志机制（WAL，Write Ahead Logs）。主要思想是：每个Spark Executor中Receiver接受到的数据持久化到分布式文件系统（HDFS）中，数据只有完成持久化，Receiver才会发起更新Zookeeper中的偏移量状态。当发生故障或者Executor重启时，从持久化的预写日志中恢复数据。 引入WAL后，保证了数据不会丢失。但是有这样的场景：Receiver接收到数据并持久化到WAL，但是系统在更新Zookeeper中相应的偏移量时出现故障，更新失败。当从故障中恢复后，由于offset滞后性，就会出现部分数据重复消费。 之所以出现这种场景，因为架构上无法保证状态信息在两个系统中保持数据一致（exactly-once）。为了避免这种情况，架构上需要只指定一个系统来维护这个状态信息。 Spark项目最后决定由Spark侧来管理offset状态，将offset信息持久化在Spark Streaming中（持久化在checkpoint中（hdfs文件系统）、数据库等，如果是client模式，可以存放在client本地文件系统等），当然与Kafka交互的API也要换成”简单版” Consumer API。这就是下面要介绍的Direct接口方式。 第二部分 Direct模式2.1 Direct模式介绍Apache Spark 1.3中项目组引入了Direct接口方式。Direct方式抛弃了Receiver，采取周期性（Batch Intervel）获取Kafka中每个topic的所有partition中的最新offsets信息，然后根据参数spark.streaming.kafka.maxRatePerPartition设置的速度来消费数据。这就避免了和Zookeeper中偏移量的不一致的问题。而且可以保证即使出现故障，每个记录仅仅被消费一次。 具体消费速度计算如下： 假设Spark window窗口设置为60s(60s拉取一次Kafka数据)；Kafka中该Topic有3个Partitions；maxRatePerPartition设置为100。那么每次拉取的最大数据量为： 60* 3 * 100 条数据。 2.2 两种方式的比较Direct模式相比Receiver模式的优势有： Receiver模式需要在内存中和预写日志中保存两份数据，如果数据量较大，特别是当任务的作业出现大量延迟（delay）的时候，会占用大量存储资源。而Direct模式只有在计算的时候才会去拉取Kafka侧数据，Kafka侧仍然要充当数据的缓冲角色（削峰填谷）。 Direct模式中，Kafka中的partition与RDD中的partition是一一对应的（即一个KafkaRDDIterator对应一个 KafkaRDDPartition）并行读取Kafka数据，即天然利用了并发处理优势。而Receiver模式需要创建多个Receiver之后，可以利用union方法合并成一个Dstream的方式提高数据传输的并行度（后面程序实现将详细介绍）。 Direct模式保证了流计算Spark Streaming和Kafka管道数据 at least once语义。 第三部分 PySpark和Kafka交互3.1 PySpark中Receiver接口3.1.1 接口说明pyspark.streaming.kafka文件中的KafkaUtils类： 123def createStream(ssc, zkQuorum, groupId, topics, kafkaParams=None, storageLevel=StorageLevel.MEMORY_AND_DISK_2, keyDecoder=utf8_decoder, valueDecoder=utf8_decoder) 参数说明： ssc：StreamingContext对象； zkQuorum：Zookeeper集群地址，格式为：hostname:port,hostname:port,..，逗号隔开； groupId：消费者群组名； topics：主题名，字典类型。例如：{“test”：1}，其中1表示线程数量（core）。 kafkaParams：可以补充的Kafka其他参数，字典数据类型；如果非空，其他Kafka的参数设置失效。 storageLevel：RDD的存储级别，StorageLevel参数决定如何存储RDD。在Spark中，StorageLevel决定RDD是应该存储在内存中还是存储在磁盘上，或两者都存储。它还决定是否序列化RDD以及是否复制RDD分区。参数类型有： 1234567891011DISK_ONLY = StorageLevel（True，False，False，False，1）DISK_ONLY_2 = StorageLevel（True，False，False，False，2）MEMORY_AND_DISK = StorageLevel（True，True，False，False，1）MEMORY_AND_DISK_2 = StorageLevel（True，True，False，False，2）MEMORY_AND_DISK_SER = StorageLevel（True，True，False，False，1）MEMORY_AND_DISK_SER_2 = StorageLevel（True，True，False，False，2）MEMORY_ONLY = StorageLevel（False，True，False，False，1）MEMORY_ONLY_2 = StorageLevel（False，True，False，False，2）MEMORY_ONLY_SER = StorageLevel（False，True，False，False，1）MEMORY_ONLY_SER_2 = StorageLevel（False，True，False，False，2）OFF_HEAP = StorageLevel（True，True，True，False，1） 从命名规范也很容易理解，MEMORY是内存，DISK是磁盘，SER表示是否序列化，数字是副本数量。那么默认参数：MEMORY_AND_DISK_2，含义是：数据同时存储内存和磁盘，并且副本数量为2（存储在不同节点）。即开启了WAL机制。 3.1.2 案例介绍下面是计算单词数的流处理任务代码案例： 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-from pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtilszkQuorum = "localhost:2181"topics = "test"groupid = "test"if __name__ == "__main__": SparkConf = SparkConf() SparkConf.set("spark.streaming.kafka.maxRatePerPartition",5000) # 开启WAL机制 #sparkConf.set("spark.streaming.receiver.writeAheadLog.enable", "true") sc = SparkContext(appName="PythonStreamingKafkaWordCount", conf=SparkConf) ssc = StreamingContext(sc, 20) # 偏移量模式：largest、smallest、none，默认largest kafkaParams = &#123;"auto.offset.reset": "smallest"&#125; # 提高并行度 numStreams = 3 kafkaStreams = [KafkaUtils.createStream(ssc, zkQuorum, groupid, &#123;topics：1&#125;, kafkaParams) for _ in range (numStreams)] unifiedStream = ssc.union(*kafkaStreams) # 统计单词数 lines = unifiedStream.map(lambda x: x[1]) counts = lines.flatMap(lambda line: line.split(" ")) \ .map(lambda word: (word, 1)) \ .reduceByKey(lambda a, b: a+b) # 打印 counts.pprint() ssc.start() ssc.awaitTermination() 关于auto.offset.reset参数补充说明一下。原生的API中这个参数有三个值： earliest：自动将偏移重置为最早的偏移量； latest（默认值）：自动将偏移量重置为最新偏移量； none：如果consumer group中没有发现先前的偏移量，则抛出异常； 与Spark Streaming整合后，有两个参数： smallest：从头开始消费，等价于上面的 earliest； largest（默认值）：从最新的开始消费，等价于上面的 latest； 在spark-streaming-kafka-0-10新客户端中，这个参数也有none值（offest保存在kafka的一个特殊的topic名为:__consumer_offsets里面）。程序具体判断逻辑是：如果存在已经提交的offest时,不管设置为earliest 或者latest 都会从已经提交的offest处开始消费。如果不存在已经提交的offest时,使用参数auto.offset.reset的值。当值为none时，topic各分区都存在已提交的offset时，从提交的offest处开始消费；只要有一个分区不存在已提交的offset，则抛出异常。 程序中我们将并发度设置为3，即实例化了3个DStreams，最后union合并。这个处理效果可以在作业的UI界面中看到差异： 补充UI界面。 https://www.cnblogs.com/juncaoit/p/9452333.html 3.2 PySpark中Direct接口Python API在Spark 1.4中引入了此功能。 3.2.1 接口说明pyspark.streaming.kafka文件中的KafkaUtils类： 123def createDirectStream(ssc, topics, kafkaParams, fromOffsets=None, keyDecoder=utf8_decoder, valueDecoder=utf8_decoder, messageHandler=None): 参数说明： ssc：StreamingContext对象； topics：主题名，List数据类型（支持多个topic同时消费）； kafkaParams：Kafka参数，字典格式； fromOffsets：offset状态信息（Per-topic/partition Kafka offsets defining the (inclusive) starting point of the stream.）。如果没有指定，则使用参数：{&quot;auto.offset.reset&quot;: &quot;largest&quot;}。 需要特别注意的是：createDirectStream在spark-streaming-kafka-0-8下不支持group id模式，因为它是使用“简单版”Kafka API。后续spark-streaming-kafka-0-10开始提供对group id模式的支持。在0.9.0.0中，引入了新的Java 客户端API，以替代旧的基于Scala的简单和高级API。Spark Streaming integration for Kafka 0.10是使用新的API，支持group id参数。 3.2.2 案例介绍下面的是接口的使用案例。 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-from pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtilsbrokers = "localhost:9092"topic = "test"offsetRanges = []def storeOffsetRanges(rdd): global offsetRanges offsetRanges = rdd.offsetRanges() return rdddef printOffsetRanges(rdd): for o in offsetRanges: print(o.topic,o.partition,o.fromOffset,o.untilOffset,o.untilOffset-o.fromOffset)if __name__ == "__main__": conf = SparkConf().set("spark.streaming.kafka.maxRatePerPartition", 5000) sc = SparkContext(appName="PythonStreamingDirectKafkaWordCount",conf=conf) ssc = StreamingContext(sc, 60) kafkaStreams = KafkaUtils.createDirectStream(ssc, [topic],&#123;"metadata.broker.list": brokers&#125;) lines = kafkaStreams.map(lambda x: x[1]) counts = lines.flatMap(lambda line: line.split(' ')) \ .map(lambda word: (word, 1)) \ .reduceByKey(lambda a, b: a+b) kafkaStreams.transform(storeOffsetRanges).foreachRDD(printOffsetRanges) counts.pprint() ssc.start() ssc.awaitTermination() 我们启动一个Kafka 生产者每秒发送一条信息hello world,截取回显： 1234567891011121314151617test 1 4599722 4599724 2 test 0 4596564 4596567 3 test 2 4605282 4605282 0 ------------------------------------------- Time: 2020-06-19 16:22:00 ------------------------------------------- ('world', 5) ('hello', 5) test 1 4599724 4599748 24 test 0 4596567 4596586 19 test 2 4605282 4605299 17 ------------------------------------------- Time: 2020-06-19 16:23:00 ------------------------------------------- ('world', 60) ('hello', 60) 回显中： 12test 1 4599724 4599748 24 分别对应下面的字段：o.topic(主题名) o.partition（分区名） o.fromOffset（起始位移）,o.untilOffset（终止位移）,o.untilOffset-o.fromOffset（终止位移和起始位移的差） 上面的程序我们只是打印了offset的状态信息（参考官方案例），但是并没有对offset状态进行持久化处理。如果任务故障或终止，重新启动时候，任务会重新开始消费。在实际生产环境中，一些高可用数据处理场景，这是不可容忍的。offset状态数据需要进行高可用持久化处理。这就是接下来我们介绍的checkpoint机制。 3.3 checkpoint机制3.3.1 持久化（persist）和Checkpoint机制Spark中对于RDD的容错性是通过storageLevel参数设置RDD存储级别（持久化级别）。persist()的默认参数为MEMORY_ONLY（即storageLevel=MEMORY_ONLY），即内存缓存，称为缓存（cache）。在这种存储级别下，Spark计算出的RDD结果将缓存在内存中，一旦计算任务中一个执行器（Executor）故障下宕，缓存在该执行器的RDD数据就会丢失，执行器重建后需要通过RDD依赖链重新计算。 例如Receiver模式，Pyspark中接口默认使用MEMORY_AND_DISK_2。在该存储级别下，RDD除了缓存在内存还会持久化到磁盘（2份），当执行器失败可以从磁盘中加载持久化数据。但是一旦Spark的Driver故障下宕或者任务正常结束，计算的所有存储资源将被集群回收。 而Checkpoint机制将RDD持久化到HDFS文件系统，天然的利用了HDFS的分布式高可用文件系统特性。 3.3.2 Checkpoint机制的实现前面的Receiver模式的例子中，我们开启了WAL机制。但是这种机制是执行器级别的高可用。这里我们提高高可用级别，增加checkpoint机制将RDD持久化到HDFS分布式文件系统中。这样即使Spark流任务的Driver重启依然能从checkpoint重启启动，继续消费数据。 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import sysfrom pyspark import SparkContext, SparkConffrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtils# 定义checkpointcheckpointDirectory = "hdfs://user/python/kafka/checkpoint"zkQuorum = "localhost:2181"topics = "test"groupid = "test"def functionToCreateContext(): sparkConf = SparkConf() sparkConf.set("spark.streaming.receiver.writeAheadLog.enable", "true") sc = SparkContext(appName="PythonStreamingKafkaWordCount",conf=sparkConf) ssc = StreamingContext(sc, 60) global zkQuorum global topics kafkaStreams = KafkaUtils.createStream(ssc, zkQuorum, "spark-streaming-consumer", &#123;topic: 1&#125;) lines = kafkaStreams.map(lambda x: x[1]) counts = lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b) counts.pprint() ssc.checkpoint(checkpointDirectory) # set checkpoint directory return sscif __name__ == "__main__": zkQuorum = "localhost:2181" topics = "test" groupid = "test" #如果 checkpointDirectory 目录存在，则context对象会从检查点数据重新构建出来。如果该目录不存在（如：首 #次运行），则 functionToCreateContext 函数会被调用，创建一个新的StreamingContext对象并定义好 #DStream数据流。 ssc = StreamingContext.getOrCreate(checkpointDirectory, lambda: functionToCreateContext()) ssc.start() ssc.awaitTermination() 但启动一个具有checkpoint机制的spark任务的时候。通过函数getOrCreate实现： StreamingContext.getOrCreate(checkpointDirectory, lambda: functionToCreateContext()) 函数参数的具体含义就是：首先检查是否有checkpoint（即checkpointDirectory）。如果非空，StreamingContext从checkpointDirectory加载启动。如果没有执行函数functionToCreateContext()创建（函数中已经声明了创建逻辑）。具体数据流参考下图： 3.4 其他持久化实现方式除了checkpoint机制，例如下面代码将offset信息持久化到本地文件系统。需要注意的是该持久方式需要任务为Client模式提交集群，否则保存在Drive中本地文件系统会被集群回收。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#!/usr/bin/python# coding=utf-8from pyspark import SparkContextfrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtils, TopicAndPartitionimport timeimport osimport jsonbroker_list = "xxxx"topic_name = "xxxx"timer = 60offsetRanges = []def store_offset_ranges(rdd): global offsetRanges offsetRanges = rdd.offsetRanges() return rdddef save_offset_ranges(rdd): root_path = os.path.dirname(os.path.realpath(__file__)) record_path = os.path.join(root_path, "offset.txt") data = dict() f = open(record_path, "w") for o in offsetRanges: data = &#123;"topic": o.topic, "partition": o.partition, "fromOffset": o.fromOffset, "untilOffset": o.untilOffset&#125; f.write(json.dumps(data)) f.close()def deal_data(rdd): data = rdd.collect() for d in data: # do something passdef save_by_spark_streaming(): # 定义本地文件系统 root_path = os.path.dirname(os.path.realpath(__file__)) record_path = os.path.join(root_path, "offset.txt") # from_offsets = &#123;&#125; # 获取已有的offset，没有记录文件时则用默认值即最大值 if os.path.exists(record_path): f = open(record_path, "r") offset_data = json.loads(f.read()) f.close() if offset_data["topic"] != topic_name: raise Exception("the topic name in offset.txt is incorrect") topic_partion = TopicAndPartition(offset_data["topic"], offset_data["partition"]) from_offsets = &#123;topic_partion: long(offset_data["untilOffset"])&#125; # 注意设置起始offset时的方法 print("start from offsets: %s" % from_offsets) sc = SparkContext(appName="Realtime-Analytics-Engine") ssc = StreamingContext(sc, int(timer)) kafkaStreams = KafkaUtils.createDirectStream(ssc=ssc, topics=[topic_name], fromOffsets=from_offsets,kafkaParams=&#123;"metadata.broker.list": broker_list&#125;) kafkaStreams.foreachRDD(lambda rec: deal_data(rec)) kafkaStreams.transform(store_offset_ranges).foreachRDD(save_offset_ranges) ssc.start() ssc.awaitTermination() #ssc.stop()if __name__ == '__main__': save_by_spark_streaming() 上面的例子中，将offset信息以json格式持久化到文件系统中。实际生产中建议以二维表形式存储在Mysql等数据库中进行持久化（不再详细举例）。 java 保存在mysql的例子 https://www.jianshu.com/p/2369a020e604 https://blog.csdn.net/lxb1022/article/details/78041168 其他交互接口： https://www.cnblogs.com/yanshw/p/11929180.html 第四部分 任务的提交4.1 兼容性Spark 针对 Kafka 的不同版本（主要还是以0.8版本为重要分界线），提供了两种方案：spark-streaming-kafka-0-8 和 spark-streaming-kafka-0-10，其主要区别如下： spark-streaming-kafka-0-8 spark-streaming-kafka-0-10 Broker Version（Kafka版本） 0.8.2.1 or higher 0.10.0 or higher Api Stability Stable Experimental Language Support Scala, Java, Python Scala, Java Receiver DStream Yes No Direct DStream Yes Yes SSL / TLS Support No Yes Offset Commit Api No Yes Dynamic Topic Subscription No Yes 目前只有spark-streaming-kafka-0-8方案是支持python语言的，所以我们提交任务是需要指定相应的依赖jar包。高版本（spark-streaming-kafka-0-10）的方案已经抛弃了对Receiver方式的支持。 spark-streaming-kafka-0-8接口的jar下载路径： spark-streaming-kafka_2.10-1.5.1.jar 和 spark-streaming-kafka-assembly_2.10-1.5.1.jar 如果出现包冲突，提交任务时添加参数--conf spark.yarn.user.classpath.first=true,这样设置后yarn中优先使用用户传上去的jar包,避免包冲突。 4.2 任务提交案例与任何Spark应用程序一样，spark-submit用于启动应用程序。但是，Scala / Java应用程序和Python应用程序的细节略有不同。 对于Scala和Java应用程序，如果您使用SBT或Maven进行项目管理，则将spark-streaming-kafka_2.11其及其依赖项打包到应用程序JAR中。确保spark-core_2.10并spark-streaming_2.10标记为providedSpark安装中已存在的依赖项。然后使用spark-submit启动应用程序。 对于缺少SBT / Maven项目管理的Python应用程序，spark-streaming-kafka_2.11可以直接将其依赖项添加到spark-submit使用中--packages。那是， 1./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.11:1.6.3 ... 另外，您也可以下载Maven构件的JAR spark-streaming-kafka-assembly从 Maven仓库，并将其添加到spark-submit用--jars。 4.3 参数调优建议Spark streaming+Kafka的使用中，当数据量较小，很多时候默认配置和使用便能够满足情况，但是当数据量大的时候，就需要进行一定的调整和优化，而这种调整和优化本身也是不同的场景需要不同的配置。 合理的批处理时间（batchDuration）：几乎所有的Spark Streaming调优文档都会提及批处理时间的调整，在StreamingContext初始化的时候，有一个参数便是批处理时间的设定。如果这个值设置的过短，即个batchDuration所产生的Job并不能在这期间完成处理，那么就会造成数据不断堆积，最终导致Spark Streaming发生阻塞。而且，一般对于batchDuration的设置不会小于500ms，因为过小会导致SparkStreaming频繁的提交作业，对整个streaming造成额外的负担。在平时的应用中，根据不同的应用场景和硬件配置，我设在1~10s之间，我们可以根据SparkStreaming的可视化监控界面，观察Total Delay来进行batchDuration的调整。 合理的Kafka拉取量（maxRatePerPartition重要）：对于Spark Streaming消费kafka中数据的应用场景，这个配置是非常关键的，配置参数为：spark.streaming.kafka.maxRatePerPartition。这个参数默认是没有上线的，即kafka当中有多少数据它就会直接全部拉出。而根据生产者写入Kafka的速率以及消费者本身处理数据的速度，同时这个参数需要结合上面的batchDuration，使得每个partition拉取在每个batchDuration期间拉取的数据能够顺利的处理完毕，做到尽可能高的吞吐量，而这个参数的调整可以参考可视化监控界面中的Input Rate和Processing Time 缓存反复使用的Dstream（RDD）：Spark中的RDD和SparkStreaming中的Dstream，如果被反复的使用，最好利用cache，将该数据流缓存起来，防止过度的调度资源造成的网络开销。可以参考观察Scheduling Delay参数 设置合理的GC：长期使用Java的小伙伴都知道，JVM中的垃圾回收机制，可以让我们不过多的关注与内存的分配回收，更加专注于业务逻辑，JVM都会为我们搞定。对JVM有些了解的小伙伴应该知道，在Java虚拟机中，将内存分为了初生代（edengeneration）、年轻代young generation）、老年代（oldgeneration）以及永久代（permanentgeneration），其中每次GC都是需要耗费一定时间的，尤其是老年代的GC回收，需要对内存碎片进行整理，通常采用标记-清楚的做法。同样的在Spark程序中，JVMGC的频率和时间也是影响整个Spark效率的关键因素。在通常的使用中建议：–conf “spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC” 设置合理的CPU资源数：CPU的core数量，每个executor可以占用一个或多个core，可以通过观察CPU的使用率变化来了解计算资源的使用情况，例如，很常见的一种浪费是一个executor占用了多个core，但是总的CPU使用率却不高（因为一个executor并不总能充分利用多核的能力），这个时候可以考虑让么个executor占用更少的core，同时worker下面增加更多的executor，或者一台host上面增加更多的worker来增加并行执行的executor的数量，从而增加CPU利用率。但是增加executor的时候需要考虑好内存消耗，因为一台机器的内存分配给越多的executor，每个executor的内存就越小，以致出现过多的数据spill over甚至out of memory的情况 设置合理的parallelism：partition和parallelism，partition指的就是数据分片的数量，每一次task只能处理一个partition的数据，这个值太小了会导致每片数据量太大，导致内存压力，或者诸多executor的计算能力无法利用充分；但是如果太大了则会导致分片太多，执行效率降低。在执行action类型操作的时候（比如各种reduce操作），partition的数量会选择parent RDD中最大的那一个。而parallelism则指的是在RDD进行reduce类操作的时候，默认返回数据的paritition数量（而在进行map类操作的时候，partition数量通常取自parent RDD中较大的一个，而且也不会涉及shuffle，因此这个parallelism的参数没有影响）。所以说，这两个概念密切相关，都是涉及到数据分片的，作用方式其实是统一的。通过spark.default.parallelism可以设置默认的分片数量，而很多RDD的操作都可以指定一个partition参数来显式控制具体的分片数量。 在SparkStreaming+kafka的使用中，我们采用了Direct连接方式，前文阐述过Spark中的partition和Kafka中的Partition是一一对应的，我们一般默认设置为Kafka中Partition的数量。 使用高性能的算子：（1）使用reduceByKey/aggregateByKey替代groupByKe(2)使用mapPartitions替代普通map(3) 使用foreachPartitions替代foreach(4) 使用filter之后进行coalesce操作5 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 使用Kryo优化序列化性能主要有三个地方涉及到了序列化在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）： // 创建SparkConf对象。 val conf = new SparkConf.setMaster(…).setAppName(…) //设置序列化器为KryoSerializer。conf.set(“spark.serializer”,”org.apache.spark.serializer.KryoSerializer”) //注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) https://www.cnblogs.com/frankdeng/p/9308585.html 参考文献及资料1、Improvements to Kafka integration of Spark Streaming，链接：https://databricks.com/blog/2015/03/30/improvements-to-kafka-integration-of-spark-streaming.html 2、Source code for pyspark.streaming.kafka，链接：https://spark.apache.org/docs/2.2.0/api/python/_modules/pyspark/streaming/kafka.html 3、Introducing Spark Streaming，链接：https://engineering.billymob.com/introducing-spark-streaming-c1b8be36c775 4、Spark Streaming基于kafka的Direct详解，链接：https://blog.csdn.net/matrix_google/article/details/80033524 5、Enabling fault-tolerant processing in Spark Streaming，链接:https://docs.cloudera.com/runtime/7.0.2/developing-spark-applications/topics/spark-streaming-fault-tolerance.html 6、Introducing the Kafka Consumer: Getting Started with the New Apache Kafka 0.9 Consumer Client，链接：https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/ 7、官网文档，链接：https://spark.apache.org/docs/1.6.3/streaming-kafka-integration.html 8、官网文档，链接：https://spark.apache.org/docs/2.3.1/streaming-kafka-0-10-integration.html]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka系列文章（第五篇 Kafka安全集群）]]></title>
    <url>%2F2020%2F03%2F02%2F2020-01-01-Kafka%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%BA%94%E7%AF%87Kafka%E5%AE%89%E5%85%A8%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Kafka集群加密传输 第二部分 Kafka集群权限认证 第三部分 加密认证集群的客户端 第四部分 加密认证集群的性能压测 第五部分 总结 参考文献及资料 背景Kafka在0.9.0.0版本前没有安全机制功能。Kafka Client程序可以直接获取到Kafka集群元数据信息和Kafka Broker地址后，连接到Kafka集群，然后完全操作集群上的所有topic数据资源。另外集群节点间通讯、broker和zookeeper通讯、客户端和集群的网络层通信都是无加密模式。集群的数据存在极大的安全风险。 自0.9.0.0版本开始，Kafka社区逐步添加了较多功能用于提高Kafka群集的安全性。目前Kafka安全集群安全机制主要有三个方面的设置：通信加密（encryption）、身份认证（authentication）和授权（authorization）。 本文重点介绍生产安全集群的一种配置方案。数据通讯传输配置SSL，认证配置SASL，授权通过ACL接口命令来完成的，即：SSL+SASL/SCRAM+ACL。 第一部分 Kafka集群加密传输1.1 背景知识介绍涉及的技术知识不做详细介绍。 1.1.1 密码学基础加密算法分为两类： 对称密钥算法（Symmetric Cryptography）：数据加密和解密时使用相同的密钥。例如常用的DES就是对称加密算法。 非对称密钥算法（Asymmetric Cryptography）：数据加密和解密时使用不同的密钥，分为：公开的公钥（public key）和用户保存的私钥（private key），私钥和公钥在数学上是相关的。利用公钥（或私钥）加密的数据只能用相应的私钥（或公钥）才能解密。举一个例子：客户在银行网银上做一笔交易，首先向银行申请公钥，银行分发公钥给用户，用户使用公钥对请求数据进行加密。银行收到加密数据后通过银行侧保存的私钥进行解密处理，并处理后更新后台数据库。这个通讯过程中银行不需要通过互联网分发私钥。因此保证了私钥的安全。目前最常用的非对称加密算法是RSA算法。 非对称密钥算法中，私钥来解密公钥加密的数据，公钥来解密私钥加密的数据。 两种加密算法的比较： 对称密钥的强度和密钥长度成正比，但是解密效率和密钥长度成反比。另外私钥的分发存在安全风险。 非对称加密保证了私钥的安全性，但是加密和解密的效率比对称加密低。 所以通常加密场景是两种密钥结合使用。使用数据主体使用对称秘钥算法，但是私钥的传输使用非对称算法在互联网环境分发非对称密钥。最常见的就是SSL/TLS。 1.1.2 CA数字证书对于非对称密钥算法存在一个安全风险点，那就是公钥的分发存在中间人攻击。还是以客户和银行的通信为例（例子简单化处理）。客户和银行分别有自己的公钥和私钥，私钥各自保留本地。公钥通过互联网分发给对方。那么公钥就是有安全风险的。存在被黑客截取风险。客户向银行申请银行公钥，结果被黑客截取，黑客伪装成银行，返回给用户自己的黑客公钥，用户收到黑客公钥后，将信息加密发给黑客。黑客用黑客私钥进行解密，获取到真实信息。这时候黑客伪装成客户用相同的方法完成和银行的数据交互。这就是中间人攻击的案例。 所以非对称加密算法的公钥传输同样存在风险。当然如果使用原始的离线方式交换密钥是安全的，但是随着互联网通信的爆炸式增长，这是落后低效的。为了保证公钥的真实性和安全性，这时候我们引入第三个角色：公开密钥认证（Public key certificate，简称CA），又称数字证书（digital certificate）或身份证书（identity certificate）。 通常CA是一家第三方权威机构。负责管理和签发证书。整个实现原理也是非对称加密算法： 机构将自己的公钥以及身份信息交给CA机构（安全的），CA使用自己的私钥对各机构的公钥进行加密。这个过程称为验签。输出的加密后的公钥及身份信息称为数字证书。 当其他机构请求A机构公钥的时候，返回的是A机构的数字证书。其他机构可以使用CA的公钥对该数字证书中加密公钥进行解密获取A机构的通信公钥。 那么新得安全问题又来了，如何保证CA机构的公钥不被伪造？通常CA的公钥是集成在浏览器或者操作系统中，并且被很好的保护起来。 当然CA证书还涉及更多的安全细节设计（Hash算法防篡改、信任链等大量细节），这里只是简单的介绍。详细介绍可以查看：维基（证书颁发机构） 对于企业内部的应用系统就没必要花钱购买CA机构的证书服务了，可以自建 Root CA，自己给自己颁发证书，充当内网的CA机构。当然这时候客户端就需要导入CA的证书了（浏览器和操作系统没有自建的CA证书）。 1.1.3 SSL/TLS加密协议SSL（Secure Sockets Layer）是一种安全协议，目的是为保障互联网上数据传输安全，利用数据加密技术，确保数据在网络上之传输过程中不会被截取。 从网络协议层看，SSL协议位于TCP/IP协议与应用层协议之间，为数据通讯提供安全支持。SSL协议自身可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。例如HTTPS就是在HTTP应用层上增加了SSL加密协议支持（HTTP over SSL）。 TLS(Transport Layer Security，传输层安全协议)，同样用于两个应用程序之间提供保密性和数据完整性。 TLS 1.0建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，即是SSL的升级版。TLS的主要目标是使SSL更安全，并使协议的规范更精确和完善。另外,TLS版本号也与SSL的不同(TLS的版本1.0使用的版本号为SSLv3.1) SSL通过握手过程在client和server之间协商会话參数，并建立会话。一共有三种方式： 仅仅验证server的SSL握手过程（单向SSL） 验证server和client的SSL握手过程（双向SSL） 恢复原有会话的SSL握手过程 第一种：单向SSL通信过程如下（SSL 客户端和SSL 服务端通信）： (1)SSL客户端向SSL服务端发起请求，请求信息包括SSL版本号、加密算法、密钥交换算法、MAC算法等信息； (2)SSL服务端确定本次通话的SSL版本和加密套件后，将携带公钥信息的证书回给客户端。如果通话可从重用，还会返回会话ID； (3)SSL服务端发送Server Hello Done消息。通知SSL客户端版本号和加密套件协商结束，开始进行密钥交换； (4)SSL客户端对CA证书进行验证，证书合法则继续、不成功弹出选择页面； (5)SSL客户端生产随机私有对称密钥key，并使用服务端公开密钥进行加密后，发给服务端； (6)SSL服务端使用自己的私钥解密，获取对称密钥key； (7)最后SSL客户端与SSL服务端将使用该对称密钥key进行加密通信。 第二种：单向认证，仅仅是客户端需要检验服务端证书是否是正确的。双向SSL和单向认证几乎一样，只是在客户端认证完服务器证书后，客户端会将自己的证书传给服务器。服务器验证通过后，才开始秘钥协商。 第三种：协商会话参数、建立会话的过程中，需要使用非对称密钥算法来加密密钥、验证通信对端的身份，计算量较大，占用了大量的系统资源。为了简化SSL握手过程，SSL允许重用已经协商过的会话。即可以重用会话ID。这就是第三种建立会话方式。 1.1.4 Openssl工具对于企业内部（内部局域网）的应用系统通讯，如果需要CA证书服务，可以使用Openssl自建CA，并完成证书签发。 先说一下常用密钥类文件的规范： 后缀名规范 通常约定后缀含义：crt或者cert 表示证书, key表示私钥, req和csr表示请求文件。 文件格式 pem表示pem格式（经过加密的文本文件），der表示der格式（经过加密的二进制文件）。所有证书和私钥可以是pem,也可以是der格式，取决于需要。两个格式可以转换。 Openssl的配置文件（openssl.cnf）定义CA的默认参数，例如ubuntu系统中配置文件位置在/usr/lib/ssl/openssl.cnf。如果不适用默认参数需要在命令中重新指定。 CA证书的制作 首先生成CA的私钥，使用下面的命令： 1$ openssl genrsa -out private/ca.key.pem 2048 private/ca.key.pem是CA私钥,格式为pem，长度（加密位数）为2048。 前面密码学知识知道CA使用一对密钥的（私钥和公钥），并且两个密钥是数学相关的。公钥可以通过私钥算出来。 CA证书自签发 参考命令如下： 1$ openssl req -new -x509 -key private/ca.key.pem -out certs/ca.cert.pem certs/ca.cert.pem 即CA的自签证书。部署导入到客户端（例如浏览器）。 用户证书签发 用户证书的签发和CA自签相同，用户证书由CA私钥签发。用户需要提供请求文件。 1$ openssl ca -in app.csr -out app.crt -days 365 app.crt为签发的证书。部署在应用服务器上。 1.1.5 Keytool工具介绍在密钥证书管理时，通常使用JAVA的Keytool工具程序。Keytool 是一个JAVA数据证书的管理工具 ,Keytool 将密钥（key）和证书（certificates）存在一个称为keystore的文件中，通常称为密钥库文件。文件的扩展名通常使用：jks，全名java key store file。 Keytool是一个Java数据证书的管理工具，所以节点需要配置JAVA_HOME环境变量。 这里列举了命令支持的参数含义及注意点（供后续使用查阅）： keystore 参数指定保存证书的文件（密钥库二进制文件）。密钥库文件包含证书的私钥，必须对其进行安全保存。 validity 参数指定密钥有效期，单位是天。默认为90天。 keyalg 参数指定密钥使用的加密算法（例如RSA，如果不指定默认采用DSA）。 keysize 参数指定密钥的长度。该参数是选项参数，默认长度是1024位。为了保证密钥安全强度，建议密码长度设置为2048位。 keypass 参数指定生成密钥的密码（私钥密码）。 storepass 指定密钥库的密码(获取keystore信息所需的密码)。另外密钥库创建后，要对其做任何修改都必须提供该密码，以便访问密钥库。 alias 参数指定密钥别名。每个密钥文件有一个唯一的别名，别名不区分大小写。 dname 参数指定证书拥有者信息。例如： “CN=名字与姓氏,OU=组织单位名称,O=组织名称,L=城市或区域名称,ST=州或省份名称,C=单位的两字母国家代码”。 list 参数显示密钥库中的证书信息。keytool -list -v -keystore 指定keystore -storepass 密码 v 参数显示密钥库中的证书详细信息。 export 将别名指定的证书导出到文件。keytool -export -alias 需要导出的别名 -keystore 指定keystore -file 指定导出的证书位置及证书名称 -storepass 密码。 file 参数指定导出到文件的文件名。 delete 删除密钥库中某条目。keytool -delete -alias 指定需删除的别名 -keystore 指定keystore -storepass 密码 printcert 查看导出的证书信息。keytool -printcert -file yushan.crt keypasswd 修改密钥库中指定条目口令。keytool -keypasswd -alias 需修改的别名 -keypass 旧密码 -new 新密码 -storepass keystore密码 -keystore sage storepasswd 修改keystore口令。keytool -storepasswd -keystore e:/yushan.keystore(需修改口令的keystore) -storepass 123456(原始密码) -new newpasswd(新密码) import 将已签名数字证书导入密钥库。keytool -import -alias 指定导入条目的别名 -keystore 指定keystore -file 需导入的证书 关于Keytool工具的详细介绍，可以参考oracle的官网。 1.2 Kafka集群配置SSL加密Apache Kafka允许客户端通过SSL连接。默认情况下，SSL是禁用的，可以根据需要打开。 1.2.1 集群环境准备为了后文讲解方便，我们部署了Kafka集群（3节点）和Zookeeper集群（3节点）测试环境。其中zookeeper和kafka混合部署。 节点编号 hostname IP地址 1 kafka.app.node1 192.168.1.5 2 kafka.app.node2 192.168.1.6 3 kafka.app.node3 192.168.1.7 Kafka集群节点对外服务端口为：9092；Zookeeper集群节点对外服务端口为：2181。 1.2.2 配置主机名验证从Kafka 2.0.0版开始，默认会为客户端连接以及broker之间的连接启用服务器的主机名验证（SSL端点识别算法），以防止中间人攻击。可以通过设置参数ssl.endpoint.identification.algorithm为空字符串来禁用服务器主机名验证。例如: 1ssl.endpoint.identification.algorithm= 另外高版本支持不停集群服务下，进行动态配置，使用脚本kafka-configs.sh，参考命令如下： 1bin/kafka-configs.sh --bootstrap-server localhost:9093 --entity-type brokers --entity-name 0 --alter --add-config "listener.name.internal.ssl.endpoint.identification.algorithm=" 对于较旧的Kafka版本，ssl.endpoint.identification.algorithm默认情况下未定义，因此不会启用主机名验证。若该属性设置HTTPS，则启用主机名验证，例如： 1ssl.endpoint.identification.algorithm=HTTPS 需要注意的是，一旦启用主机名验证，客户端将根据以下两个字段之一验证服务器的完全限定域名（FQDN）： 通用名称（CN，Common Name） 主题备用名称（SAN，Subject Alternative Name） 两个字段都有效，但RFC-2818建议使用SAN。 SAN也更灵活，允许声明多个DNS条目。 另一个优点是，CN可以设置为更有意义的值用于授权。如要添加SAN字段，需要将以下参数-ext SAN = DNS：{FQDN}添加到keytool命令中，例如： 1$ keytool -keystore server.keystore.jks -alias localhost -validity &#123;validity&#125; -genkey -keyalg RSA -ext SAN=DNS:&#123;FQDN&#125; 更通俗一点讲，SSL 握手期间验证主机名时，它会检查服务器证书是否具有 SAN 集。如果检测到 SAN 集，那么只使用 SAN 集中的名称或 IP 地址。如果未检测到 SAN 集，那么只使用主题专有名称 (DN) 最重要的属性，通常是通用名称(CN)。将该值与客户端尝试连接的服务器启的主机名进行比较。如果它们相同，主机名验证成功，允许建立连接。 1.2.3 生成SSL密钥和证书（密钥库）为了方便管理证书密钥，我们使用统一的路径保存。例如统一放在/usr/ca作为文件目录。 1$ mkdir -p /usr/ca/&#123;root,server,client,trust&#125; 这里各文件夹的功能是：root：存储CA私钥和证书；server：存储服务端的私钥和证书；client：存储客户端私钥和证书；trust：存储信任库文件； 节点1（kafka.app.node1） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn" -storepass app123 -ext SAN=DNS:kafka.app.node1 其中dname参数的含义参考Keytool工具介绍，文件名为：server.keystore.jks，这是密钥库。 节点2（kafka.app.node2） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn" -storepass app123 -ext SAN=DNS:kafka.app.node2 节点3（kafka.app.node3） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -validity 3650 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=depart,O=org,L=shanghai,S=shanghai,C=cn" -storepass app123 -ext SAN=DNS:kafka.app.node3 证书生成后可以通过下面的命令进行查询（需要输入密钥库管理密码，即keypass的参数）： 1$ keytool -list -v -keystore server.keystore.jks 1.2.4 创建Kafka集群CA证书集群中每个服务节点都有一对公钥和私钥，以及用于标识该节点的证书。但这个证书是未签名的，存在中间者攻击的风险。所以需要证书颁发机构（CA）负责签署颁发证书，使用openssl工具实现。 同一个集群的所有节点共用一个CA证书，所以只需要在集群的一个节点（集群外节点均可）生成CA证书，然后分发给集群其他节点。例如在kafka.app.node1节点上创建CA证书，命令如下： 1$ openssl req -new -x509 -keyout /usr/ca/root/ca.key.pem -out /usr/ca/root/ca.cert.pem -days 3650 -passout pass:app123 -subj "/C=cn/ST=shanghai/L=shanghai/O=org/OU=depart/CN=kafka.app.node1" 然后使用scp命令分发给其他节点： 12$ scp /usr/ca/root/* root@kafka.app.node2:/usr/ca/root/$ scp /usr/ca/root/* root@kafka.app.node3:/usr/ca/root/ 生成两个文件，分别是私钥（ca.key.pem）和证书（ca.cert.pem），它用来签署其他证书。 1.2.5 集群服务节点签署证书首先给集群各服务节点签发证书（即签名）。步骤如下： 第一步 从密钥容器中提取和导出服务端证书（输出文件：server.cert-file，未签名） 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.itdw -certreq -file /usr/ca/server/server.cert-file -storepass app123 第二步 给服务端证书签名（输出文件：server.cert-signed，已签名） 1$ openssl x509 -req -CA /usr/ca/root/ca.cert.pem -CAkey /usr/ca/root/ca.key.pem -in /usr/ca/server/server.cert-file -out /usr/ca/server/server.cert-signed -days 365 -CAcreateserial -passin pass:app123 第三步 将CA证书导入服务端密钥容器中 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123 第四步 将已签名的证书导入密钥容器中 1$ keytool -keystore /usr/ca/server/server.keystore.jks -alias kafka.app -import -file /usr/ca/server/server.cert-signed -storepass app123 需要注意集群上每个服务节点均需要签署。 1.2.6 生成服务端信任库如果kafka集群中配置中的参数ssl.client.auth设置为： requested或required，需要为集群节点提供一个信任库，这个库中需要包含所有CA证书。 使用下面的命令将CA证书导入服务端信任库，输出为信任库文件：server.truststore.jks 1$ keytool -keystore /usr/ca/trust/server.truststore.jks -alias CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123 将CA证书导入服务端信任库，意味着信任该CA证书签名的所有证书。此属性称为信任链，在大型Kafka群集上部署SSL时特别有用。您可以使用单个CA对群集中的所有证书进行签名，并使所有计算机共享信任该CA的同一信任库。这样，所有计算机都可以对所有其他计算机进行身份验证。 1.2.7 配置Kafka BrokersKafka Broker节点支持侦听多个端口上的连接。在server.properties中配置，多个端口类型使用逗号分隔，我们以集群中kafka.app.node1为例： 1listeners=SSL://kafka.app.node1:9092 代理端需要以下SSL配置 12345ssl.keystore.location=/usr/ca/server/server.keystore.jksssl.keystore.password=app123ssl.key.password=app123ssl.truststore.location=/usr/ca/trust/server.truststore.jksssl.truststore.password=app123 其他可选配置设置： ssl.client.auth（可选） 参数控制SSL认证模式。默认参数值为requested，默认使用单向认证，即客户端认证Kafka brokers。此时，没有证书的客户端仍然可以连接集群。参数值为required，指定开启双向验证(2-way authentication)。Kafka服务器同时会验证客户端证书。生成集群建议开始双向认证。 ssl.cipher.suites（可选） 密码套件是认证，加密，MAC和密钥交换算法的命名组合，用于协商使用TLS或SSL网络协议的网络连接的安全设置。（默认为空列表） ssl.enabled.protocols 建议参数值为TLSv1.2,TLSv1.1,TLSv1。列出支持的SSL协议。生成环境不建议使用SSL，建议使用TLS。 ssl.keystore.type和ssl.truststore.type` 文件格式：JKS security.inter.broker.protocol参数 kafka集群节点（brokers）之间启用SSL通讯，需要配置该配置参数为：SSL。 最后我们总结合并一下所有的配置参数： 123456789101112listeners=SSL://kafka.app.node1:9092ssl.keystore.location=/usr/ca/server/server.keystore.jksssl.keystore.password=app123ssl.key.password=app123ssl.truststore.location=/usr/ca/trust/server.truststore.jksssl.truststore.password=app123ssl.client.auth=requiredssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1ssl.keystore.type=JKS ssl.truststore.type=JKS ssl.endpoint.identification.algorithm=HTTPSsecurity.inter.broker.protocol=SSL 1.2.8 初步验证正常启动集群的Zookeeper集群，然后依次启动集群的所有节点。使用下面的命令检查： 1$ openssl s_client -debug -connect kafka.app.node1:9092 -tls1 该命令检查服务器的密钥库和信任库是否正确设置。命令中tls1必须是集群配置参数ssl.enabled.protocols所支持的协议。 12345678910111213141516171819202122232425262728293031Certificate chain（省略）---Server certificate-----BEGIN CERTIFICATE-----（省略）-----END CERTIFICATE-----subject=（省略）issuer=（省略）---No client certificate CA names sent---SSL handshake has read 2029 bytes and written 264 bytes---New, TLSv1/SSLv3, Cipher is ECDHE-RSA-DES-CBC3-SHAServer public key is 2048 bitSecure Renegotiation IS supportedCompression: NONEExpansion: NONESSL-Session: Protocol : TLSv1 Cipher : ECDHE-RSA-DES-CBC3-SHA Session-ID: 5E580D610AEB5DDD8BCD0D31E88180F45391109792CA3CDD1E861EB87C704261 Session-ID-ctx: Master-Key: E544FF34B993B2C3B7F7CB28D8166213F8D3A9864A82247F6948E33B319CD1A8943127DDF9B528EA73435EBC73B0DD55 Key-Arg : None Start Time: 1582828897 Timeout : 7200 (sec) Verify return code: 7 (certificate signature failure)---（省略） 如果证书未显示或有其他错误消息，则说明设置不正确。 另外对于’OpenSSL 0.9.8j-fips 07 Jan 2009’版本的openssl版本，由于这个版本不能自己检测出ssl的版本。会报下面的错误信息。 1816:error:1408E0F4:SSL routines:SSL3_GET_MESSAGE:unexpected message:s3_both.c:463: 1.3 配置kafka客户端kafka集群需要支持集群内外的客户端交互访问。安全集群的客户端同样需要进行相关安全配置。这里客户端指的是Console客户端。 1.3.1 签发客户端证书类似集群内部服务端的证书签发步骤，客户端证书签发过程入下： 生成客户端SSL密钥和证书，输出密钥容器：client.keystore.jks 12$ keytool -keystore /usr/ca/client/client.keystore.jks -alias kafka.app.node1 -validity 365 -genkey -keypass app123 -keyalg RSA -dname "CN=kafka.app.node1,OU=dccsh,O=icbc,L=shanghai,S=shanghai,C=cn" -ext SAN=DNS:kafka.app.node1 -storepass app123 从密钥容器中提取和导出客户端证书（输出文件：client.cert-file，未签名） 1$ keytool -keystore /usr/ca/client/client.keystore.jks -alias kafka.app.node1 -certreq -file /usr/ca/client/client.cert-file -storepass app123 给客户端证书签名（输出文件：client.cert-signed，已签名） 1$ openssl x509 -req -CA /usr/ca/root/ca.cert.pem -CAkey /usr/ca/root/ca.key.pem -in /usr/ca/client/client.cert-file -out /usr/ca/client/client.cert-signed -days 365 -CAcreateserial -passin pass:app123 将CA证书导入客户端密钥容器中 1$ keytool -keystore /usr/ca/client/client.keystore.jks -alias CARoot -import -file /usr/ca/root/client.cert-file -storepass app123 将已签名的证书导入密钥容器中 1$ keytool -keystore /usr/ca/client/client.keystore.jks -alias kafka.app.node1 -import -file /usr/ca/client/client.cert-signed -storepass app123 1.3.2 生成客户端信任库使用下面的命令将CA证书导入客户端信任库，输出为信任库文件：client.truststore.jks 1$ keytool -keystore /usr/ca/trust/client.truststore.jks -alias CARoot -import -file /usr/ca/root/ca.cert.pem -storepass app123 1.3.3 配置客户端客户端的console-producer和console-consumer命令需要添加相关安全配置。 如果kafka集群不需要客户端身份验证，只需配置下面的配置： 123security.protocol=SSLssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=app123 如果需要客户端身份验证，还需要补充下面的配置信息： 123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.keystore.password=app123ssl.key.password=app123 根据我们的要求和代理配置，可能还需要其他配置设置： ssl.provider（可选）。用于SSL连接的安全提供程序的名称。 ssl.cipher.suites（可选）。密码套件是认证，加密，MAC和密钥交换算法的命名组合，用于协商使用TLS或SSL网络协议的网络连接的安全设置。 ssl.enabled.protocols = TLSv1.2，TLSv1.1，TLSv1。它应列出在代理方配置的至少一种协议 ssl.truststore.type = JKS ssl.keystore.type = JKS 最后我们总结合并一下所有的配置参数（编辑文件名为：client-ssl.properties）： 123456security.protocol=SSLssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=app123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.keystore.password=app123ssl.key.password=app123 1.3.4 消费者生产者使用console-producer的命令： 1kafka-console-producer.sh --broker-list kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092 --topic test --producer.config client-ssl.properties 使用console-consumer的命令： 1kafka-console-consumer.sh --bootstrap-server kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092 --topic test --new-consumer --consumer.config client-ssl.properties 这里test为topic名称，在只有SSL通信加密集群中，topic的创建、删除、生产、消费并没有权限管理，依然存在安全问题。所以kafka集群需要进一步配置权限管理。 第二部分 Kafka集群权限认证Kafka集群的权限认证管理主要涉及： 身份认证（Authentication）。对客户端与服务器的连接进行身份认证，brokers和zookeeper之间的连接进行Authentication（producer 和 consumer）、其他 brokers、tools与 brokers 之间连接的认证。 权限控制（Authorization）。实现对于消息级别的权限控制，客户端的读写操作进行Authorization（生产、消费）管理。 通俗的讲，身份认证解决的是证明你是谁，而权限控制解决的是你能干什么。在Kafka中身份认证和权限控制是两套独立的安全配置。 2.1 集群权限认证策略Kafka从0.9.0.0版本后开始支持下面的SASL安全策略管理。这些安全功能为Kafka通信安全、多租户管理、集群云化提供了安全保障。截止目前Kafka 2.3版本，一共支持5种SASL方式。 验证方式 版本 说明 SASL/PLAIN 0.10.0.0 不能动态增加用户 SASL/SCRAM 0.10.2.0 可以动态增加用户。有两种方式：SASL/SCRAM-SHA-256 和SASL/SCRAM-SHA-512 SASL/GSSAPI 0.9.0.0 需要独立部署验证服务（即Kerberos服务） SASL/OAUTHBEARER 2.0.0 需自己实现接口实现token的创建和验证，需要额外Oauth服务 SASL/Delegation Token 1.1.0 补充现有 SASL 机制的轻量级认证机制 对于生产环境，SASL/PLAIN方式有个缺点：只能在JAAS文件KafkaServer参数中配置用户，集群运行期间无法动态新增用户（需要重启重新加载JAAS文件），这对维护管理带来不便。而SASL/SCRAM方式，将认证数据存储在Zookeeper中，可以动态新增用户并分配权限。 SASL/GSSAPI方式需要依赖Kerberos服务。对于一些已经部署了集中式的Kerberos服务的大厂，只需要申请一个principal即可。如果生产Kerberos认证中出现TGT分发性能瓶颈，可以使用SASL/Delegation Token模式。使用 Kafka 提供的 API 去获取对应的 Delegation Token。Broker 和客户端在做认证的时候，可以直接使用这个 token，不用每次都去 KDC 获取对应的 ticket（Kerberos 认证），减少性能压力。 同样SASL/OAUTHBEARER方式需要Oauth服务。 各种方式引入版本不同，使用依赖各有差异，需要结合自身业务特点选择合适的架构方式。 2.2 SASL/SCRAM策略配置介绍SASL/SCRAM方式将身份认证和权限控制的凭证（credential）数据均存储在Zookeeper中，需要对Zookeeper进行安全配置。 2.2.1 Zookeeper集群侧配置对Zookeeper集群中所有节点更新下面的策略后，重启集群生效。 配置zoo.cfg文件 文件尾部追加下面的配置： 123authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProviderrequireClientAuthScheme=sasljaasLoginRenew=3600000 新增zk_server_jaas.conf文件 配置文件内容如下： 123456Server &#123;org.apache.kafka.common.security.plain.PlainLoginModule requiredusername="admin"password="admin-secret"user_admin="admin-secret";&#125;; 其中username和password定义的用户和密钥，用于Zookeeper与Kafka集群进行认证。配置项user_admin=&quot;admin-secret&quot; 中 admin为用户名，admin-secret为密码，用于Zookeeper集群外客户端和集群内进行认证。 拷贝依赖包 将kafka文件系统中kafka/libs目录下的jar包拷贝到zookeeper/lib目录。 12345kafka-clients-2.1.1.jarlz4-java-1.5.0.jarosgi-resource-locator-1.0.1.jarslf4j-api-1.7.25.jarsnappy-java-1.1.7.2.jar 若没有引入依赖包，启动时会报找不到org.apache.kafka.common.security.plain.PlainLoginModule包的错误。 修改zookeeper启动参数 修改bin/zkEnv.sh文件, 在文件尾追加下面的配置内容。该配置完成引入的包的加载。变量CLASSPATH和SERVER_JVMFLAGS都会在Zookeeper启动时传给JVM虚拟机。 下面的配置中$ZOOKEEPER_HOME是zookeeper的环境变量，如果没有配置，使用绝对路径即可。 1234for i in $ZOOKEEPER_HOME/lib/*.jar; do CLASSPATH="$i:$CLASSPATH"doneSERVER_JVMFLAGS=" -Djava.security.auth.login.config=$ZOOKEEPER_HOME/conf/zk_server_jaas.conf" 2.2.2 kafka集群侧配置kafka集群中每一台节点均需要更新下面的配置。 新增kafka_server_scram_jaas.conf文件（在config目录中） 12345678910KafkaServer &#123;org.apache.kafka.common.security.scram.ScramLoginModule requiredusername="admin"password="admin-secret";# 自定义用户：# user_admin="admin-secret"# user_alice="alice-secret"# user_reader="reader-secret"# user_writer="writer-secret";&#125;; 其中配置username和password为Kafka集群之间通讯的SCRAM凭证，用户名为admin，密码为admin-secret。 配置中类似user_XXX格式的配置项为自定义用户。如果是SASL/PLAIN方式，用户只能在该文件中定义，不能动态新增。我们使用SASL/SCRAM方式，可以后续动态声明admin用户，不再此处进行配置。 更新Kafka的配置文件server.properties（在config目录中）： 12345678910111213141516171819#SASL CONFIGlisteners=SASL_SSL://kafka.app.node1:9092sasl.enabled.mechanisms=SCRAM-SHA-512sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer#allow.everyone.if.no.acl.found=truesuper.users=User:admin#SSL CINFIGssl.keystore.location=/usr/ca/server/server.keystore.jksssl.keystore.password=app123ssl.key.password=app123ssl.truststore.location=/usr/ca/trust/server.truststore.jksssl.truststore.password=app123ssl.client.auth=requiredssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1ssl.keystore.type=JKSssl.truststore.type=JKSssl.endpoint.identification.algorithm=HTTPSsecurity.inter.broker.protocol=SASL_SSL 需要注意参数allow.everyone.if.no.acl.found，如果开启参数开关，当客户端和集群交互时候未找到ACL策略时，允许所有类型的访问操作。建议该参数关闭（false）。 参数security.inter.broker.protocol指定集群brokers之间的通讯协议。不加密协议有：SASL_SSL、SASL_PLAINTEXT、PLAINTEXT；加密协议有：SSL。为了提高节点之间的交互性能，内部网络环境建议使用非加密协议。这里使用加密的SASL_SSL协议。 参数super.users指定了集群的超级用户为：admin。注意如果指定多个超级用户，每个用户使用分号隔开，例如：super.users=User:admin;User:alice 参数sasl.enabled.mechanisms列出支持的认证方式。即可以支持多种。 参数sasl.mechanism.inter.broker.protocol指定集群内部的认证方式。Kafka仅支持最小迭代次数为4096的强哈希函数SHA-256和SHA-512。所以有SCRAM-SHA-512和SCRAM-SHA-256两种方式。 配置kafka启动环境变量（bin目录下面的kafka-run-class.sh） 为 Kafka 添加 java.security.auth.login.config 环境变量（配置文件路径）。并且在启动模式中添加KAFKA_SASL_OPTS。 12345678# 截取配置文件片段：KAFKA_SASL_OPTS='-Djava.security.auth.login.config=/opt/software/kafka/config/kafka_server_scram_jaas.conf'# Launch modeif [ "x$DAEMON_MODE" = "xtrue" ]; then nohup $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS "$@" &gt; "$CONSOLE_OUTPUT_FILE" 2&gt;&amp;1 &lt; /dev/null &amp;else exec $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_SASL_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS "$@"fi 2.2.3 SCRAM认证管理在集群的配置文件kafka_server_scram_jaas.conf中，定义了集群内部的认证用户。对于客户端和集群之间认证可以使用kafka-configs.sh来动态创建。 创建用户SCRAM凭证 例如集群中的超级用户admin用户，使用下面的命令创建： 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --add-config 'SCRAM-SHA-256=[password=admin-secret],SCRAM-SHA-512=[password=admin-secret]' --entity-type users --entity-name admin 创建自定义普通用户alice。 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --add-config 'SCRAM-SHA-256=[iterations=8192,password=alice-secret],SCRAM-SHA-512=[password=alice-secret]' --entity-type users --entity-name alice 查看SCARM凭证 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --describe --entity-type users --entity-name admin 删除SCRAM凭证 1$ kafka-configs.sh --zookeeper kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --alter --delete-config 'SCRAM-SHA-512' --entity-type users --entity-name alice 2.3 Kafka客户端配置Kafka集群配置了认证，那么对于Console客户端访问集群自然需要配置认证信息。可集群节点内部通讯凭证的认知，同样需要定义JAAS文件。加入我们自定义了用户alice，JAAS文件名为：kafka_console_client_jaas.conf，配置内容如下： 12345KafkaClient &#123;org.apache.kafka.common.security.scram.ScramLoginModule requiredusername="alice"password="alice-secret";&#125;; 然后更新kafka-console-producer.sh脚本和kafka-console-consumer.sh脚本的启动参数。 12345# 文件截取更新部分：if [ "x$KAFKA_OPTS" ]; thenexport KAFKA_OPTS="-Djava.security.auth.login.config=/opt/software/kafka/config/kafka_write_jaas.conf"fi 在配置SSL时候，我们新建了client-ssl.properties配置文件，作为Console客户端启动配置。在集群启用SASL_SSL后，我们同步更新如下： 123456security.protocol=SASL_SSLssl.truststore.location=/usr/ca/trust/client.truststore.jksssl.truststore.password=app123ssl.keystore.location=/usr/ca/client/client.keystore.jksssl.keystore.password=app123ssl.key.password=app123 至此Console客户端已经配置完毕，但目前Console客户端还不能通过命令方式和集群进行交互，因为我们指定的用户对于集群的资源还没有任何权限。需要对用户进行集群资源的ACL控制设置，赋予相关权限。 2.4 ACL控制Kafka权限资源包含Topic、Group、Cluster、TransactionalId（事务id），每个资源涉及的权限内容如下： 资源类型 权限类型 Topic Read,Write,Describe,Delete,DescribeConfigs,AlterConfigs,All Group Read,Describe,All Cluster Create,ClusterAction,DescribeConfigs,AlterConfigs,IdempotentWrite,Alter,Describe,All TransactionalId Describe,Write,All 对于常用类型进行说明： 权限 说明 Read 读取topic、group信息 Write 写topic、TransactionalId（存储在内部topic） Delete 删除topic Create 创建topic ALTER 修改topic Describe 获取topic、group、TransactionalId信息 ALL 所有权限 Kafka提供ACL管理脚本：kafka-acls.sh。 2.4.1 更新脚本配置认证数据均存储在Zookeeper集群中，需要和Zookeeper交互自然需要配置相关认证信息。 首先需要新建JAAS文件，文件名为：zk_client_jaas.conf。这里的用户已经在Zookeeper集群中进行定义。 12345Client &#123;org.apache.kafka.common.security.plain.PlainLoginModule requiredusername="admin"password="admin-secret";&#125;; 最后更新kafka-acls.sh脚本： 12345# 截取更新部分if [ "x$KAFKA_OPTS" ]; thenexport KAFKA_OPTS="-Djava.security.auth.login.config=/opt/software/kafka/config/zk_client_jaas.conf"fi 当然Kafka集群的配置文件中已经开启了ACL： 1authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer 至此完成配置。 2.4.2 ACL配置根据官网的介绍，ACL的格式如下： “Principal P is [Allowed/Denied] Operation O From Host H On Resource R” 参数含义描述如下： principal：指定一个Kafka user； operation：指定一个具体的操作类型，例如：Read, Write, Delete等； Host：表示与集群交互的客户端IP地址，如果是通配符‘*’表示所有IP。目前不支持主机名（hostname）形式，只能是IP地址； Resource：指定一种Kafka资源类型（共有4种类型）； 例如下面的ACL命令： 1$ sh kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --add --allow-principal User:alice --allow-host '*' --operation ALL --topic test 赋权之后，用户alice对test具有全部权限，并且访问请求可以是来自任何IP的客户端。 常用参数的补充说明： 对主机IP的限制参数，allow-host指定允许的IP，deny-host指定禁用IP； 新增和删除一个赋权策略，分别使用：add和remove 2.4.3 ACL策略查看使用参数list参看ACL策略。例如： 1$ sh kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect=kafka.app.node1:2181,kafka.app.node2:2181,kafka.app.node3:2181 --list --topic test-topic 该查看命令显示test-topic资源相关的所有ACL策略。 2.4.4 超级用户kafka集群的配置文件server.properties中定义了超级用户（Super Users），超级用户不在ACL控制范围内，默认可以访问集群中所有资源，并具备所有权限。 2.5 权限认证数据访问集群的认证数据存储在Zookeeper，可以通过Zookeeper的console客户端访问认证数据。 使用zookeeper自带的命令行客户端： 12345/dmqs/zookeeper/bin&gt; ./zkCli.shConnecting to localhost:2181Welcome to ZooKeeper!JLine support is enabled[zk: localhost:2181(CONNECTING) 0] 查看zookeeper中的数据： 12[zk: localhost:2181(CONNECTED) 1] ls /[cluster, controller_epoch, controller, brokers, zookeeper, kafka-acl, kafka-acl-changes, admin, isr_change_notification, consumers, config] 其中kafka-acl中存储相关权限认证数据。 12[zk: localhost:2181(CONNECTED) 3] ls /kafka-acl[Cluster, Topic] 可以查看其中的权限信息。 2.6 SSL和SASL的说明SSL是传输层安全协议，是位于传输层（TCP/IP）和应用层（HTTP）的协议，SSL是对整个传输过程的加密，SSL是对客户端和服务器之间传输的所有数据进行加密。假如在配置的时候使用了SASL，但是没有使用SSL，那么除了账号密码外，所有的传输内容都是裸奔的。 所以生产集群采用SSL和SASL结合方式，即SSL_SASL方式。 第三部分 安全集群的客户端3.1 开发语言类3.1.1 Python客户端目前市面上kafka的python API常用的有三种： 第一种 kafka 该项目是kafka-python的老项目，2017年后调整为kafka-python项目。 第二种 kafka-python 最新版本为2.0，首先从客户端的密钥库中导出CA证书。 1$ keytool -exportcert -alias CARoot -keystore client.keystore.jks -rfc -file ca.cert.pem 生产者和消费者的案例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-from kafka import KafkaConsumer, KafkaProducerimport kafkaimport sslimport loggingimport time#logging.basicConfig(level=logging.DEBUG)try: bootstrap_servers = 'kafka.itdw.node1:9092,kafka.itdw.node2:9092,kafka.itdw.node3:9092' topic = "test" sasl_mechanism = "SCRAM-SHA-512" username = "alice" password = "alice-secret" security_protocol = "SASL_SSL" # CA 证书路径 ssl_cafile = 'ca.cert.pem' # SSL context = ssl.SSLContext(ssl.PROTOCOL_SSLv23) context.verify_mode = ssl.CERT_NONE context.check_hostname = False context.load_verify_locations(ssl_cafile) # 消费者 consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers, api_version=(0, 10), security_protocol=security_protocol, ssl_context=context, sasl_mechanism = sasl_mechanism, sasl_plain_username = username, sasl_plain_password = password ) # 生产者 producer = KafkaProducer(bootstrap_servers=bootstrap_servers, api_version=(0, 10), acks='all', retries=1, security_protocol=security_protocol, ssl_context=context, sasl_mechanism=sasl_mechanism, sasl_plain_username=username, sasl_plain_password=password ) # 生产数据 for i in range(10): producer.send(topic, bytes("测试",encoding='utf8')) producer.flush() # 消费数据 for msg in consumer: print(msg)except Exception as e: print(e) 需要的注意的事项有： Kafka集群启用主机名模式，所以应用程序运行节点的hosts文件需要配置Kafka集群节点的域名映射。 ssl_context参数为包装套接字连接的预配置SSLContext。如果非None，将忽略所有其他ssl_ *配置。 主机名验证问题。如果证书中域名和主机名不匹配，客户端侧需要配置需要调整如下： 12ssl_ctx.check_hostname=Falsessl_ctx.verify_mode = CERT_NONE 如果不提前预配置SSLContext，还需要客户端的证书。 1$ keytool -exportcert -alias localhost -keystore client.keystore.jks -rfc -file client.cert.pem 生产者的参数需要添加： 1234567891011121314ssl_certfile = "client.cert.pem"ssl_cafile = "ca.cert.pem"producer = KafkaProducer(bootstrap_servers=bootstrap_servers, api_version=(0, 10), acks='all', retries=1, security_protocol=security_protocol, ssl_context=context, sasl_mechanism=sasl_mechanism, sasl_plain_username=username, sasl_plain_password=password, ssl_check_hostname=False, ssl_certfile=ssl_certfile, ssl_cafile=ssl_cafile) 第三种 confluent-kafka confluent-kafka包由confluent公司开源，主要是对C/C++客户端包（librdkafka）的封装。案例代码如下： 12345678910111213141516171819202122232425# -*- coding: utf-8 -*-from confluent_kafka import Producer# 回调函数def delivery_report(err, msg): """ Called once for each message produced to indicate delivery result. Triggered by poll() or flush(). """ if err is not None: print(’Message delivery failed: &#123;&#125;‘.format(err)) else: print(‘Message delivered to &#123;&#125; [&#123;&#125;]‘.format(msg.topic(), msg.partition()))if __name__ == ‘__main__‘: producerConfing = &#123;"bootstrap.servers": 'kafka.itdw.node1:9092,kafka.itdw.node2:9092,kafka.itdw.node3:9092', "security.protocol": 'SASL_SSL', "sasl.mechanisms": 'SCRAM-SHA-256', "sasl.username": 'alice', "sasl.password": 'alice-secret', "ssl.ca.location": 'ca.cert.pem' &#125; ProducerTest = Producer(producerConfing) ProducerTest.poll(0) ProducerTest.produce(‘testTopic‘, ‘confluent kafka test‘.encode(‘utf-8‘),callback=delivery_report) ProducerTest.flush() 3.1.2 Go客户端我们的Go语言中常用的Kafka的客户端包有： 1234"github.com/Shopify/sarama""github.com/bsm/sarama-cluster""github.com/confluentinc/confluent-kafka-go/kafka""github.com/segmentio/ksuid" 其中最常用的是sarama，案例参考github项目。 3.1.3 Java客户端生产者： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.kafka.security;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerConfig;import org.apache.kafka.clients.producer.ProducerRecord;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import java.util.Properties;import java.util.Random;public class KafkaProducerWithSASL_SSL &#123; private static final String KAFKA_TOPIC = "topsec"; private static final String BOOTSTRAP_SERVER = "docker31:9092"; private static final String[] strs = new String[]&#123;"zhao", "qian", "sun", "li", "zhou", "wu", "zheng", "wang", "feng", "chen"&#125;; private static final Random r = new Random(); public static void main(String[] args) &#123; try &#123; producer(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private static void producer() throws InterruptedException &#123; Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER); //SASL_SSL加密 props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL"); props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\trust\\client.truststore.jks"); props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "hadoop"); // SSL用户认证 props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\client\\client.keystore.jks"); props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "hadoop"); props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "hadoop"); //SASL用户认证 props.put(SaslConfigs.SASL_JAAS_CONFIG,"org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";"); props.put(SaslConfigs.SASL_MECHANISM, "SCRAM-SHA-512"); props.put(ProducerConfig.ACKS_CONFIG, "all"); props.put(ProducerConfig.RETRIES_CONFIG, 0); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer"); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer"); props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384); props.put(ProducerConfig.LINGER_MS_CONFIG, 1); props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); while (true) &#123; producer.send(new ProducerRecord&lt;&gt;(KAFKA_TOPIC, strs[r.nextInt(10)],strs[r.nextInt(10)])); Thread.sleep(2000); &#125; &#125;&#125; 消费者： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.topsec.kafka.security;import org.apache.kafka.clients.CommonClientConfigs;import org.apache.kafka.clients.consumer.ConsumerConfig;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import org.apache.kafka.common.config.SaslConfigs;import org.apache.kafka.common.config.SslConfigs;import java.util.Collections;import java.util.Properties;public class KafkaConsumerWithSASLAndSSL &#123; private static final String KAFKA_TOPIC = "topsec"; private static final String BOOTSTRAP_SERVER = "docker31:9092"; public static void main(String[] args) &#123; consumer(); &#125; private static void consumer() &#123; Properties props = new Properties(); //SASL_SSL加密配置 props.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL"); props.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\trust\\client.truststore.jks"); props.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "hadoop"); //SSL身份验证配置 props.put(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, "D:\\Download\\ca\\client\\client.keystore.jks"); props.put(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, "hadoop"); props.put(SslConfigs.SSL_KEY_PASSWORD_CONFIG, "hadoop"); //SASL身份验证 props.put(SaslConfigs.SASL_JAAS_CONFIG,"org.apache.kafka.common.security.scram.ScramLoginModule required username=\"admin\" password=\"admin-secret\";"); props.put(SaslConfigs.SASL_MECHANISM, "SCRAM-SHA-512"); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVER); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer"); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer"); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000"); props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, "6000"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Collections.singletonList(KAFKA_TOPIC)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(2000); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("offset = %d, key = %s, value = %s, partition = %d %n", record.offset(), record.key(), record.value(), record.partition()); &#125; &#125; &#125;&#125; 3.2 组件类3.2.1 Console客户端客户端节点部署kafka项目，在bin目录下面我们已经更新了kafka-console-consumer.sh和kafka-console-producer.sh两个脚本。并分别新增了加密访问的配置文件consumer.config和producer.config。 命令案例参考：1.3.4 章节内容。 3.2.2 Flume客户端目前Flume项目官网项目文档介绍支持下面三种方式： SASL_PLAINTEXT - 无数据加密的 Kerberos 或明文认证； SASL_SSL - 有数据加密的 Kerberos 或明文认证； SSL - 基于TLS的加密，可选的身份验证； 事实上对于SASL/SCRAM方式Flume也是支持的。具体配置如下（以Flume1.9版本为例）： 3.2.2.1 第一步 新增jaas配置文件在Flume的conf配置目录下面新增flume_jaas.conf文件，文件内容： 123456789101112Server &#123; org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="admin-secret" user_admin="admin-secret";&#125;;KafkaClient &#123; org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";&#125;; 3.2.2.2 第二步 更新flume-env.sh文件Flume的conf配置目录下面flume-env.sh文件添加JAVA_OPTS配置更新： 1JAVA_OPTS="$JAVA_OPTS -Djava.security.auth.login.config=/dmqs/apache-flume-1.9.0-bin/conf/flume_jaas.conf" 其中路径为第一步中新增的flume_jaas.conf文件路径。 3.2.2.3 测试案例（sinks）我们使用一个简单的案例来测试，Flume的source为监控文件尾写入，Flume的sinks为加密kafka集群。具体配置如下： 123456789101112131415161718192021222324252627282930313233343536373839#definea1.sources = r1a1.sinks = k1a1.channels = c1# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1# sourcea1.sources.r1.type = execa1.sources.r1.command = tail -f /dmqs/apache-flume-1.9.0-bin/data/flume/flume.loga1.sources.r1.shell = /bin/bash -c# channela1.channels.c1.type = memorya1.channels.c1.capacity = 15000a1.channels.c1.transactionCapacity = 15000# sinka1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSinka1.sinks.k1.kafka.bootstrap.servers = kafka.itdw.node1:9093a1.sinks.k1.kafka.topic = flumea1.sinks.k1.kafka.flumeBatchSize = 15000a1.sinks.k1.kafka.producer.acks = 1a1.sinks.k1.kafka.producer.linger.ms = 1000a1.sinks.k1.kafka.producer.security.protocol=SASL_SSLa1.sinks.c1.kafka.producer.sasl.mechanism =SCRAM-SHA-512a1.sinks.c1.kafka.producer.sasl.jaas.config =org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret"####a1.sinks.k1.kafka.producer.ssl.truststore.location =/usr/ca/trust/client.truststore.jksa1.sinks.k1.kafka.producer.sasl.mechanism =SCRAM-SHA-512a1.sinks.k1.kafka.producer.ssl.truststore.password=app123a1.sinks.k1.kafka.producer.ssl.keystore.location=/usr/ca/client/client.keystore.jksa1.sinks.k1.kafka.producer.ssl.keystore.password=app123a1.sinks.k1.kafka.producer.ssl.key.password=app123a1.sinks.k1.kafka.producer.timeout.ms = 100a1.sinks.k1.batchSize=15000a1.sinks.k1.batchDurationMillis=2000 配置保存为flume-sink-auth-kafka.conf,为了检查输出结果使用下面命令启动（在bin目录中）： 1./flume-ng agent --conf ../conf --conf-file ../conf/flume-sink-auth-kafka.conf --name a1 -Dflume.root.logger=INFO,console 向文件尾部追加信息： 1echo "test" &gt;&gt; /dmqs/apache-flume-1.9.0-bin/data/flume/flume.log 然后使用消费者客户端查看数据是否写入kafka的flume主题中。 3.2.2.3 测试案例（source）同样可以也可以将加密Kafka作为Flume的source，配置案例如下： 1234567891011121314151617181920212223242526272829303132333435#definea1.sources = r1a1.sinks = k1a1.channels = c1# binda1.sources.r1.channels = c1a1.sinks.k1.channel = c1# sourcea1.sources.r1.type = org.apache.flume.source.kafka.KafkaSourcea1.sources.r1.kafka.bootstrap.servers = kafka.app .node1:9093,kafka.app.node2:9093,kafka.app.node3:9093a1.sources.r1.kafka.topics = flumea1.sources.r1.kafka.consumer.group.id = flumea1.sources.r1.kafka.consumer.timeout.ms = 2000a1.sources.r1.batchSize=150a1.sources.r1.batchDurationMillis=1000#####a1.sources.r1.kafka.consumer.ssl.truststore.location =/usr/ca/trust/client.truststore.jksa1.sources.r1.kafka.consumer.sasl.mechanism =SCRAM-SHA-512a1.sources.r1.kafka.consumer.ssl.truststore.password=itdw123a1.sources.r1.kafka.consumer.ssl.keystore.location=/usr/ca/client/client.keystore.jksa1.sources.r1.kafka.consumer.ssl.keystore.password=itdw123a1.sources.r1.kafka.consumer.ssl.key.password=itdw123a1.sources.r1.kafka.consumer.security.protocol=SASL_SSLa1.sources.r1.kafka.consumer.sasl.mechanism =SCRAM-SHA-512a1.sources.r1.kafka.consumer.sasl.jaas.config =org.apache.kafka.common.security.scram.ScramLoginModule required username="admin" password="admin-secret";# channela1.channels.c1.type = memorya1.channels.c1.capacity = 15000a1.channels.c1.transactionCapacity = 15000# sinka1.sinks.k1.type = file_rolla1.sinks.k1.sink.directory = /dmqs/apache-flume-1.9.0-bin/data/flumea1.sinks.k1.sink.serializer = TEXT 案例中将加密Kafka中flume主题中的数据汇入到指定目录的文件中。 3.2.3 Logstash客户端Logstash和Kafka交互使用Kafka output plugin插件实现。其中配置文件中output部分如下： 123456789101112131415161718output &#123; kafka &#123; id =&gt; "kafkaSLL_SASL" codec =&gt; "json" ssl_endpoint_identification_algorithm =&gt; "" bootstrap_servers =&gt; "kafka.app.node1:9092,kafka.app.node2:9092,kafka.app.node3:9092" jaas_path =&gt;"/etc/logstash/certificates/kafka_servcer_jaas.conf" ssl_keystore_location =&gt; "/etc/logstash/certificates/client.keystore.jks" ssl_keystore_password =&gt; "app123" ssl_keystore_type =&gt; "JKS" ssl_truststore_location =&gt; "/etc/logstash/certificates/client.truststore.jks" ssl_truststore_password =&gt; "app123" ssl_truststore_type =&gt; "JKS" sasl_mechanism =&gt; "SCRAM-SHA-512" security_protocol =&gt; "SASL_SSL" topic_id =&gt; "test" &#125;&#125; 参考：https://www.elastic.co/guide/en/logstash/current/plugins-outputs-kafka.html 第四部分 加密认证集群的性能压测集群启用SSL后，数据交互中需要加密、解密。kafka集群的I/O性能会降低。我们使用Kafka自带的压侧工具对集群加密前和加密后性能进行评测。 4.1生产者压力测试客户端写入参数配置为acks=all（即Topic中Leader和fellow副本均写入成功）。每条消息大小为1M（消息体小吞吐量会大一些）。另外测试客户端为集群内部节点，忽略了数据网络传输的性能消耗。 4.1.1不加密集群1./kafka-consumer-perf-test.sh --topic topsec --throughput 50000 --num-records 1500000 --record-size 10000 --producer-props bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093 acks=all 测试结果： 11500000 records sent, 38538.615693 records/sec (37.64 MB/sec), 748.44 ms avg latency, 5485.00 ms max latency, 227 ms 50th, 3194 ms 95th, 3789 ms 99th, 3992 ms 99.9th. 4.1.2加密集群1./kafka-producer-perf-test.sh --topic topsec --throughput 50000 --num-records 1500000 --record-size 10000 --producer-props bootstrap.servers=kafka.itdw.node1:9093,kafka.itdw.node2:9093,kafka.itdw.node3:9093 acks=all --producer.config producer.config 测试结果： 11500000 records sent, 16901.027582 records/sec (16.50 MB/sec), 1713.43 ms avg latency, 9345.00 ms max latency, 72 ms 50th, 1283 ms 95th, 2067 ms 99th, 2217 ms 99.9th. 4.2 压侧结论加密改造前，生产者的吞吐量为3.8w 条/秒，改造后1.7W 条/秒。整体吞吐性能降低50%左右，数据的加密、解密导致吞吐量性能降低。平均时延也增加了一倍多（改造前700ms，改造后1700ms）。在实际生产中可参考这个性能折扣基线配置集群资源。 参考文献及资料1、Kafka官网对安全类功能介绍，链接：http://kafka.apache.org/documentation/#security 2、Kafka ACLs in Practice – User Authentication and Authorization，链接：https://developer.ibm.com/opentech/2017/05/31/kafka-acls-in-practice/ 3、维基百科（数字证书），链接：https://zh.wikipedia.org/wiki/公開金鑰認證 4、SSL技术白皮书，链接：https://blog.51cto.com/xuding/1732723 5、Kafka权限管理，链接：https://www.jianshu.com/p/09129c9f4c80 5、Flume文档，链接：https://flume.apache.org/FlumeUserGuide.html#kafka-sinkorg/FlumeUserGuide.html#kafka-sink]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Go语言学习系列(一)Go语言Win开发环境部署]]></title>
    <url>%2F2020%2F01%2F31%2F2020-01-31-Go%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97(%E4%B8%80)Go%E8%AF%AD%E8%A8%80Win%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%20-%20%E5%89%AF%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 windows下安装 第二部分 配置环境变量 第三部分 IDE配置 第四部分 HelloWorld案例 参考文献及资料 背景Go语言的官方网站：https://golang.org/ 由于防火墙原因，请大家在这个网站下载：https://studygolang.com/dl 第一部分 windows下安装下载的是msi包，直接执行安装即可。 go1.12.5.windows-amd64.msi 第二部分 配置环境变量Go 语言需要配置 GOROOT 和 Path 两个环境变量：GOROOT 和 GOPATH。如果使用msi包安装，那么会自动配置好两个环境变量。 可以使用下面的命令检查变量： 1234567891011121314151617181920212223242526272829C:\Users\rongxiang&gt;go envset GOARCH=amd64set GOBIN=set GOCACHE=C:\Users\rongxiang\AppData\Local\go-buildset GOEXE=.exeset GOFLAGS=set GOHOSTARCH=amd64set GOHOSTOS=windowsset GOOS=windowsset GOPATH=C:\Users\rongxiang\goset GOPROXY=set GORACE=set GOROOT=C:\Goset GOTMPDIR=set GOTOOLDIR=C:\Go\pkg\tool\windows_amd64set GCCGO=gccgoset CC=gccset CXX=g++set CGO_ENABLED=1set GOMOD=set CGO_CFLAGS=-g -O2set CGO_CPPFLAGS=set CGO_CXXFLAGS=-g -O2set CGO_FFLAGS=-g -O2set CGO_LDFLAGS=-g -O2set PKG_CONFIG=pkg-configset GOGCCFLAGS=-m64 -mthreads -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=C:\Users\RONGXI~1\AppData\Local\Temp\go-build030961398=/tmp/go-build -gno-record-gcc-switches 查看版本： 12C:\Users\rongxiang&gt;go versiongo version go1.12.5 windows/amd64 默认情况下，GOROOT = C:\Go；GOPATH = C:\Users\用户名\go。如果需要调整，修改环境变量参数即可。 第三部分 IDE配置使用Jetbrain公司的GoLand IDE（https://www.jetbrains.com/go/）。 下载安装成功后，打来GoLand。菜单File–&gt;Settings–&gt;GO中有两个配置项：GOROOT、GOPATH。 第四部分 HelloWorld案例配置好IDE环境，我们新建第一个项目（project）。 4.1 创建HelloWorld项目菜单栏File–&gt;New–&gt;Project,打开新建项目对话框。配置项目的文件位置（Location），例如我们配置为： D:\golang\workspace\HelloWorld，然后确定就新建Go项目。 在项目中新建main.go文件： 12345package mianimport "fmt"func main() &#123; fmt.Println("Hello World!") 4.2 编译并运行选中main.go文件，邮件选择运行。IDE将编译，并运行： 123Hello World!Process finished with exit code 0 控制台上打印上面的信息，说明执行成功。 参考文献及资料1、Go入门指南，https://github.com/Unknwon/the-way-to-go_ZH_CN 2、a tour of go，https://tour.golang.org/welcome/1]]></content>
      <categories>
        <category>Go</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中的Structured Streaming介绍]]></title>
    <url>%2F2020%2F01%2F27%2F2021-05-04-Spark%E4%B8%AD%E7%9A%84Structured%20Streaming%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景Structured Streaming接口在社区2.0版本发布测试接口，主要暴露最初的设计思路及基本接口，不具备在生产环境使用的能力；2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于event_time的window及watermark功能，虽然还在Alapha阶段，但从实现的完备程度及反馈来看已具备初步的功能需求。 发展历程： 2.0版本发布测试接口 2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于event_time的window及watermark功能，虽然还在Alapha阶段，但从实现的完备程度及反馈来看已具备初步的功能需求 spark-2.2.0 ，可用于生产环境 第一部分 设计原理1.1 Spark streaming存在的问题 第二部分 编程实践第三部分 总结https://www.iteblog.com/archives/2084.html http://slamke.github.io/2017/04/06/Structured-Streaming%E4%BB%8B%E7%BB%8D/ https://zhuanlan.zhihu.com/p/51883927 参考文献和资料1、Structured Streaming Programming Guide，链接：http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch系列文章-数据的写入]]></title>
    <url>%2F2020%2F01%2F27%2F2021-05-01-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%95%B0%E6%8D%AE%E7%9A%84%E5%86%99%E5%85%A5%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景参考文献和资料1、Elasticsearch 主节点和暖热节点 https://dongbo0737.github.io/2017/06/06/elasticsearch-hot-warm/]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark中的Watermark]]></title>
    <url>%2F2020%2F01%2F27%2F2021-05-01-Spark%E4%B8%AD%E7%9A%84Watermark%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景https://towardsdatascience.com/watermarking-in-spark-structured-streaming-9e164f373e9 参考文献和资料1、]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（SpringBoot项目中配置文件总结）]]></title>
    <url>%2F2020%2F01%2F11%2F2020-10-03-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88SpringBoot%E9%A1%B9%E7%9B%AE%E4%B8%AD%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%80%BB%E7%BB%93%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景配置文件 SpringBoot使用一个全局配置文件application.properties或者application.yml(或者yaml)。 配置文件语法 即yaml文件的语法。 properties文件配置文件值的注入： 导入依赖： 123456&lt;!--导入配置文件处理器，配置文件进行绑定就会有提示--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; @Value注解前面的例子我们使用的是@ConfigurationProperties注解将本类中所有属性与配置文件中相关的配置进行绑定，除此之外还可以使用@value注解来一个个的指定属性值。 12345678910@Componentpublic class Person &#123; @Value("$&#123;person.last-name&#125;") private String lastName; @Value("#&#123;3 * 2&#125;") private Integer age; @Value("true") private Boolean isStu;//省略其他属性和get/set，toString方法&#125; 参考文献及资料1、Spring官网，链接：https://juejin.cn/post/6844904186539147271]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot系列文章（第一个SpringBoot实践练习项目）]]></title>
    <url>%2F2020%2F01%2F11%2F2020-01-01-SpringBoot%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0%EF%BC%88%E7%AC%AC%E4%B8%80%E4%B8%AASpringBoot%E5%AE%9E%E8%B7%B5%E7%BB%83%E4%B9%A0%E9%A1%B9%E7%9B%AE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开发环境准备 第二部分 使用Maven构建项目 第三部分 项目目录结构 第四部分 编写HelloWorld项目 参考文献及资料 背景第一次使用Spring Boot构建测试项目，实现一个简单的Http请求处理，通过这个例子对Spring Boot有一个初步的了解。 第一部分 开发环境准备 Maven版本：Maven 3，version 3.3.9 Java版本：Java 1.8（Java8） 第二部分 使用Maven构建项目Spring官网提供Spring Initializr工具生成项目 第一步：登录Spring Initializr网站： https://start.spring.io/ 第二步：配置项目的参数： Group ID是项目组织唯一的标识符，实际对应项目中的package包。 Artifact ID是项目的唯一的标识符，实际对应项目的project name名称，Artifact不可包含大写字母。 然后点击生成项目压缩文件，并下载到本地。 第三步：使用IDE加载项目（使用IntelliJ IDEA） 菜单中选择File–&gt;New–&gt;Project from Existing Sources... 选择解压后的项目文件夹，点击OK 点击Import project from external model并选择Maven，点击Next到底为止。 第三部分 项目目录结构开发环境是Win7环境，导入后，使用tree /f命令查看项目结构目录： 123456789101112131415161718192021222324252627282930313233# tree /f│ .gitignore│ HELP.md│ mvnw│ mvnw.cmd│ pom.xml├─.idea│ │ azureSettings.xml│ │ compiler.xml│ │ misc.xml│ │ workspace.xml│ └─inspectionProfiles│ Project_Default.xml├─.mvn│ └─wrapper│ maven-wrapper.jar│ maven-wrapper.properties│ MavenWrapperDownloader.java└─src ├─main │ ├─java │ │ └─com │ │ └─example │ │ └─demo │ │ DemoApplication.java │ └─resources │ application.properties └─test └─java └─com └─example └─demo DemoApplicationTests.java .gitignore文件是git控制文件。 mvnw(linux shell)和mvnw.cmd(windows),还有.mvn文件夹(包含Maven Wrapper Java库及其属性文件)。mvnw全名是Maven Wrapper,它的原理是在maven-wrapper.properties文件中记录你要使用的Maven版本，当用户执行mvnw clean 命令时，发现当前用户的Maven版本和期望的版本不一致，那么就下载期望的版本，然后用期望的版本来执行mvn命令，比如刚才的mvn clean。带有mvnw文件项目，只要有java环境，仅仅通过使用本项目的mvnw脚本就可以完成编译，打包，发布等一系列操作。 HELP.mdMaven的帮助文件。 pom.xmlProject Object Model 的缩写，即项目对象模型。maven 的配置文件，用以描述项目的各种信息。 .idea/文件夹来存放项目的配置信息。其中包括版本控制信息、历史记录等等。 src/main/java下的程序入口：DemoApplication.java。 src/main/resources下的配置文件：application.properties。 src/test/下的测试入口：DemoApplicationTests.java。 第四部分 编写HelloWorld项目引入Web模块，在pom.xml文件添加下面的依赖包： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 新建controller程序(HelloSpringBootController)： 123456789101112package com.example.demo.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestControllerpublic class HelloSpringBootController &#123; @RequestMapping("/") public String hello()&#123; return "Hello, SpringBoot!"; &#125;&#125; 完成后项目的结构如下： 1234567891011121314151617181920├─src│ ├─main│ │ ├─java│ │ │ └─com│ │ │ └─example│ │ │ └─demo│ │ │ │ DemoApplication.java│ │ │ ││ │ │ └─controller│ │ │ HelloSpringBootController.java│ │ ││ │ └─resources│ │ application.properties│ ││ └─test│ └─java│ └─com └─example └─demo DemoApplicationTests.java 最后使用maven编译： 12mvn clean mvn package #编译项目 会生成编译结果，在项目根目录生成target文件目录。其中demo-0.0.1-SNAPSHOT.jar为编译后的入口程序。在IDEA中，或者使用java -jar demo-0.0.1-SNAPSHOT.jar命令在win CMD命令窗口中运行。 1234567891011121314151617181920"C:\Program Files\Java\jdk-10.0.2\bin\java.exe" -Dfile.encoding=GBK -jar C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar . ____ _ __ _ _ /\\ / ___'_ __ _ _(_)_ __ __ _ \ \ \ \( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \ \\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v2.2.2.RELEASE)2020-01-12 13:56:37.333 INFO 5060 --- [ main] com.example.demo.DemoApplication : Starting DemoApplication v0.0.1-SNAPSHOT on rongxiang-PC with PID 5060 (C:\Users\rongxiang\Desktop\SpringBoot\demo\target\demo-0.0.1-SNAPSHOT.jar started by rongxiang in C:\Users\rongxiang\Desktop\SpringBoot\demo)2020-01-12 13:56:37.338 INFO 5060 --- [ main] com.example.demo.DemoApplication : No active profile set, falling back to default profiles: default2020-01-12 13:56:40.479 INFO 5060 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http)2020-01-12 13:56:40.497 INFO 5060 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2020-01-12 13:56:40.497 INFO 5060 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.29]2020-01-12 13:56:40.598 INFO 5060 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2020-01-12 13:56:40.598 INFO 5060 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 3142 ms2020-01-12 13:56:40.945 INFO 5060 --- [ main] o.s.s.concurrent.ThreadPoolTaskExecutor : Initializing ExecutorService 'applicationTaskExecutor'2020-01-12 13:56:41.227 INFO 5060 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path ''2020-01-12 13:56:41.233 INFO 5060 --- [ main] com.example.demo.DemoApplication : Started DemoApplication in 4.773 seconds (JVM running for 5.475) 在本地启了一个web服务（Tomcat started on port(s): 8080 (http)），对外服务端口为8080。 浏览器中输入url地址（http://localhost:8080/），网页显示“Hello, SpringBoot!”，说明服务服务正常。 注意： 如果运行工程，出现这个报错信息：Failed to clean project: Failed to delete 由于之前编译的工程还在运行，无法clean，导致maven生命周期无法继续进行。即由于已启动了另一个tomcat 进程，导致报错,关闭tomcat进程即可。可以在程序控制台中终止该进程即可。 参考文献及资料1、Spring官网，链接：https://spring.io/]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[外网环境访问内网（NAT）Kafka集群介绍]]></title>
    <url>%2F2019%2F09%2F07%2F2019-09-07-%E5%A4%96%E7%BD%91%E7%8E%AF%E5%A2%83%E8%AE%BF%E9%97%AE%E5%86%85%E7%BD%91%EF%BC%88NAT%EF%BC%89Kafka%E9%9B%86%E7%BE%A4%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Kafka几个配置参数介绍 第二部分 外网环境访问内网（NAT）Kafka集群配置 第三部分 总结 参考文献及资料 背景其实这是一个踩坑笔记。首先介绍踩坑背景。生产环境我们有两个网络区域，记为网络区域A（内网）、网络区域B（外网），其中为了外网环境能访问内网环境，内网对内部IP实施了IP映射（NAT），将内网IP映射为外部IP。Kafka版本为：kafka_2.11-0.10.2.0。IP清单及网络数据流图如下： hostname 内网ip 外网Ip kafka.node1 192.168.1.1 10.0.0.1 kafka.node2 192.168.1.2 10.0.0.2 kafka.node3 192.168.1.3 10.0.0.3 整个架构图为： 外网网络区域的客户端开始使用NAT地址（10.0.0.1-3）地址访问内部kafka，发现无法生产和消费kafka数据（telnet netip 9092是通的），会报解析服务器hostname失败的错误。而内部网络的客户端使用内网地址（192.168.1.1-3）是可以正常生产和消费kafka数据。 原因：advertised.listeners配置的是内网实地址，这个地址注册到Zookeeper中，当消费者和生产者访问时，Zookeeper将该地址提供给消费者和生产者。由于是内网地址，外网根本看不到这个地址（路由寻址）。所以无法获取元数据信息，通信异常。 第一部分 Kafka几个配置参数介绍首先要了解一下几个配置： host.name已弃用。 仅当listeners属性未配置时被使用，已用listeners属性代替。表示broker的hostname。 advertised.host.name已弃用。仅当advertised.listeners或者listeners属性未配置时被使用。官网建议使用advertised.listeners。该配置的意思是注册到zookeeper上的broker的hostname或ip。是提供给客户端与kafka通信使用的。如果没有设置则使用host.name。 listeners监听列表，broker对外提供服务时绑定的IP和端口。多个以逗号隔开，如果监听器名称是一个安全的协议， listener.security.protocol.map也必须设置。主机名称设置0.0.0.0绑定所有的接口，主机名称为空则绑定默认的接口。如： 1listeners = PLAINTEXT://myhost:9092,SSL://:9091 CLIENT://0.0.0.0:9092,REPLICATION://localhost:9093 如果未指定该配置，则使用java.net.InetAddress.getCanonicalHostName()函数的的返回值。 advertised.listeners客户端使用。发布至zookeeper的监听，broker会上送此地址到zookeeper，zookeeper会将此地址提供给消费者和生产者，消费者和生产者根据此地址获取消息。如果和上面的listeners不同则以此为准，在IaaS环境，此配置项可能和 broker绑定的接口主机名称不同，如果此配置项没有配置则以上面的listeners为准。 第二部分 外网环境访问内网（NAT）Kafka集群配置2.1 配置hosts方式 Kafka集群节点配置 每一台Kafka节点的hosts节点配置内部地址映射： 192.168.1.1 kafka.node1192.168.1.2 kafka.node2192.168.1.3 kafka.node3 Kafka中的配置文件（config/server.properties配置文件） advertised.listeners=PLAINTEXT://kafka.node1:9092 advertised.listeners=PLAINTEXT://kafka.node2:9092 advertised.listeners=PLAINTEXT://kafka.node3:9092 客户端节点配置 客户端的hosts文件也需要配置外部地址映射： 10.0.0.1 kafka.node110.0.0.2 kafka.node210.0.0.3 kafka.node3 应用程序使用 bootstrap.servers: [‘kafka.node1:9092’,’kafka.node2:9092’,’kafka.node3:9092’] 配置完成后，重启Kafka集群，重新使用客户端链接，测试客户端可以正常向Topic生产和消费数据。 2.2 内外部流量分离通常对于外部网络访问内网安全区域，架构使用安全套接字层 (SSL) 来保护外部客户端与 Kafka 之间的流量。而使用明文进行内部网络的broker间的通信。当 Kafka 侦听器绑定到用于内部和外部通信的网络接口时，配置侦听器就非常简单了。但在许多情况下，例如在云原生环境上部署时，集群中 Kafka broker 的外部通告地址将与 Kafka 使用的内部网络接口不同。在此情况下，可以 server.properties 中的参数 advertised.listeners 进行如下配置： 1234567891011# Configure protocol maplistener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:SSL# Use plaintext for inter-broker communicationinter.broker.listener.name=INTERNAL# Specify that Kafka listeners should bind to all local interfaceslisteners=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093# Separately, specify externally visible addressadvertised.listeners=INTERNAL://kafkabroker-n.mydomain.com:9092,EXTERNAL://kafkabroker-n.mydomain.com:9093 内部使用9092端口，而外部网络使用9093端口。 第三部分 总结Kafka集群在内外网网络环境下，需要关注地址映射。使用hosts 本地DNS进行主机名和内外地址的映射。至此爬出该坑。 参考文献及资料1、kafka - advertised.listeners和listeners，链接：https://www.cnblogs.com/fxjwind/p/6225909.html 2、Kafka从上手到实践-Kafka集群：Kafka Listeners，链接：http://www.devtalking.com/articles/kafka-practice-16/ 3、使用 Cloud Dataflow 处理来自 Kafka 的外部托管消息 链接：https://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp?hl=zh-cn 4、Kafka Listeners - Explained 链接：https://rmoff.net/2018/08/02/kafka-listeners-explained/]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Helm简介和安装部署]]></title>
    <url>%2F2019%2F08%2F08%2F2019-08-08-Helm%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Minikube集群启动 第一部分 Kubernetes中StatefulSet介绍 第三部分 部署Zookeeper集群 第四部分 部署Kafka集群 第五部分 总结 参考文献及资料 背景Helm 是 Kubernetes 的软件包管理工具(Application deployment management for Kubernetes)。 通常我们在k8s集群中部署一个可以使用的应用，一般会涉及多个组件资源的共同协作。例如需要安装部署一个web应用，包括 Deployment 用于部署应用、Service 提供服务发现、Secret 配置 应用的用户名和密码，可能还需要 pv 和 pvc 来提供持久化服务。web应用的后台数据是存储在mysql里面的，所以需要 mysql启动就绪后才能启动 web前台。涉及的资源较多还有前后项编排等管理，如果只是通过kubectl管理，这是一个复杂辛苦的工作。 helm的出现就是解决上面的痛点。主要解决的问题有： 统一管理、配置和更新应用资源文件； 分发和服用应用模板； 将应用的一系列资源整体作为一个软件包管理； 第一部分 Helm的部署Helm是由helm CLI和Tiller组成，即典型的C/S应用。helm运行与客户端，提供命令行界面，而Tiller应用运行在Kubernetes内部。 helm 是一个命令行工具，用于本地开发及管理chart，chart仓库管理等； Tiller 是 Helm 的服务端。Tiller 负责接收 Helm 的请求，与 k8s 的 apiserver 交互，根据chart 来生成一个 release 并管理 release； chart Helm的打包格式叫做chart，所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源； release 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release； Repoistory Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包； 1.1 部署Helm CLI客户端helm客户端是一个单纯的可执行文件，我们从github上直接下载压缩包（由于墙的原因可能很慢）： 1root@deeplearning:/data/helm# wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz 解压缩介质文件： 123456root@deeplearning:/data/helm# tar -zxvf helm-v2.14.3-linux-amd64.tar.gz linux-amd64/linux-amd64/helmlinux-amd64/README.mdlinux-amd64/LICENSElinux-amd64/tiller 部署： 1root@deeplearning:/data/helm# mv linux-amd64/helm /usr/local/bin 检查： 123root@deeplearning:/data/helm# helm versionClient: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;Error: could not find tiller 提示未连接到服务端tiller。下面我们部署服务端。 1.2 部署服务端Helm 的服务器端部分 Tiller 通常运行在 Kubernetes 集群内部。但是对于开发，它也可以在本地运行，并配置为与远程 Kubernetes 群集通信。 安装 tiller 到群集中最简单的方法就是运行 helm init。这将验证 helm 本地环境设置是否正确（并在必要时进行设置）。然后它会连接到 kubectl 默认连接的任何集群（kubectl config view）。一旦连接，它将安装 tiller 到 kube-system 命名空间中。 helm init 以后，可以运行 kubectl get pods --namespace kube-system 并看到 Tiller 正在运行。 你可以通过参数运行 helm init: --canary-image 参数安装金丝雀版本 --tiller-image 安装特定的镜像（版本） --kube-context 使用安装到特定群集 --tiller-namespace 用一个特定的命名空间 (namespace) 安装 --service-account 使用 Service Account 安装 RBAC enabled clusters --automount-service-account false 不适用 service account 安装 一旦安装了 Tiller，运行 helm version 会显示客户端和服务器版本。（如果它仅显示客户端版本， helm 则无法连接到服务器, 使用 kubectl 查看是否有任何 tiller Pod 正在运行。） 除非设置 --tiller-namespace 或 TILLER_NAMESPACE 参数，否则 Helm 将在命名空间 kube-system 中查找 Tiller 。 在缺省配置下， Helm 会利用 “gcr.io/kubernetes-helm/tiller“ 镜像在Kubernetes集群上安装配置 Tiller；并且利用 “https://kubernetes-charts.storage.googleapis.com“ 作为缺省的 stable repository 的地址。由于在国内可能无法访问 “gcr.io“, “storage.googleapis.com“ 等域名，阿里云容器服务为此提供了镜像站点。 首先创建服务。创建rbac-config.yaml文件，文件内容为： 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-system 通过yaml文件创建服务： 123root@deeplearning:/data/helm# kubectl create -f rbac-config.yamlserviceaccount/tiller createdclusterrolebinding.rbac.authorization.k8s.io/tiller created 启Helm pod，即安装tiller： 1234567891011121314151617root@deeplearning:/data/helm#helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v3.0.2 \ --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \ --service-account tiller # 回显Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm.Warning: Tiller is already installed in the cluster.(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.) 这时候我们再次执行： 123root@deeplearning:/data/helm# helm versionClient: &amp;version.Version&#123;SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"&#125;Server: &amp;version.Version&#123;SemVer:"v2.10.0", GitCommit:"9ad53aac42165a5fadc6c87be0dea6b115f93090", GitTreeState:"clean"&#125; 这样Helm的服务端tiller就部署完毕。 1.3 Helm CLI 命令简要汇总12345678910111213141516171819202122232425// 安装一个 Charthelm install stable/mysql// 列出 Kubernetes 中已部署的 Charthelm list --all// helm repo 的操作helm repo updatehelm repo listhelm repo add dev https://example.com/dev-charts// 创建一个 Chart，会产生一个 Chart 所需的目录结构helm create deis-workflow// 安装自定义 charthelm inspect values stable/mysql # 列出一个 chart 的可配置项helm install -f config.yaml stable/mysql # 可以将修改的配置项写到文件中通过 -f 指定并替换helm install --set name: value stable/mysql # 也可以通过 --set 方式替换// 当新版本 chart 发布时，或者当你需要更改 release 配置时，helm 必须根据现在已有的 release 进行升级helm upgrade -f panda.yaml happy-panda stable/mariadb// 删除 releasehelm delete happy-panda 参考文献及材料1、Helm User Guide - Helm 用户指南 https://whmzsu.github.io/helm-doc-zh-cn/ 2、Kubernetes 包管理工具 Helm 简介 https://www.jianshu.com/p/d55e91e28f94 3、Helm介绍 https://zhaohuabing.com/2018/04/16/using-helm-to-deploy-to-kubernetes/]]></content>
      <categories>
        <category>Helm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从Spark on Yarn到Python on Yarn]]></title>
    <url>%2F2019%2F08%2F01%2F2019-01-28-%E4%BB%8ESpark%20on%20Yarn%E5%88%B0Python%20on%20Yarn%2F</url>
    <content type="text"><![CDATA[目录 术语说明 背景 第一部分 Apache Spark运行模式介绍 第二部分 Spark on Yarn 第三部分 Pyspark Application原理 第四部分 Python on Yarn配置及运行 第五部分 总结 参考文献及资料 背景Apache Spark属于重要的大数据计算框架，另外spark还提供了Python的原生API和机器学习组件Spark Ml，使的可以通过Python编写机器学习任务由Spark运行。本篇文件从Spark运行模式开始讲起，重点介绍Spark on Yarn运行模式，最后重点介绍Python on Yarn（即Pyspark on Yarn）上运行原理和案例。 第一部分 Apache Spark运行模式目前 Apache Spark已知支持5种运行模式。按照节点资源数量可以分为单节点模式（2种）和集群模式（3种）。 单节点模式：本地模式、本地伪集群模式 集群模式：Standalone模式、Spark on Yarn模式、Spark on Mesos模式 原生云模式：在Kubernetes上运行。随着Docker容器和原生云技术的兴起，Spark开始支持在Kubernetes上运行。 对于Spark on Kubernetes可以参考官方文档：https://spark.apache.org/docs/latest/running-on-kubernetes.html。另外可以参考我的另外一篇技术总结：《在Minikube上运行Spark集群》。 1.1 本地模式（单节点模式）本地模式又称为Loacl[N]模式。该模式只需要在单节点上解压spark包即可运行，使用多个线程模拟Spark分布式计算，Master和Worker运行在同一个JVM虚拟机中。这里参数N代表可以使用（预申请）N个线程资源，每个线程拥有一个Core（默认值N=1）。 如果参数为：Loacl[*]，表明：Run Spark locally with as many worker threads as logical cores on your machine。即线程数和物理核数相同。 例如下面的启动命令： 1# ./spark-submit –class org.apache.spark.examples.JavaWordCount –master local[*] spark-examples_2.11-2.3.1.jar file:///opt/README.md 该模式不依懒于HDFS分布式文件系统。例如上面的命令使用的本地文件系统。 1.2 本地伪集群模式（单节点模式）该模式和Local[N]类似，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程在一个进程下共享资源。通常用来测试和验证应用程序逻辑上有没有问题，或者想使用Spark的计算框架而而受限于没有太多资源。 作业提交命令中使用local-cluster[x,y,z]参数模式：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数值。例如下面的命令作业申请了2个executor 进程，每个进程分配3个core和1G的内存，来运行应用程序。 1# ./spark-submit –master local-cluster[2, 3, 1024] 1.3 Standalone模式（集群模式）1.3.1 构架部署Standalone为spark自带资源管理系统（即经典的Master/Slaves架构模式）。该模式下集群由Master和Worker节点组成，程序通过与Master节点交互申请资源，Worker节点启动Executor运行。具体数据流图如下： 另外考虑到Master节点存在单点故障。Spark支持使用Zookeeper实现HA高可用（high avalible）。Zookeeper提供一种领导选举的机制，通过该机制可以保证集群中只有一个Master节点处于RecoveryState.Active状态，其他Master节点处于RecoveryState.Standby状态。 1.3.2 作业运行模式在该模式下，用户提交任务有两种方式：Standalone-client和Standalone-cluster。 1.5.1 Client模式执行流程： (1)客户端启动Driver进程。 (2)Driver向Master申请启动Application启动需要的资源。 (3)资源申请成功后，Driver将task发送到相应的Worker节点执行，并负责监控task运行情况。 (4)Worker将task执行结果返回到客户端的Driver进程。 Client模式适用于调试程序。Driver进程在客户端侧启动，如果生产采用这种模式，当业务量较大时，客户端需要启动大量Driver进程，会消耗大量系统资源，导致资源枯竭。 1.5.2 Cluster模式执行流程： (1)客户端会想Master节点申请启动Driver。 (2)Master受理客户端的请求，分配一个Work节点，启动Driver进程。 (3)Driver启动后，重新想Master节点申请运行资源，Master分配资源，并在相应的Worker节点上启动Executor进程。 (4)Driver发送task到相应的Worker节点运行，并负责监控task。 (5)Worker将task执行结果返回到Driver进程。 Driver运行有Master在集群Worker节点上随机分配，相当于在集群上负载资源。 两种方式最大的区别就是Driver进程运行的位置。Cluster模式相对于Client模式更适合于生成环境的部署。 1.4 Spark on Yarn（集群模式）目前大部分企业级Spark都是跑在已有的Hadoop集群（hadoop 2.0系统）中，均使用Yarn来作为Spark的Cluster Manager，为Spark提供资源管理服务，Spark自身完成任务调度和计算。这部分内容会在后文中细致介绍。 1.5 Spark on Mesos（集群模式）参考官方文档介绍：https://spark.apache.org/docs/latest/running-on-mesos.html 第二部分 Spark on Yarn我们知道MapReduce任务是运行在Yarn上的，同样Spark Application也可以运行在Yarn上。这种模式下，资源的管理、协调、执行和监控交给Yarn集群完成。 Yarn集群上可以运行：MapReduce任务、Spark Application、Hbase集群、Storm集群、Flink集群等等，还有我们后续重点介绍的Python on Yarn。 从节点功能上看，Yarn也采用类似Standalone模式的Master/Slave结构。资源框架中RM（ResourceManager）对应Master，NM（NodeManager）对应Slave。RM负责各个NM资源的统一管理和调度，NM节点负责启动和执行任务以及各任务间的资源隔离。 当集群中存在多种计算框架时，架构上选用Yarn统一管理资源要比Standalone更合适。类似Standalone模式，Spark on Yarn也有两种运行方式：Yarn-Client模式和Yarn-Cluster模式。从适用场景上看，Yarn-Cluster模式适用于生产环境，而Yarn-Client模式更适用于开发（交互式调试）。 2.1 Client模式在Yarn-client模式下，Driver运行在本地Client上，通过AM（ApplicationMaster）向RM申请资源。本地Driver负责与所有的executor container进行交互，并将最后的结果汇总。结束掉Client，相当于kill掉这个spark应用。 Spark Yarn Client向YARN的ResourceManager申请启动Application Master。同时在SparkContent初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend。 ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派。 Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）。 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task。 client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 2.2 Cluster模式在YARN-Cluster模式中，当用户向YARN中提交一个应用程序后，YARN将分两个阶段运行该应用程序： 第一个阶段是把Spark的Driver作为一个ApplicationMaster在YARN集群中先启动。 第二个阶段是由ApplicationMaster创建应用程序，然后为它向ResourceManager申请资源，并启动Executor来运行Task，同时监控它的整个运行过程，直到运行完成。 应用的运行结果不能在客户端显示（可以在history server中查看），所以最好将结果保存在HDFS而非stdout输出，客户端的终端显示的是作为YARN的job的简单运行状况，下图是yarn-cluster模式： 执行过程： Spark Yarn Client向YARN中提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等。 ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster，其中ApplicationMaster进行SparkContext等的初始化。 ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束。 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动CoarseGrainedExecutorBackend，而Executor对象的创建及维护是由。CoarseGrainedExecutorBackend负责的，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等。 ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 2.3 两种模式的比较在client模式下，Spark Application运行的Driver会在提交程序的节点上，而该节点可以是YARN集群内部节点，也可以不是。一般来说提交Spark Application的客户端节点不是YARN集群内部的节点，那么在客户端节点上可以根据自己的需要安装各种需要的软件和环境，以支撑Spark Application正常运行。在cluster模式下，Spark Application运行时的所有进程都在YARN集群的NodeManager节点上，而且具体在哪些NodeManager上运行是由YARN的调度策略所决定的。 对比这两种模式，最关键的是Spark Application运行时Driver所在的节点不同，而且，如果想要对Driver所在节点的运行环境进行配置，区别很大，但这对于PySpark Application运行来说是非常关键的。 第三部分 Pyspark Application原理PySpark是Spark为使用Python程序编写Spark Application而实现的客户端库，通过PySpark也可以编写Spark Application并在Spark集群上运行。Python具有非常丰富的科学计算、机器学习处理库，如numpy、pandas、scipy等等。为了能够充分利用这些高效的Python模块，很多机器学习程序都会使用Python实现，同时也希望能够在Spark集群上运行。 理解PySpark Application的运行原理，有助于我们使用Python编写Spark Application，并能够对PySpark Application进行各种调优。PySpark构建于Spark的Java API之上，数据在Python脚本里面进行处理，而在JVM中缓存和Shuffle数据，数据处理流程如下图所示: Spark Application会在Driver中创建pyspark.SparkContext对象，后续通过pyspark.SparkContext对象来构建Job DAG并提交DAG运行。使用Python编写PySpark Application，在Python编写的Driver中也有一个pyspark.SparkContext对象，该pyspark.SparkContext对象会通过Py4J模块启动一个JVM实例，创建一个JavaSparkContext对象。PY4J只用在Driver上，后续在Python程序与JavaSparkContext对象之间的通信，都会通过PY4J模块来实现，而且都是本地通信。 PySpark Application中也有RDD，对Python RDD的Transformation操作，都会被映射到Java中的PythonRDD对象上。对于远程节点上的Python RDD操作，Java PythonRDD对象会创建一个Python子进程，并基于Pipe的方式与该Python子进程通信，将用户编写Python处理代码和数据发送到Python子进程中进行处理。 第四部分 Python on Yarn配置及运行4.1 Yarn节点配置Python环境该模式需要在Yarn集群上每个NM节点（Node Manager）上部署Python编译环境，即安装Python安装包、依赖模块。用户编写的Pyspark Application由集群中Yarn调度执行。 通常使用Anaconda安装包进行统一部署，简化环境的部署。 该模式存在下面缺点： 新增依赖包部署安装代价大。如果后续用户编写的Spark Application需要依赖新的Python模块或包，那么就需要依次在集群Node Manager上部署更新依赖包。 用户对于Python环境的依赖差异化无法满足。通常不同用户编写Spark Application会依赖不同的Python环境，比如Python2、Python3环境等等。该模式下只能支持一种环境，无法满足Python多环境的需求。 各节点的Python环境需要统一。由于用户提交的Spark Application具体在哪些Node Manager上执行，由YARN调度决定，所以必须保证每个节点的Python环境（基础环境+依赖环境）都是相同的，环境维护成本高。 4.2 Yarn节点不配置Python环境该模式不需要提前在集群Node Manager上预安装Python环境。 参考文章：http://quasiben.github.io/blog/2016/4/15/conda-spark/ 我们基于华为C60集群（开源集群相同）以及Anaconda环境对该模式进行了测试验证。具体实现思路如下所示： 在一台SUSE节点上部署Anaconda，并创建虚拟Python环境（如果需要可以部署安装部分依赖包）。 创建conda虚拟环境，并整体打包为zip文件。 用户提交PySpark Application时，使用--archives参数指定该zip文件路径。 详细操作步骤如下： 第一步 下载Anaconda3-4.2.0-Linux-x86_64.sh安装软件（基于python3.5），在SUSE服务器上部署安装。Anaconda的安装路径为/usr/anaconda3。查看客户端服务器的python环境清单： 1234dkfzxwma07app08:/usr/anaconda3 # conda env list# conda environments:#root * /usr/anaconda3 其中root环境为目前的主环境。为了便于环境版本管理我们新建一个专用环境（mlpy_env）。 1dkfzxwma07app08:/usr/anaconda3/envs # conda create -n mlpy --clone root 上述命令创建了一个名称为mlpy_env的Python环境，clone选项将对应的软件包都安装到该环境中，包括一些C的动态链接库文件。 接着，将该Python环境打包，执行如下命令： 12dkfzxwma07app08:/usr/anaconda3/envs # cd /root/anaconda2/envsdkfzxwma07app08:/usr/anaconda3/envs # zip -r mlpy_env.zip mlpy_env 将该zip压缩包拷贝到指定目录中（或者后续引用使用绝对路径），方便后续提交PySpark Application： 1dkfzxwma07app08:/usr/anaconda3/envs # cp mlpy_env.zip /tmp/ 最后，我们可以提交我们的PySpark Application，执行如下命令（或打包成shell脚本）： 123456PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python spark-submit \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \--master yarn-cluster \--archives /tmp/mlpy_env.zip#ANACONDA \/var/lib/hadoop-hdfs/pyspark/test_pyspark_dependencies.py 注意：下面命令指的是zip包将在ANACONDA的目录中展开，需要注意路径。 12&gt; --archives /tmp/mlpy_env.zip#ANACONDA&gt; 环境打包需要注意压缩路径。 上面的依赖zip压缩包将整个Python的运行环境都包含在里面，在提交PySpark Application时会将该环境zip包上传到运行Application的所在的每个节点上。解压缩后为Python代码提供运行时环境。如果不想每次都从客户端将该环境文件上传到集群中运行节点上，也可以提前将zip包上传到HDFS文件系统中，并修改–archives参数的值为hdfs:///tmp/mlpy_env.zip #ANACONDA（注意环境差异），也是可以的。 另外，需要说明的是，如果我们开发的/var/lib/hadoop-hdfs/pyspark /test_pyspark_dependencies.py文件中，依赖多个其他Python文件，想要通过上面的方式运行，必须将这些依赖的Python文件拷贝到我们创建的环境中，对应的目录为mlpy_env/lib/python2.7/site-packages/下面。 注意：pyspark不支持python3.6版本，所以python环境使用python3.5 否则程序执行回显会有这样的报错信息： TypeError: namedtuple() missing 3 required keyword-only arguments: ‘verbose’, ‘rename’, and ‘module’ https://issues.apache.org/jira/browse/SPARK-19019?page=com.atlassian.jira.plugin.system.issuetabpanels%3Aall-tabpanel 4.3 一个机器学习任务栗子举一个Kmeans无监督算法的Python案例： 123456789101112131415161718192021222324252627282930313233343536import osfrom pyspark import SparkContextfrom pyspark.mllib.clustering import KMeans, KMeansModelfrom numpy import arrayfrom math import sqrt# 创建spark contextsc = SparkContext(appName="kmeans")# 加载和解析数据文件stg_path = "hdfs://hacluster" + "/user/" + str(os.environ['USER']) + "/.sparkStaging/" + str(sc.applicationId) + "/" data = sc.textFile(os.path.join(stg_path,'kmeans_data.txt')) parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))# 创建模型clusters = KMeans.train(parsedData, 2, maxIterations=10,runs=10,initializationMode="random")# 模型训练def error(point): i = clusters.predict(point) center = clusters.centers[i] print("(" + str(point[0]) + "," + str(point[1]) + "," + str(point[2]) + ")" + "blongs to cluster " + str(i+1)) # print("Cluster Number:" + str(len(clusters.centers))) return sqrt(sum([x**2 for x in (point - center)]))WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)print("Within Set Sum of Squared Error = " + str(WSSSE))# 打印类心for mCenter in clusters.centers: print(mCenter)# 保存模型myModelPath = "hdfs://hacluster"+"/user/model/"+"KMeansModel.ml"clusters.save(sc, myModelPath)# 加载模型并测试loadModel = KMeansModel.load(sc, myModelPath)print(loadModel.predict(array([1,1,1]))) 整理成下面的提交命令，将作业提交到Yarn集群： 123456789dkfzxwma07app08:/tmp/pyspark # cat run.shPYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \/approot1/utility/hadoopclient/Spark/spark/bin/spark-submit \--master yarn \--deploy-mode cluster \--archives /tmp/pyspark/mlpy_env.zip#ANACONDA \--files /tmp/pyspark/kmeans_data.txt \--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./ANACONDA/mlpy_env/bin/python \/tmp/pyspark/kmeanTest.py 模型训练结果会写到路径下面：/user/model： 123dkfzxwma07app08:/tmp/pyspark/pythonpkg # hdfs dfs -ls /user/modelFound 1 itemsdrwxr-xr-x+ - itdw hadoop 0 2019-07-30 11:30 /user/model/KMeansModel.ml 模型加载的预测结果可以在Yarn日志中查询： 12345678910dkfzxwma07app08:/tmp/pyspark # yarn logs -applicationId application_1562162322775_150207# 提取部分回显LogType:stdoutLog Upload Time:星期二 七月 30 13:47:23 +0800 2019LogLength:120414Log Contents:Within Set Sum of Squared Error = 0.6928203230275529[ 9.1 9.1 9.1][ 0.1 0.1 0.1]1 当然对于输出可以选择其他输出源(表或者文件)。 第五部分 总结5.1 混合多语言数据流通常一个完整的机器学习应用的数据流设计中，可以将数据ETL准备阶段和算法计算分离出来。使用Java/scala/sql进行数据的预处理，输出算法计算要求的数据格式。这会极大降低算法计算的数据输入规模，降低算法计算的节点的IO。 机器学习的算法计算部分具有高迭代计算特性，对于非分布式的机器学习算法，我们通常部署在高性能的节点上，基于丰富、高性能的Python科学计算模块，使用Python语言实现。而对于数据准备阶段，更适合使用原生的Scala/java编程语言实现Spark Application来处理数据，包括转换、统计、压缩等等，将满足算法输入格式的数据输出到HDFS文件系统中。特别对于数据规模较大的情况，在Spark集群上处理数据，Scala/Java实现的Spark Application运行（多机并行分布式处理）性能要好一些。然后输出数据交给Python进行迭代计算训练。 当然对于分布式机器学习框架，将数据迭代部分分解到多个节点并行处理，由参数服务器管理迭代参数的汇总和更新。在这种计算框架下可以利用数据集群天然的计算资源，实现分布式部署。这就形成了一个高效的混合的多语言的数据处理流。 5.2 架构建议和总结1、对于Python on Yarn架构下，采用“Yarn节点不配置Python环境”模式，便于Python环境的管理。这时候可以将Python环境zip文件上传至集群HDFS文件系统，避免每次提交任务都需要上传zip文件，但是不可避免集群内部HDFS文件系统分发到运行节点产生的网络IO。但比集群外部的上传效率高。 2、对于机器学习任务数据流建议采用混合多语言数据流方式，发挥各计算组件的优势。 3、对于分布式机器学习框架，建议结合集群的计算资源，直接在集群上展开分布式计算（例如Tensorflow计算框架）。而不是单独新建新的机器学习分布式集群。减少两个集群的数据搬运，并且使得数据和计算更加贴近，最重要的提高机器学习任务端到端的效率。 参考文献及资料1、Running Spark Python Applications，链接：https://www.cloudera.com/documentation/enterprise/5-9-x/topics/spark_python.html 2、基于YARN集群构建运行PySpark Application，链接： http://shiyanjun.cn/archives/1738.html 3、Running Spark on YARN，链接： https://spark.apache.org/docs/latest/running-on-yarn.html 4、Running Spark on Kubernetes，链接：https://spark.apache.org/docs/latest/running-on-kubernetes.html 5、Apache Spark Resource Management and YARN App Models，链接：https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/ 6、Spark On Yarn的两种模式yarn-cluster和yarn-client深度剖析，链接：https://www.cnblogs.com/ITtangtang/p/7967386.html 7、Introducing Skein: Deploy Python on Apache YARN the Easy Way，链接：https://jcrist.github.io/introducing-skein.html 8、当Spark遇上TensorFlow分布式深度学习框架原理和实践，链接：https://juejin.im/post/5ad4b620f265da23a04a0ad0 9、Spark On Yarn的优势，链接：https://www.cnblogs.com/ITtangtang/p/7967386.html 10、基于YARN集群构建运行PySpark Application，链接：http://www.uml.org.cn/bigdata/201711132.asp]]></content>
      <categories>
        <category>pyspark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[TDengine时间序列数据库压力测试]]></title>
    <url>%2F2019%2F07%2F01%2F2019-07-01-TDengine%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 部署压测工具vegeta 第二部分 部署TDengine数据库 第三部分 TDengine数据库RESTful接口介绍 第四部分 压力测试 第五部分 压力测试结果 参考文献及资料 背景最近涛思数据团队开源了自己的时序数据库，关注度较高。按照官方介绍，性能较为强悍，所以使用压测工具对该数据库进行了性能压测。压测使用的工具Vegeta 是一个用 Go 语言编写的多功能的 HTTP 负载测试工具。Vegeta 提供了命令行工具和一个开发库。 第一部分 部署压测工具vegeta1.1 部署从github上下载vegeta安装介质： 1$ wget https://github.com/tsenart/vegeta/releases/download/cli%2Fv12.5.1/vegeta-12.5.1-linux-amd64.tar.gz 该工具开箱即用，解压tar包： 1$ tar -zxvf vegeta-12.5.1-linux-amd64.tar.gz 1.2 简单测试使用我们使用vegeta测试一下下面的压力场景（对百度主页发起每秒100次（-rate=100）的请求，持续10秒（-duration=10s）），测试结果重定向到文件（result/results.bin）： 1$ echo "GET https://www.baidu.com" |./vegeta attack -duration=10s -rate=100 &gt;result/results.bin 查看一下测试结果： 123456789root@deeplearning:/data/vegeta# ./vegeta report result/results.binRequests [total, rate] 1000, 100.10Duration [total, attack, wait] 10.002241136s, 9.990128173s, 12.112963msLatencies [mean, 50, 95, 99, max] 2.315958984s, 1.889487082s, 6.064678156s, 6.583899297s, 6.961230585sBytes In [total, mean] 227000, 227.00Bytes Out [total, mean] 0, 0.00Success [ratio] 100.00%Status Codes [code:count] 200:1000 Error Set: 另外可以生成html文件报告（可视化）： 1root@deeplearning:/data/vegeta# ./vegeta plot result/results.bin &gt; result/plot.html 第二部分 部署TDengine数据库2.1 制作docker镜像 由于墙的原因我们在VPS上打包镜像，然后本机拉取部署。 为了保证测试环境的隔离性，我们制作docker镜像，使用docker环境进行测试。首先拉取ubuntu的基础镜像： 1$ docker run -t -i ubuntu:16.04 /bin/bash 通过TDengine源码安装。在这过程有大量操作系统工具未安装，需要使用atp-get安装部署。 2.1.1 第一步 clone项目1$ git clone https://github.com/taosdata/TDengine.git 2.1.2 第二步 编译12$ mkdir build &amp;&amp; cd build$ cmake .. &amp;&amp; cmake --build . 2.1.3 第三步 安装1$ make install 2.2 生成镜像打包成镜像，推送到Docker Hub： 123root@vultr:~# docker commit a35c43242a8e rongxiang1986/tdengineroot@vultr:~# docker tag rongxiang1986/tdengine rongxiang1986/tdengine:1.0root@vultr:~# docker push rongxiang1986/tdengine:1.0 2.3 拉取镜像部署拉取镜像： 1root@deeplearning:/data/TDengine/TDengine# docker pull rongxiang1986/tdengine:1.0 启动一个docker容器： 1root@deeplearning:/data/TDengine/TDengine# docker run -t -i --name tdengine -d -p 6020:6020 rongxiang1986/tdengine:1.0 /bin/bash 查看正在运行的容器： 12root@deeplearning:/data/TDengine/TDengine# docker ps# bec2e166c29f 进入容器： 1root@deeplearning:/data/TDengine/TDengine# docker attach bec2e166c29f 最后启动数据库服务，下面是启动回显信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107root@bec2e166c29f:~/TDengine/TDengine/build# ./build/bin/taosd -c test/cfg &amp;[1] 23root@bec2e166c29f:~/TDengine/TDengine/build# TDengine:[23]: Starting TDengine service...07/17 14:48:21.615248 7ff90254e700 UTL timezone not configured, set to system default:Etc/UTC (Etc, +0000)07/17 14:48:21.615261 7ff90254e700 UTL locale not configured, set to system default:C07/17 14:48:21.616266 7ff90254e700 UTL taos config &amp; system info:07/17 14:48:21.616274 7ff90254e700 UTL ==================================07/17 14:48:21.616278 7ff90254e700 UTL internalIp: 172.17.0.3 07/17 14:48:21.616283 7ff90254e700 UTL localIp: 172.17.0.3 07/17 14:48:21.616288 7ff90254e700 UTL httpIp: 0.0.0.0 07/17 14:48:21.616294 7ff90254e700 UTL httpPort: 6020 07/17 14:48:21.616299 7ff90254e700 UTL mgmtShellPort: 6030 07/17 14:48:21.616304 7ff90254e700 UTL vnodeShellPort: 6035 07/17 14:48:21.616308 7ff90254e700 UTL configDir: test/cfg 07/17 14:48:21.616312 7ff90254e700 UTL dataDir: /root/TDengine/TDengine/build/test/data 07/17 14:48:21.616318 7ff90254e700 UTL logDir: /root/TDengine/TDengine/build/test/log 07/17 14:48:21.616325 7ff90254e700 UTL scriptDir: /etc/taos 07/17 14:48:21.616330 7ff90254e700 UTL numOfThreadsPerCore: 1.000000 07/17 14:48:21.616338 7ff90254e700 UTL ratioOfQueryThreads: 0.500000 07/17 14:48:21.616344 7ff90254e700 UTL numOfVnodesPerCore: 8 07/17 14:48:21.616349 7ff90254e700 UTL numOfTotalVnodes: 0 07/17 14:48:21.616355 7ff90254e700 UTL tables: 1000 07/17 14:48:21.616360 7ff90254e700 UTL cache: 16384(byte)07/17 14:48:21.616366 7ff90254e700 UTL rows: 4096 07/17 14:48:21.616371 7ff90254e700 UTL fileBlockMinPercent: 0.250000 07/17 14:48:21.616376 7ff90254e700 UTL ablocks: 4 07/17 14:48:21.616382 7ff90254e700 UTL tblocks: 100 07/17 14:48:21.616386 7ff90254e700 UTL monitorInterval: 30(s)07/17 14:48:21.616391 7ff90254e700 UTL rpcTimer: 300(ms)07/17 14:48:21.616396 7ff90254e700 UTL rpcMaxTime: 600(s)07/17 14:48:21.616402 7ff90254e700 UTL ctime: 3600(s)07/17 14:48:21.616407 7ff90254e700 UTL statusInterval: 1(s)07/17 14:48:21.616414 7ff90254e700 UTL shellActivityTimer: 3(s)07/17 14:48:21.616420 7ff90254e700 UTL meterMetaKeepTimer: 7200(s)07/17 14:48:21.616423 7ff90254e700 UTL metricMetaKeepTimer: 600(s)07/17 14:48:21.616428 7ff90254e700 UTL maxUsers: 1000 07/17 14:48:21.616432 7ff90254e700 UTL maxDbs: 1000 07/17 14:48:21.616439 7ff90254e700 UTL maxTables: 650000 07/17 14:48:21.616442 7ff90254e700 UTL maxVGroups: 1000 07/17 14:48:21.616445 7ff90254e700 UTL minSlidingTime: 10(ms)07/17 14:48:21.616454 7ff90254e700 UTL minIntervalTime: 10(ms)07/17 14:48:21.616460 7ff90254e700 UTL maxStreamCompDelay: 20000(ms)07/17 14:48:21.616463 7ff90254e700 UTL maxFirstStreamCompDelay:10000(ms)07/17 14:48:21.616467 7ff90254e700 UTL retryStreamCompDelay: 10(ms)07/17 14:48:21.616470 7ff90254e700 UTL clog: 1 07/17 14:48:21.616474 7ff90254e700 UTL comp: 2 07/17 14:48:21.616480 7ff90254e700 UTL days: 10 07/17 14:48:21.616483 7ff90254e700 UTL keep: 3650 07/17 14:48:21.616488 7ff90254e700 UTL defaultDB: 07/17 14:48:21.616494 7ff90254e700 UTL defaultUser: root 07/17 14:48:21.616503 7ff90254e700 UTL defaultPass: taosdata 07/17 14:48:21.616508 7ff90254e700 UTL timezone: Etc/UTC (Etc, +0000) 07/17 14:48:21.616515 7ff90254e700 UTL locale: C 07/17 14:48:21.616520 7ff90254e700 UTL charset: UTF-8 07/17 14:48:21.616525 7ff90254e700 UTL maxShellConns: 2000 07/17 14:48:21.616531 7ff90254e700 UTL maxMeterConnections: 10000 07/17 14:48:21.616535 7ff90254e700 UTL maxMgmtConnections: 2000 07/17 14:48:21.616540 7ff90254e700 UTL maxVnodeConnections: 10000 07/17 14:48:21.616543 7ff90254e700 UTL enableHttp: 1 07/17 14:48:21.616549 7ff90254e700 UTL enableMonitor: 1 07/17 14:48:21.616553 7ff90254e700 UTL httpCacheSessions: 2000 07/17 14:48:21.616556 7ff90254e700 UTL httpMaxThreads: 2 07/17 14:48:21.616561 7ff90254e700 UTL numOfLogLines: 10000000 07/17 14:48:21.616565 7ff90254e700 UTL asyncLog: 1 07/17 14:48:21.616570 7ff90254e700 UTL debugFlag: 131 07/17 14:48:21.616574 7ff90254e700 UTL mDebugFlag: 135 07/17 14:48:21.616579 7ff90254e700 UTL dDebugFlag: 131 07/17 14:48:21.616584 7ff90254e700 UTL sdbDebugFlag: 135 07/17 14:48:21.616590 7ff90254e700 UTL taosDebugFlag: 131 07/17 14:48:21.616595 7ff90254e700 UTL tmrDebugFlag: 131 07/17 14:48:21.616601 7ff90254e700 UTL cDebugFlag: 131 07/17 14:48:21.616605 7ff90254e700 UTL jniDebugFlag: 131 07/17 14:48:21.616612 7ff90254e700 UTL odbcDebugFlag: 131 07/17 14:48:21.616617 7ff90254e700 UTL uDebugFlag: 131 07/17 14:48:21.616621 7ff90254e700 UTL httpDebugFlag: 131 07/17 14:48:21.616629 7ff90254e700 UTL monitorDebugFlag: 131 07/17 14:48:21.616634 7ff90254e700 UTL qDebugFlag: 131 07/17 14:48:21.616637 7ff90254e700 UTL gitinfo: 82cbce3261d06ab37c3bd4786c7b2e3d2316c42a 07/17 14:48:21.616643 7ff90254e700 UTL buildinfo: Built by ubuntu at 2019-07-05 18:42 07/17 14:48:21.616648 7ff90254e700 UTL version: 1.6.0.0 07/17 14:48:21.616653 7ff90254e700 UTL os pageSize: 4096(KB)07/17 14:48:21.616658 7ff90254e700 UTL os openMax: 104857607/17 14:48:21.616662 7ff90254e700 UTL os streamMax: 1607/17 14:48:21.616666 7ff90254e700 UTL os numOfCores: 807/17 14:48:21.616670 7ff90254e700 UTL os totalDisk: 426(GB)07/17 14:48:21.616676 7ff90254e700 UTL os totalMemory: 32028(MB)07/17 14:48:21.616682 7ff90254e700 UTL os sysname: Linux07/17 14:48:21.616686 7ff90254e700 UTL os nodename: bec2e166c29f07/17 14:48:21.616690 7ff90254e700 UTL os release: 4.15.0-51-generic07/17 14:48:21.616694 7ff90254e700 UTL os version: #55~16.04.1-Ubuntu SMP Thu May 16 09:24:37 UTC 201907/17 14:48:21.616700 7ff90254e700 UTL os machine: x86_6407/17 14:48:21.616707 7ff90254e700 UTL ==================================07/17 14:48:21.616712 7ff90254e700 DND Server IP address is:172.17.0.307/17 14:48:21.616717 7ff90254e700 DND starting to initialize TDengine engine ...07/17 14:48:21.619440 7ff90254e700 HTP failed to open telegraf schema config file:test/cfg/taos.telegraf.cfg, use default schema07/17 14:48:21.869736 7ff90254e700 DND vnode is initialized successfully07/17 14:48:21.869780 7ff90254e700 MND starting to initialize TDengine mgmt ...07/17 14:48:21.872494 7ff90254e700 MND first access, set total vnodes:6407/17 14:48:21.917758 7ff90254e700 MND TDengine mgmt is initialized successfully07/17 14:48:21.917781 7ff90254e700 HTP starting to initialize http service ...07/17 14:48:21.918417 7ff90254e700 DND TDengine is initialized successfully07/17 14:48:21.918533 7ff8e4f41700 HTP http service init success at ip:0.0.0.0:6020TDengine:[23]: Started TDengine service successfully.07/17 14:48:22.022278 7ff8ff835700 MON starting to initialize monitor service ..07/17 14:48:22.022747 7ff8eb109700 MND user:monitor login from 172.17.0.3, code:007/17 14:48:22.024412 7ff901038700 MON dnode:172.17.0.3 is started07/17 14:48:22.026780 7ff901038700 MON monitor service init success 上面回显service init success说明服务service启动成功。 2.4 镜像使用说明TDengine项目地址：https://github.com/taosdata/TDengine。使用ubuntu16.04作为基础镜像，部署安装TDengine。拉取镜像后，启动容器后： 启动服务：To start the TDengine server, run the command below in terminal: 1$ /root/TDengine/TDengine/build# ./build/bin/taosd -c test/cfg 启动客户端：In another terminal, use the TDengine shell to connect the server: 1$ /root/TDengine/TDengine/build#./build/bin/taos -c test/cfg 第三部分 TDengine数据库RESTful接口介绍按照官方给的例子我们新建案例数据库和表，并且新增数据记录。首先进入shell交互： 1234567root@a35c43242a8e:~/TDengine/TDengine/build# ./build/bin/taos -c test/cfg07/17 04:30:19.818000 7fa154b0e700 MND user:root login from 172.17.0.2, code:0Welcome to the TDengine shell, server version:1.6.0.0 client version:1.6.0.0Copyright (c) 2017 by TAOS Data, Inc. All rights reserved.taos&gt; 创建案例数据： 12345678910111213141516171819202122taos&gt; create database db;07/17 04:31:46.970580 7fa14ffff700 MND DB:0.db is created by rootQuery OK, 1 row(s) affected (0.001848s)taos&gt; use db;Database changed.taos&gt; create table t (ts timestamp, cdata int);Query OK, 1 row(s) affected (0.334998s)taos&gt; insert into t values ('2019-07-15 10:00:00', 10);Query OK, 1 row(s) affected (0.001639s)taos&gt; insert into t values ('2019-07-15 10:01:05', 20);Query OK, 1 row(s) affected (0.000245s)taos&gt; select * from t; ts | cdata |=================================== 19-07-15 10:00:00.000| 10| 19-07-15 10:01:05.000| 20|Query OK, 2 row(s) in set (0.001408s) 退出shell交互后，我们使用Restfull接口与数据库交互。 注意：目前RESTfull接口认证方式使用Http Basic Authorization请求格式，token使用base64(username:password)，即base64(root:taosdata)=cm9vdDp0YW9zZGF0YQ== 可以在在线网站上：https://www.base64encode.org/ encode一下。 1234root@a35c43242a8e:~/TDengine/TDengine/build# curl -H 'Authorization: Basic cm9vdDp0YW9zZGF0YQ==' -d 'select * from db.t' localhost:6020/rest/sql#下面是回显：07/17 04:33:34.834283 7fa154b0e700 MND user:root login from 172.17.0.2, code:0&#123;"status":"succ","head":["ts","cdata"],"data":[["2019-07-15 10:00:00.000",10],["2019-07-15 10:01:05.000",20]],"rows":2&#125; 返回结果是一个JSON格式串，规范化一下： 123456789101112131415161718&#123; "status":"succ", "head":[ "ts", "cdata" ], "data":[ [ "2019-07-15 10:00:00.000", 10 ], [ "2019-07-15 10:01:05.000", 20 ] ], "rows":2&#125; 第四部分 对RESTful接口压力测试最后我们使用vegeta对restful接口进行压力测试： 4.1 查询压测使用目标文件的内容进行压力测试。 首先创建target.txt文件以及数据文件data.json，内容如下： 1234root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat target.txtPOST http://localhost:6020/rest/sqlAuthorization: Basic cm9vdDp0YW9zZGF0YQ==@data.json 12root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat data.jsonselect * from db.t 最后使用下面的命令开始压测（每秒15000次请求，持续5分钟）： 123456789101112root@bec2e166c29f:~/TDengine/vegeta/TDengineTest#../vegeta attack -rate 15000 -targets target.txt -duration 5m &gt; out.dat# 生成报告root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out.dat# 回显：Requests [total, rate] 4500044, 15000.15Duration [total, attack, wait] 5m0.000202809s, 4m59.999911126s, 291.683µsLatencies [mean, 50, 95, 99, max] 314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168msBytes In [total, mean] 954009328, 212.00Bytes Out [total, mean] 90000880, 20.00Success [ratio] 100.00%Status Codes [code:count] 200:4500044 Error Set: 4.2 写入压测首先创建writetarget.txt和数据文件：writedata.json，内容如下： 1234root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writetarget.txtPOST http://localhost:6020/rest/sqlAuthorization: Basic cm9vdDp0YW9zZGF0YQ==@writedata.json 12root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# cat writedata.jsoninsert into db.cpu values (NOW,20,12); 这里写入语句使用时间函数NOW，保证写入时间无重复。 最后使用命令开始压测（每秒30000次请求，持续1分钟）： 1234567891011root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta attack -rate 30000 -targets writetarget.txt -duration 1m &gt; writeout.dat# 生成报告root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.datRequests [total, rate] 1800018, 30000.31Duration [total, attack, wait] 1m0.000154842s, 59.99998565s, 169.192µsLatencies [mean, 50, 95, 99, max] 254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831msBytes In [total, mean] 115201152, 64.00Bytes Out [total, mean] 70200702, 39.00Success [ratio] 100.00%Status Codes [code:count] 200:1800018 Error Set: 第五部分 压力测试结果由于机器环境的差异，只是做了尝试性测试，不代表产品的实际性能。 官方测试报告参考：https://www.taosdata.com/downloads/TDengine_Testing_Report_cn.pdf 5.1 查询对于查询性能我们每秒15000的请求结果如下： 123456789root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out15000.datRequests [total, rate] 4500044, 15000.15Duration [total, attack, wait] 5m0.000202809s, 4m59.999911126s, 291.683µsLatencies [mean, 50, 95, 99, max] 314.712µs, 230.113µs, 869.502µs, 1.501018ms, 205.870168msBytes In [total, mean] 954009328, 212.00Bytes Out [total, mean] 90000880, 20.00Success [ratio] 100.00%Status Codes [code:count] 200:4500044 Error Set: 当提高到每秒20000次时，数据库出现响应失败，成功率只有38.78%： 123456789101112131415161718192021root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report out20000.dat Requests [total, rate] 904794, 2824.86Duration [total, attack, wait] 6m19.120695351s, 5m20.296894938s, 58.823800413sLatencies [mean, 50, 95, 99, max] 38.726984614s, 43.82040722s, 1m13.377381823s, 1m25.843611324s, 1m52.393219406sBytes In [total, mean] 58181450, 64.30Bytes Out [total, mean] 8016960, 8.86Success [ratio] 38.78%Status Codes [code:count] 0:503946 200:350896 400:49952 Error Set:400 Bad RequestPost http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in usePost http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: read tcp 127.0.0.1:34037-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:53020-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:47081-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:35442-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:58175-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:41857-&gt;127.0.0.1:6020: read: connection reset by peerPost http://localhost:6020/rest/sql: read tcp 127.0.0.1:58943-&gt;127.0.0.1:6020: read: connection reset by peer 5.2 写入对于写入测试。对于查询性能我们每秒30000的请求结果如下： 123456789root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout30000.datRequests [total, rate] 1800018, 30000.31Duration [total, attack, wait] 1m0.000154842s, 59.99998565s, 169.192µsLatencies [mean, 50, 95, 99, max] 254.824µs, 137.651µs, 904.696µs, 1.687601ms, 12.528831msBytes In [total, mean] 115201152, 64.00Bytes Out [total, mean] 70200702, 39.00Success [ratio] 100.00%Status Codes [code:count] 200:1800018 Error Set: 当提高到每秒50000次时，数据库性能开始恶化，成功率只有55.73%： 12345678910111213root@bec2e166c29f:~/TDengine/vegeta/TDengineTest# ../vegeta report writeout50000.datRequests [total, rate] 160712, 2676.70Duration [total, attack, wait] 1m47.789030336s, 1m0.041100866s, 47.74792947sLatencies [mean, 50, 95, 99, max] 36.064568863s, 39.103737526s, 50.372069878s, 54.866214748s, 1m12.470342472sBytes In [total, mean] 5731840, 35.67Bytes Out [total, mean] 3492840, 21.73Success [ratio] 55.73%Status Codes [code:count] 0:71152 200:89560 Error Set:Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in usePost http://localhost:6020/rest/sql: net/http: request canceled (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)Post http://localhost:6020/rest/sql: dial tcp 0.0.0.0:0-&gt;127.0.0.1:6020: bind: address already in use (Client.Timeout exceeded while awaiting headers) 结论：单机环境下同等场景（都是压测RESTfull接口）下，TDengine可以抗住每秒15000次的读请求和每秒30000次写请求。influxdb只能抗住每秒6000次持续读、写。按照官网介绍如果写入是批量形式会更快。 参考文献和材料1、推荐一款高性能 HTTP 负载测试工具 Vegeta 链接：https://www.hi-linux.com/posts/4650.html 2、TDengine官网 链接：https://www.taosdata.com/cn/ 3、比Hadoop快至少10倍的物联网大数据平台，我把它开源了 链接：https://weibo.com/ttarticle/p/show?id=2309404394278649462890]]></content>
      <categories>
        <category>TDengine Vegeta</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sqlflow初体验]]></title>
    <url>%2F2019%2F05%2F06%2F2019-05-06-Sqlflow%E5%88%9D%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Sqlflow安装部署 第二部分 机器学习例子 第三部分 系统架构 参考文献及资料 背景2019年5 月 6 日，在QCon 全球软件开发大会（北京站）上，蚂蚁金服副 CTO 胡喜正式宣布开源机器学习工具 SQLFlow。实际上3个月前sqkflow项目已经在github上开源了。 本篇文件主要参考sqlflow官网的案例和说明对sqlflow进行了体验，并记录下来。 sqlflow按照官网的定义，将SQL引擎（例如MySQL，Hive，SparkSQL或SQL Server）和tensorflow和其他机器学习的桥梁。扩展了SQL语言，支持对机器学习模型训练、预测和推理。 目前开源版本仅支持MySQL和TensorFlow 介绍文档中也提到，在sqlflow之前也有SQL引擎提供了支持机器学习功能的扩展。 Microsoft SQL Server：Microsoft SQL Server具有机器学习服务，可以将R或Python中的机器学习程序作为外部脚本运行。 Teradata SQL for DL：Teradata还提供RESTful服务，可以从扩展的SQL SELECT语法中调用。 Google BigQuery：Google BigQuery通过引入CREATE MODEL语句在SQL中实现机器学习。 第一部分 Sqlflow安装部署1.1 部署mysql做为数据源（1）构建镜像官网提供了一个dockerfile，可以git clone整个项目。 https://github.com/sql-machine-learning/sqlflow/tree/7c873780bd8a3a9ea4d39ed7d0fcf154b2f8821f/example/datasets 1234# 进入Dockerfile文件所在目录cd example/datasets# 使用Dockerfile构建镜像docker build -t sqlflow:data . 可以查看创建了一个docker images： 12docker images# 创建了REPOSITORY：sqlflow镜像，TAG为：data （2）启动mysql容器用镜像启mysql容器： 12345docker run --rm -d --name sqlflowdata \ -p 3306:3306 \ -e MYSQL_ROOT_PASSWORD=root \ -e MYSQL_ROOT_HOST=% \ sqlflow:data 使用镜像：sqlflow:data，启动一个名为：sqlflowdata的容器，并且把3306端口映射到宿主机。mysql的root用户的密码为root。 （3）生成测试数据进入容器： 1docker exec -it sqlflowdata bash 执行SQL语句： 12345# 建库建表，注意宿主机目录：datasetscat /popularize_churn.sql | mysql -uroot -prootcat /popularize_iris.sql | mysql -uroot -proot# 建库echo "CREATE DATABASE IF NOT EXISTS sqlflow_models;" | mysql -uroot -proot 至此完成mysql容器的启动和测试数据的生成。按Ctrl+P+Q，正常退出不关闭容器。 1.2 使用docker部署slqflow（1）拉取镜像并启动容器首先从docker Hub上拉取镜像： 1# docker pull sqlflow/sqlflow:latest 启动容器： 123# docker run --rm -it --name sqlflowServer -p 8888:8888 sqlflow/sqlflow:latest \bash -c "sqlflowserver --datasource='mysql://root:root@tcp(192.168.31.3:3306)/?maxAllowedPacket=0' &amp;SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root" 命令使用镜像：sqlflow/sqlflow:lates，启动了名为：sqlflowServer的容器。将8888端口映射到宿主机上。这里需要配置datasource，指向mysql使用套接字：192.168.31.3:3306。这里使用之前构建的mysql容器的连接信息，可以根据实际情况配置。 如果mysql套接字配置错误，报错信息：connect: connection refused 如果没有报错： 12345672019/05/06 14:47:30 Server Started at :50051[I 14:47:30.261 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret[I 14:47:30.874 NotebookApp] Serving notebooks from local directory: /[I 14:47:30.874 NotebookApp] The Jupyter Notebook is running at:[I 14:47:30.874 NotebookApp] http://(fd2b9b3f994b or 127.0.0.1):8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863[I 14:47:30.874 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[W 14:47:30.877 NotebookApp] No web browser found: could not locate runnable browser. 这里启动了Jupyter Notebook服务，对外服务端口为8888，并且映射到宿主机。例如这里可以使用下面的url范围web界面：http://192.168.31.3:8888/?token=265b3fc832b5b48689fa9f88483125dc9335188dd7c1d863 （2）简单测试Jupyter Notebook 新建一个python3交互环境。测试一下： 12%%sqlflowselect * from iris.train limit 5; 第二部分 机器学习例子使用iris数据集体验机器学习的例子，使用Jupyter Notebook 完成： （1）训练模型：12%%sqlflowSELECT * FROM iris.train TRAIN DNNClassifier WITH n_classes = 3, hidden_units = [10, 20] COLUMN sepal_length, sepal_width, petal_length, petal_width LABEL class INTO sqlflow_models.my_dnn_model; 使用iris.train表中的数据训练神经网络。 模型训练结果输入到sqlflow_models.my_dnn_model，回显训练正确率为：0.97273 12Training set accuracy: 0.97273Done training （2）模型应用使用训练结果对数据进行预测应用： 12%%sqlflowSELECT * FROM iris.test PREDICT iris.predict.class USING sqlflow_models.my_dnn_model; 使用iris.test中的数据喂给训练好的模型，预测结果输出到表：iris.predict。 1Done predicting. Predict table : iris.predict 查看结果表中的数据案例： 12%%sqlflowselect * from iris.predict limit 2 123456+--------------+-------------+--------------+-------------+-------+| sepal_length | sepal_width | petal_length | petal_width | class |+--------------+-------------+--------------+-------------+-------+| 6.3 | 2.7 | 4.9 | 1.8 | 2 || 5.7 | 2.8 | 4.1 | 1.3 | 1 |+--------------+-------------+--------------+-------------+-------+ 第三部分 系统架构系统原型使用下面的架构： 12SQL statement -&gt; our SQL parser --standard SQL-&gt; MySQL \-extended SQL-&gt; code generator -&gt; execution engine 原型运行的数据流为： 它通过MySQL Connector Python API从MySQL检索数据 从MySQL检索模型 通过调用用户指定的TensorFlow估算器训练模型或使用训练模型进行预测 并将训练过的模型或预测结果写入表格 参考文献及资料1、sqlflow项目官网 链接：https://github.com/sql-machine-learning/sqlflow 2、会 SQL 就能搞定 AI！蚂蚁金服重磅开源机器学习工具 SQLFlow 链接：https://www.infoq.cn/article/vlVqC68h2MT-028lh68C]]></content>
      <categories>
        <category>sqlflow</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Sqoop组件详细介绍]]></title>
    <url>%2F2019%2F04%2F28%2F2019-04-28-Sqoop%E7%BB%84%E4%BB%B6%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Sqoop原理 第一部分 Sqoop安装 第二部分 第三部分 第四部分 总结 参考文献及资料 背景在信息系统中，应用程序背后通常是关系型数据库（RDBMS），这类数据成为大数据的重要来源，在这样的需求背景下，诞生了Sqoop项目。 Apache Sqoop（发音：skup）是一个用于Hadoop生态系统（HDFS、Hive、HBase）和关系型数据库（RDBMS）（例如：Mysql、Oracle、Postgres、Tidb等）之间传输数据的工具。Sqoop是Apache Software Foundation的开源软件产品，官网介绍如下： Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases. 另外sqoop的命名取自：SQl+hadOOP=SQOOP。Sqoop项目开始于2009年（最初由Cloudera开发和维护），最早作为Hadoop的一个第三方模块，后来为了方便用户快速部署使用、开发人员快速开发和迭代，Sqoop独立成为Apache的项目（时间轴上，2011年7月23日，开始由Apache孵化；到2012年4月，Sqoop项目被提升为Apache的顶级项目）。 Latest stable release is 1.4.7 (download, documentation). Latest cut of Sqoop2 is 1.99.7 (download, documentation). Note that 1.99.7 is not compatible with 1.4.7 and not feature complete, it is not intended for production deployment. Sqoop的两代版本Sqoop目前有两代版本，分别是是Sqoop1（1.4.x）和Sqoop2（1.99.x），两个版本不兼容，架构和用法上已经完全不同。Sqoop1最新稳定版为1.4.7，Sqoop2最新版本为1.99.7（1.99.7这个版本官网不建议部署生产系统）。 Sqoop2相对于Sqoop1主要改进有： 引入Sqoop Server，集中化管理Connector 支持多种访问方式：CLI、WEB UI、Rest Api、Java API 引入基于角色的安全机制 两代Sqoop主要功能差异如下： Feature（功能） Sqoop 1 Sqoop 2 Connectors for all major RDBMS（连接所有主流RDBMS数据库） Supported（支持） Not supported.Workaround: Use the generic JDBC Connector which has been tested on the following databases: Microsoft SQL Server, PostgreSQL, MySQL and Oracle.This connector should work on any other JDBC compliant database. However, performance might not be comparable to that of specialized connectors in Sqoop.不再支持。但是使用通用连接器（generic JDBC Connector），性能上无法和专用连接器相比。 Kerberos Security Integration（Kerberos安全集成） Supported（支持） Supported（支持） Data transfer from RDBMS to Hive or HBase（数据从RDBMS传输到hive或者Hbase） Supported（支持） Not supported.Workaround: Follow this two-step approach.Import data from RDBMS into HDFSLoad data into Hive or HBase manually using appropriate tools and commands such as the LOAD DATA statement in Hive。不支持。解决办法： 按照此两步方法操作。 将数据从 RDBMS 导入 HDFS 在 Hive 中使用相应的工具和命令（例如 LOAD DATA 语句），手动将数据载入 Hive 或 HBase。 Data transfer from Hive or HBase to RDBMS（数据从Hive或者Hbase传输到RDBMS） Not supported.Workaround: Follow this two-step approach.Extract data from Hive or HBase into HDFS (either as a text or Avro file)Use Sqoop to export output of previous step to RDBMS。解决办法： 按照此两步方法操作。 从 Hive 或 HBase 将数据提取至 HDFS （作为文本或 Avro 文件） 使用 Sqoop 将上一步的输出导出至 RDBMS Not supported.Follow the same workaround as for Sqoop 1.（参考Sqoop1） Sqoop1支持集成Kerberos认证,Sqoop2的1.99.3版本还不支持(Sqoop 1.99.6版本已经支持)。 两代Sqoop的架构数据流图如下： Sqoop1 Sqoop1属于单机模式部署形态，使用客户端（CLI控制台）直接提交命令给hadoop集群，命令和脚本需要显示指定用户数据库名和密码。主要数据流如下： Sqoop2 Sqoop2属于C/S模式部署的形态，架构上引进了Server框架，对Connector实现集中管理。主要数据流如下： Sqoop的metadata保存在Derby数据库中。 两代版本的优缺点比较：Sqoop1使用客户端架构，用户需要在客户端部署Sqoop和相关连接器，而Sqoop2使用服务框架，连接器部署在Sqoop服务器上。从MR角度看，Sqoop1只是提交maper only作业，而Sqoop2提交了MapReduce作业，map完成从源传输数据，reduce根据指定的源转换数据，这是一个完整的MR抽象。另外从安全角度，Sqoop1每次任务都需要输入用户名和密钥，Sqoop2中由管理员设置源和目标的连接，普通用户只需要使用已经创建的连接，无需知道连接的详细信息。 第一部分 Sqoop原理Sqoop将导入和导出命令翻译成Mapreduce程序来实现。 第二部分 Sqoop的安装Sqoop只是一个工具，只需要在一个节点上安装部署即可。 Sqoop只能在Linux系统上部署。 目前那部分 2.1 Sqoop1的部署2.2 Sqoop2的部署2.3 部署总结第三部分 Sqoop的使用3.1 Sqoop1的使用3.2 Sqoop2的使用3.3 使用总结第四部分 华为hadoop集群中Loader产品的使用华为Loader组件产品基于Sqoop开源组件进行封装和优化，并且Loader提供一个可视化的配置管理界面，有效的降低的用户使用门槛。 参考文献及资料1、Sqoop项目官网 链接：http://sqoop.apache.org 2、Sqoop User Guide (v1.4.6) 链接：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html 3、sqoop1vs sqoop2 链接：https://www.wikitechy.com/tutorials/sqoop/sqoop1-vs-sqoop2 4、Feature Differences - Sqoop 1 and Sqoop 2 https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cdh_ig_sqoop_vs_sqoop2.html 5、sqoop关系型数据迁移原理以及map端内存为何不会爆掉窥探 6、重拾初心——Sqoop1和Sqoop2的刨析对比 链接：https://blog.csdn.net/gamer_gyt/article/details/55225700 7、sqoop2基本架构、部署和个人使用感受 https://cloud.tencent.com/info/d865fe6ed0d92d03bdf8512a4c9e4212.html https://blog.csdn.net/qq_38776653/article/details/77802871]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Avro详细介绍]]></title>
    <url>%2F2019%2F04%2F25%2F2019-07-13-%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Avro中的Schema 第二部分 数据序列化和反序列化 第二部分 命令行读写Avro文件 第三部分 Avro的使用场景介绍 第四部分 总结 参考文献及资料 背景同样的数据不同的表达，会直接影响后续任务的难易程度和有效性，因此寻找好的数据表示是机器学习的核心任务。 根据数据表示的提取方式（人工和自动）分为： 表示学习（representation learning）：自动学习数据的表示。 特征工程（feature engineering）：人工提起数据特征和表示。 比较：数据结构和算法紧密相连。数据表示和学习算法。 特征表达学习追溯到始祖象阶段，主要是无监督特征表达PCA和有监督特征表达LDA 但放在统计学习框架下，我们会碰到模型复杂性问题。这一问题的来源在于，设计的每个模型离真实的模型之间总会有偏差的存在，同时，模型的参数会导致其模型自身在寻优时存在波动，即会产生方差。因此，从统计意义来讲，一个好的模型需要在偏差和方差之间寻找平衡，如图1所示。在深度学习未包打天下之前的年代，这种平衡往往是通过控制模型的复杂性来获得的。对于复杂性的认识，这几十年来一直在变迁中。有通过控制模型的参数数量来实现的，如贝叶斯信息准则、Akaike信息准则；有从信息论的编码长度角度出发的，如Kolmogrov引出的最小描述长度，面向聚类的最小信息长度；有从数据几何结构出发的，如限束空间光滑性的流形约束；有从稀疏性角度出发的，如惩罚模型系数总量的L1范数；还有从模型结构的推广能力进行惩罚的，如统计机器学习中曾经盛行一时的VC维、最大边缘等约束。 那么，深度学习又是怎么玩的呢？不管采用什么样的结构，深度学习最明显的特点就是模型深，参数多。自2006年深度伯兹曼机的提出至今，残差网、稠密网、Inception网等各种深度学习模型的可调整参数的数量都在百万级甚至百万级的百倍以上。这带来一个好处，即他能表达一个远大于原有空间的空间，学术上称之为过完备空间。一般来说，在这个过完备空间上寻找不符合统计规律、但却具有优良品质的个例的机会就显著增大了。 那么为什么以前不做呢？一方面之前没有那么大规模的数据量，另一方面以前的工程技术也不支持考虑这么大规模的模型。目前多数已知的传感器成本降了不少、各种类型的数据获取成本也下来了，所以能看到PB级甚至ZB级的数据，如图像、语音、文本等。实在找不到数据的领域，还可以通过14年提出的生成式对抗网络来生成足够逼真的、海量的大数据。这两者都使得训练好的模型在刻画这个过完备空间的能力上增强了不少。 其次，工程技术上的革新也推动了深度学习的成功。深度学习的前身如多层感知器或其它神经网络模型在利用经典的反向传播算法调整模型的参数时，往往会陷入局部极小、过度拟合、会存在调参停滞的梯度消失、梯度爆炸等问题，还缺乏处理大规模数据需要的并行计算能力。这些问题，在近10年的深度学习发展中或多或少都得到了部分解决，比如通过规一化来防止梯度消失的Batch Normalization(批规范化)技术，考虑增强网络的稳定性、对网络层进行百分比随机采样的Drop Out技术，还有数据增广技术等。这使得深度学习在这个过完备空间搜索具有优良品质的个例的算力得到了显著增强。 总结参考文献及资料1、Avro项目官网 链接：https://avro.apache.org/docs/current/ 2、Avro Tools API官方文档 链接：http://avro.apache.org/docs/1.7.4/api/java/org/apache/avro/tool/package-summary.html socket http://layer0.authentise.com/getting-started-with-avro-and-python-3.html]]></content>
      <categories>
        <category>Avro，hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Avro详细介绍]]></title>
    <url>%2F2019%2F04%2F25%2F2020-10-25-Avro%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Avro中的Schema 第二部分 数据序列化和反序列化 第二部分 命令行读写Avro文件 第三部分 Avro的使用场景介绍 第四部分 总结 参考文献及资料 背景Avro（发音[ævrə]）是Apache基金会的一个独立项目，按照项目介绍Avro是一个数据序列化系统（Apache Avro™ is a data serialization system）。这个项目是Ddoug Cutting（Hadoop之父）创立的，Hadoop生态圈中其他项目如Hbase、Hive也采用Avro进行数据传输。Avro将数据转换成便于存储和传输的格式，实现数据密集型应用完成远程和本地大规模数据的交互和存储。 按照项目官网的介绍，Avro可以提供： 丰富的数据结构类型 快速可压缩的二进制数据形式 存储持久数据的文件容器 远程过程调用 RPC 简单的动态语言结合功能，Avro 和动态语言结合后，读写数据文件和使用 RPC 协议都不需要生成代码，而代码生成作为一种可选的优化只值得在静态类型语言中实现。 Avro 支持跨编程语言实现（C、C++、C#、Java、Python、Ruby、PHP），例如我们使用Java API写入Avro文件，然后通过Python API来读取该文件。这个特性有点像Thrift 和 Protocol Buffers 等系统，区别主要是： 动态类型：Avro 并不需要将生成代码，模式和数据存放在一起，而模式使得整个数据的处理过程并不生成代码、静态数据类型等等。这方便了数据处理系统和语言的构造。 未标记的数据：由于读取数据的时候模式是已知的，那么需要和数据一起编码的类型信息就很少了，这样序列化的规模也就小了。 不需要用户指定字段号：即使模式改变，处理数据时新旧模式都是已知的，所以通过使用字段名称可以解决差异问题。 第一部分 Avro中的SchemaAvro Schema（模式）使用JSON语言定义。通过Schema定义Avro数据结构，只有确定了Schema才能对数据进行解析。 1.1 数据类型数据类型分为：基本类型和复杂类型。 1.1.1 基本类型（Primitive Types）Schema定义了8种简单数据类型，主要有： 类型 说明 null no value boolean a binary value int 32-bit signed integer long 64-bit signed integer float single precision (32-bit) IEEE 754 floating-point number double double precision (64-bit) IEEE 754 floating-point number bytes sequence of 8-bit unsigned bytes string unicode character sequence 基本类型没有子属性，基本类型的名字也就是类型的名字，例如： 1&#123;"type": "string"&#125; 1.1.2 复杂类型（Complex Types）Avro提供了6种复杂类型，分别是Record（记录类型）、Enum（枚举类型）、Array（数组类型）、Map（映射类型）、Union（组合类型）和Fixed（混合类型）。 Record（记录类型）record类型支持属性有： name：record类型的名字(必填) namespace：命名空间(可选) doc：这个类型的文档说明(可选) aliases：record类型的别名，是个字符串数组(可选) fields：record类型中的字段，是个对象数组(必填)。每个字段有以下属性： 序号 属性名 介绍 1 name 字段名字(必填) 2 doc 字段说明文档(可选) 3 type 一个schema的json对象或者一个类型名字(必填) 4 default 默认值(可选) 5 order 排序(可选)，只有3个值ascending(默认)，descending或ignore 6 aliases 别名，字符串数组(可选) 我们来看一个record类型例子。下面的例子定义了一个名为test的Schema，test有两个属性：name和value。 123456789&#123; "type": "record", "name": "test", "aliases": ["JustTest"], "fields" : [ &#123;"name": "name", "type": "string","default": "name"&#125;, &#123;"name": "value", "type": "long"&#125; ]&#125; 对于default属性意义是：当Schema实例数据中某个field属性没有提供实例数据时候，由默认值替代。 Enum（枚举类型）Enum为枚举类型，类型名字为”enum”，还支持其它属性的设置： name：枚举类型的名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) doc：说明文档(可选) symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。 下面定义一个名为language的Schema： 12345&#123; "name": "language", "type": "enum", "symbols" : ["python", "java", "go", "c++","C","R"]&#125; Array（数据类型）数组类型的类型名为”array”并且只支持一个属性：items。下面的Schema定义了一个字符串数组： 1234&#123; "type": "array", "items": "string" &#125; Map（映射类型）Map类型的类型名字是”map”并且只支持一个属性：values（Map的key必须是字符串）。例子： 1234&#123; "type": "map", "values": "long" &#125; Union（组合类型）组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。 组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。 组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。 组合类型不允许嵌套组合类型。 Fixed（混合类型）混合类型的类型名为：fixed，支持以下属性： name：名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) size：一个整数，表示每个值的字节数(必填) 比如16个字节数的fixed类型例子如下： 12345&#123; "name": "md5"， "type": "fixed", "size": 16&#125; 1.2 一个Schema例子下面是一个Schema的例子使用的record类型、array类型、string类型、enum类型。 123456789101112131415&#123;"name":"Parent", "type":"record", "fields":[ &#123;"name":"children", "type":&#123; "type":"array", "items":&#123;"name":"Child","type":"record", "fields":[&#123;"name":"name","type":"string"&#125;] &#125; &#125; &#125;,&#123;"name":"language", "type":&#123;"type":"enum","symbols":["python","java","go","c++","C","R"]&#125; &#125; ]&#125; 数据案例： 1&#123;"children":[&#123;"name": "xiaoming"&#125;,&#123;"name": "xaioqiang"&#125;&#125;],"language": "python"&#125; 下面网站提供根据json数据案例生成Schema： https://toolslick.com/generation/metadata/avro-schema-from-json? 第二部分 数据序列化和反序列化Avro支持两种数据序列化编码方式：二进制格式（Binary encoding） 和JSON格式（json encoding）。 二进制格式。使用二进制编码会高效序列化，并且序列化后得到的结果会比较小。用于生产环境。 JSON格式。而JSON一般用于调试系统或是基于WEB的应用。 下面将详细介绍使用Python和Java实现Avro的数据序列化和反序列化。重点介绍二进制格式的序列化。 2.1 Python API2.1.1 部署avro-python3环境针对python2和python3分别安装： 1234# python3pip install avro-python3# python2pip install avro 在解析Schema的时候python2和python3有个方法名的差异需要注意： python2：avro.schema.parse python3：avro.schema.Parse 2.1.2 用python实现avro的文件读写案例使用二进制序列化数据并反序列化解析打印： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import avro.schemafrom avro.io import DatumReader,DatumWriter,BinaryEncoder,BinaryDecoderimport ioimport json# 数据案例data = &#123;"name": "Virutal conference", "date": 25612345, "location":"New York", "speakers":["Speaker1","Speaker2"], "participants":["Participant1","Participant2","Participant3","Participant4", "Participant5"], "seatingArrangement":&#123;"Participant1":1,"Participant2":2, "Participant3":3, "Participant4":4, "Participant5":5&#125;&#125;# print(data)# 定义Schema模式，python中使用字典格式存储Schema。schema_dict = &#123; "namespace": "demo.avro", "type": "record", "name": "Conference", "fields": [ &#123;"name": "name", "type": "string"&#125;, &#123;"name": "date", "type": "long"&#125;, &#123;"name": "location", "type": "string"&#125;, &#123;"name": "speakers", "type": &#123;"type":"array","items":"string"&#125;&#125;, &#123;"name": "participants", "type": &#123;"type": "array", "items": "string"&#125;&#125;, &#123;"name": "seatingArrangement", "type": &#123;"type": "map", "values": "int"&#125;&#125; ] &#125;# print(schema_demo)# 解析Schema文件schema = avro.schema.Parse(json.dumps(schema_demo))############################### 定义writewriter = DatumWriter(schema)bytes_writer = io.BytesIO()# 使用二进制序列化BinaryEncoder数据encoder = BinaryEncoder(bytes_writer)# 写入数据writer.write(data,encoder)raw_bytes = bytes_writer.getvalue()print(len(raw_bytes))print(type(raw_bytes))############################## 定义read，使用二进制反序列化BinaryDecoder数据bytes_reader = io.BytesIO(raw_bytes)decoder = BinaryDecoder(bytes_reader)reader = DatumReader(schema)# 读取数据并打印item = reader.read(decoder)print(item)#回显："""382&lt;class 'bytes'&gt;&#123;'name': 'Virutal conference', 'date': 25612345, 'location': 'New York', 'speakers': ['Speaker1', 'Speaker2'], 'participants': ['Participant1', 'Participant2', 'Participant3', 'Participant4', 'Participant5'], 'seatingArrangement': &#123;'Participant1': 1, 'Participant2': 2, 'Participant3': 3, 'Participant4': 4, 'Participant5': 5&#125;&#125;""" json序列化：https://pythonhosted.org/avro_json_serializer/ https://wp.huangshiyang.com/avro-schema-python-serialize-and-deseralize 2.2 Java API2.2.1 Maven依赖新建一个Maven项目，在pom.xml中添加两个依赖项：Apache Avro库和Maven插件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;avro.version&gt;1.8.2&lt;/avro.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-compiler&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-ipc&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;avro.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;schemas&lt;/id&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;schema&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;StringPair.avsc&lt;/include&gt; &lt;/includes&gt; &lt;stringType&gt;String&lt;/stringType&gt; &lt;sourceDirectory&gt;src/main/avro/&lt;/sourceDirectory&gt; &lt;outputDirectory&gt;src/main/java/&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 我们正在配置插件以在src/main/avro中查找规范文件并将生成的Java类放到src/main/java中 2.2.2 定义Schame文件https://www.hascode.com/2014/03/using-apache-avro-with-java-and-maven/ https://my.oschina.net/weiwei02/blog/877979 https://yanbin.blog/java-python-communicate-via-apache-avro/ http://cloudurable.com/blog/avro/index.html https://fangjian0423.github.io/2016/02/21/avro-intro/ 简单： https://juejin.im/post/5c36018ee51d4552475fb347 第三部分 命令行读写Avro文件Avro项目提供Avro Tools支持用户使用命令行方式快速读取一个二进制的Avro文件。这里案例使用下面github的项目文件： https://github.com/miguno/avro-cli-examples 3.1 Avro Tools获取目前Avro最新版本为1.8.2，我们获取最新的版本： 1wget http://ftp.wayne.edu/apache/avro/avro-1.8.2/java/avro-tools-1.8.2.jar 可以使用下面的命名查看工具支持的功能清单： 12345678910111213141516171819202122232425262728293031323334# java -jar avro-tools-1.8.2.jar --helpVersion 1.8.2 of Apache AvroCopyright 2010-2015 The Apache Software FoundationThis product includes software developed atThe Apache Software Foundation (http://www.apache.org/).----------------Available tools: cat extracts samples from files compile Generates Java code for the given schema. concat Concatenates avro files without re-compressing. fragtojson Renders a binary-encoded Avro datum as JSON. fromjson Reads JSON records and writes an Avro data file. fromtext Imports a text file into an avro data file. getmeta Prints out the metadata of an Avro data file. getschema Prints out schema of an Avro data file. idl Generates a JSON schema from an Avro IDL file idl2schemata Extract JSON schemata of the types from an Avro IDL file induce Induce schema/protocol from Java class/interface via reflection. jsontofrag Renders a JSON-encoded Avro datum as binary. random Creates a file with randomly generated instances of a schema. recodec Alters the codec of a data file. repair Recovers data from a corrupt Avro Data file rpcprotocol Output the protocol of a RPC service rpcreceive Opens an RPC Server and listens for one message. rpcsend Sends a single RPC message. tether Run a tethered mapreduce job. tojson Dumps an Avro data file as JSON, record per line or pretty. totext Converts an Avro data file to a text file. totrevni Converts an Avro data file to a Trevni file. trevni_meta Dumps a Trevni file's metadata as JSON.trevni_random Create a Trevni file filled with random instances of a schema.trevni_tojson Dumps a Trevni file as JSON. 3.2 JSON文件到二进制Avro文件将Json数据文件通过schema转换成Avro文件。使用fromjson命令，分为不压缩和压缩： 不压缩 1# java -jar avro-tools-1.8.2.jar fromjson --schema-file twitter.avsc twitter.json &gt; twitterTest.avro 使用twitter.avsc最为schema，转换结果重定向到文件twitterTest.avro。 压缩 1# java -jar avro-tools-1.8.2.jar fromjson --codec snappy --schema-file twitter.avsc twitter.json &gt; twitter.snappy.avro 使用snappy方式压缩 3.3 二进制Avro文件转换成json文件将Avro文件转换成json文件，使用tojson命令。 1# java -jar avro-tools-1.8.2.jar tojson twitter.snappy.avro &gt; twitter_tojson.json 注意这里不区分avro文件是否是压缩过的。另外可以使用-pretty参数打印出json数据： 1234567891011121314root@deeplearning:# java -jar avro-tools-1.8.2.jar tojson twitterTest.avro -prettylog4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.&#123; "username" : "miguno", "tweet" : "Rock: Nerf paper, scissors is fine.", "timestamp" : 1366150681&#125;&#123; "username" : "BlizzardCS", "tweet" : "Works as intended. Terran is IMBA.", "timestamp" : 1366154481&#125; 3.4 从Avro文件中获取Schema模式使用getschema 命令可以获取到Avro文件中Schema的模式信息（不区分是否压缩）： 1# java -jar avro-tools-1.8.2.jar getschema twitterTest.avro 3.5 将Schema编译成Java类使用下面的命令将Schema编译成java类文件： 1# java -jar avro-tools-1.8.2.jar compile schema twitter.avsc . 在当前目录会生成一个com目录。 第四部分 Avro文件结构https://www.cnblogs.com/duanxz/p/3799256.html https://blog.csdn.net/xyw_blog/article/details/8967362 第五部分 Avro的使用场景介绍kafka 使用：https://sonnguyen.ws/simple-example-python-kafka-avro/ 总结参考文献及资料1、Avro项目官网 链接：https://avro.apache.org/docs/current/ 2、Avro Tools API官方文档 链接：http://avro.apache.org/docs/1.7.4/api/java/org/apache/avro/tool/package-summary.html socket http://layer0.authentise.com/getting-started-with-avro-and-python-3.html]]></content>
      <categories>
        <category>Avro</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose使用介绍]]></title>
    <url>%2F2019%2F01%2F27%2F2020-10-03-Docker%E7%B3%BB%E7%BB%9F%E6%96%87%E7%AB%A0-Docker%20compose%E7%BC%96%E6%8E%92Docker%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Docker Compose的安装 第二部分 Docker Compose的命令介绍 第三部分 模板文件 总结 背景Docker Compose是Docker开源项目（项目在github上的地址为： https://github.com/docker/compose）。功能上实现对Docker容器集群的资源定义和快速编排（官网上对项目功能的简述：Defining and running multi-container Docker applications）。 对于单独一个容器，我们通常使用Dockerfile模板文件进行定义，而实际应用场景中我们需要定义和管理多个容器。Compose提供用户使用docker-compose.yml文件定义一组相关联的容器集群。项目使用Python语言编写，实现上，调用Docker提供的API对容器进行管理。 Docker Compose中有两个重要的概念： 服务（service）：一个应用容器； 项目（project）：一组相关联的应用容器组成的整体； Docker Compose默认管理的对象是项目。通过命令对项目中的容器进行生命周期管理。 第一部分 Docker Compose的安装Docker Compose支持Linux、MAC、WIN10三个平台。由于项目是Python编写，所以安装方式上支持： pip安装工具安装 使用编译好的二进制文件安装 其中Docker for Mac 、Docker for Windows 自带 docker-compose 二进制文件，无需单独安装，安装 Docker 之后可以直接使用。所以主要介绍Linux系统安装部署。 1.1 pip安装这种安装方式将Compose当作一个Python应用使用pip命令安装，需要提前部署Python环境。安装命令： 1# pip install -U docker-compose 建议ARM架构（比如树莓派）的Linux使用pip源安装，对于X86_64架构的Linux建议使用二进制包安装方式。 1.2 二进制包安装从Compose在Github上项目网站下载编译好的二进制文件到本地文件系统即可。 1234567# curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 617 0 617 0 0 617 0 --:--:-- --:--:-- --:--:-- 5016100 8648k 100 8648k 0 0 8648k 0 0:00:01 --:--:-- 0:00:01 8648k# chmod +x /usr/local/bin/docker-compose 对于卸载，如果是pip安装直接通过pip uninstall卸载，对于二进制文件安装，直接删除相关文件即可。 验证安装（查看版本号）： 12# docker-compose --versiondocker-compose version 1.16.1, build 6d1ac21 第二部分 Docker Compose的模板Docker Compose的核心思想是文件定义资源。默认的模板文件名称为 docker-compose.yml，格式为 YAML 格式。下面是一个模板案例： 12345678version: "3"services: webapp: image: examples/web ports: - "80:80" volumes: - "/data" 注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。下面分别介绍各个指令的用法。 2.1 镜像容器类image指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试从远端镜像服务拉取这个镜像。 1234image: ubuntuimage: orchardup/postgresql# 支持使用images idimage: a4bc65fd build服务除了可以基于现有的镜像，还可以基于指定的Dockerfile。指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）。 Compose 将会利用它自动构建这个镜像，然后使用这个镜像。 1234version: '3'services: webapp: build: ./dir 另外支持使用 context 指令指定 Dockerfile 所在文件夹的路径。使用 dockerfile 指令指定 Dockerfile 文件名。使用 arg 指令指定构建镜像时的变量。 123456789version: '3'services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 使用 cache_from 指定构建镜像的缓存： 12345build: context: . cache_from: - alpine:latest - corp/web_app:3.14 container_name指定容器名称。 1container_name: docker-web-container 注意: 指定容器名称后，该服务将无法进行扩展（scale），因为 Docker 不允许多个容器具有相同的名称。 labels为容器添加 Docker 元数据（metadata）信息。例如可以为容器添加辅助说明信息。 1234labels: com.startupteam.description: "webapp for a startup team" com.startupteam.department: "devops department" com.startupteam.release: "rc3 for v1.0" cap_add, cap_drop指定容器的内核能力（capacity）分配。例如，让容器拥有所有能力可以指定为： 12cap_add: - ALL 去掉 NET_ADMIN 能力可以指定为： 12cap_drop: - NET_ADMIN 2.2 网络资源类命令expose暴露端口，但不映射到宿主机，只被连接的服务访问。仅可以指定内部端口为参数。 123expose: - "3000" - "8000" network_mode设置网络模式。使用和 docker run 的 --network 参数一样的值。 12345network_mode: "bridge"network_mode: "host"network_mode: "none"network_mode: "service:[service name]"network_mode: "container:[container name/id]" networks配置容器连接的网络。 1234567891011version: "3"services: some-service: networks: - some-network - other-networknetworks: some-network: other-network: dns自定义 DNS 服务器。可以是一个值，也可以是一个列表。 12345dns: 8.8.8.8dns: - 8.8.8.8 - 114.114.114.114 dns_search配置 DNS 搜索域。可以是一个值，也可以是一个列表。 12345dns_search: example.comdns_search: - domain1.example.com - domain2.example.com ports暴露端口信息。使用宿主端口：容器端口 (HOST:CONTAINER) 格式，或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。 12345ports: - "3000" - "8000:8000" - "49100:22" - "127.0.0.1:8001:8001" *注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 并且没放到引号里，可能会得到错误结果，因为 YAML 会自动解析 xx:yy 这种数字格式为 60 进制。为避免出现这种问题，建议数字串都采用引号包括起来的字符串格式。 extra_hosts类似 Docker 中的 --add-host 参数，指定额外的 host 名称映射信息。 123extra_hosts: - "googledns:8.8.8.8" - "dockerhub:52.1.157.61" 会在启动后的服务容器中 /etc/hosts 文件中添加如下两条条目。 128.8.8.8 googledns52.1.157.61 dockerhub 2.3 系统资源类命令devices指定设备映射关系。 12devices: - "/dev/ttyUSB1:/dev/ttyUSB0" tmpfs挂载一个 tmpfs 文件系统到容器。 12345tmpfs: /runtmpfs: - /run - /tmp sysctls配置容器内核参数。 1234567sysctls: net.core.somaxconn: 1024 net.ipv4.tcp_syncookies: 0sysctls: - net.core.somaxconn=1024 - net.ipv4.tcp_syncookies=0 ulimits指定容器的 ulimits 限制值。例如，指定最大进程数为 65535，指定文件句柄数为 20000（软限制，应用可以随时修改，不能超过硬限制） 和 40000（系统硬限制，只能 root 用户提高）。 12345ulimits: nproc: 65535 nofile: soft: 20000 hard: 40000 volumes数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。该指令中路径支持相对路径。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 2.4 调度管理类depends_on解决容器的依赖、启动先后的问题，比如让数据库内容器先运行起来。以下例子中会先启动 redis db 再启动 web 1234567891011121314version: '3'services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres 注意：web 服务不会等待 redis db 「完全启动」之后才启动。 pid跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 ID 来相互访问和操作。 1pid: "host" command覆盖容器启动后默认执行的命令。 1command: echo "hello world" cgroup_parent指定父 cgroup 组，意味着将继承该组的资源限制。例如，创建了一个 cgroup 组名称为 cgroups_1。 1cgroup_parent: cgroups_1 stop_signal设置另一个信号来停止容器。在默认情况下使用的是 SIGTERM 停止容器。 1stop_signal: SIGUSR1 2.5 安全类secrets存储敏感数据，例如 mysql 服务密码。 12345678910111213141516version: "3.1"services:mysql: image: mysql environment: MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password secrets: - db_root_password - my_other_secretsecrets: my_secret: file: ./my_secret.txt my_other_secret: external: true security_opt指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等）。例如配置标签的用户名和角色名。 123security_opt: - label:user:USER - label:role:ROLE 2.6 日志和监控类healthcheck通过命令检查容器是否健康运行。 12345healthcheck: test: ["CMD", "curl", "-f", "http://localhost"] interval: 1m30s timeout: 10s retries: 3 logging配置日志选项。 1234logging: driver: syslog options: syslog-address: "tcp://192.168.0.42:123" 目前支持三种日志驱动类型。 123driver: "json-file"driver: "syslog"driver: "none" options 配置日志驱动的相关参数。 123options: max-size: "200k" max-file: "10" configs仅用于 Swarm mode，详细内容请查看 Swarm mode 一节。 deploy仅用于 Swarm mode，详细内容请查看 Swarm mode 一节 env_file从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过 docker-compose -f FILE 方式来指定 Compose 模板文件，则 env_file中变量的路径会基于模板文件路径。 如果有变量名称与 environment 指令冲突，则按照惯例，以后者为准。 123456env_file: .envenv_file: - ./common.env - ./apps/web.env - /opt/secrets.env 环境变量文件中每一行必须符合格式，支持 # 开头的注释行。 12# common.env: Set development environmentPROG_ENV=development environment设置环境变量。你可以使用数组或字典两种格式。只给定名称的变量会自动获取运行 Compose 主机上对应变量的值，可以用来防止泄露不必要的数据。 1234567environment: RACK_ENV: development SESSION_SECRET:environment: - RACK_ENV=development - SESSION_SECRET 如果变量名称或者值中用到 true|false，yes|no 等表达布尔含义的词汇，最好放到引号里，避免 YAML 自动解析某些内容为对应的布尔语义。这些特定词汇，包括 1y|Y|yes|Yes|YES|n|N|no|No|NO|true|True|TRUE|false|False|FALSE|on|On|ON|off|Off|OFF external_links 注意：不建议使用该指令。 链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的外部容器。 1234external_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql links 注意：不推荐使用该指令。 其它指令此外，还有包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir等指令，基本跟 docker run 中对应参数的功能一致。 指定服务容器启动后执行的入口文件。 1entrypoint: /code/entrypoint.sh 指定容器中运行应用的用户名。 1user: nginx 指定容器中工作目录。 1working_dir: /code 指定容器中搜索域名、主机名、mac 地址等。 123domainname: your_website.comhostname: testmac_address: 08-00-27-00-0C-0A 允许容器中运行一些特权命令。 1privileged: true 指定容器退出后的重启策略为始终重启。该命令对保持服务始终运行十分有效，在生产环境中推荐配置为 always 或者 unless-stopped。 1restart: always 以只读模式挂载容器的 root 文件系统，意味着不能对容器内容进行修改。 1read_only: true 打开标准输入，可以接受外部输入。 1stdin_open: true 模拟一个伪终端。 1tty: true 读取变量Compose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。 例如，下面的 Compose 文件将从运行它的环境中读取变量 ${MONGO_VERSION} 的值，并写入执行的指令中。 12345version: "3"services:db: image: "mongo:$&#123;MONGO_VERSION&#125;" 如果执行 MONGO_VERSION=3.2 docker-compose up 则会启动一个 mongo:3.2 镜像的容器；如果执行 MONGO_VERSION=2.8 docker-compose up 则会启动一个 mongo:2.8镜像的容器。 若当前目录存在 .env 文件，执行 docker-compose 命令时将从该文件中读取变量。 在当前目录新建 .env 文件并写入以下内容。 12# 支持 # 号注释MONGO_VERSION=3.6 执行 docker-compose up 则会启动一个 mongo:3.6 镜像的容器。 第三部分 案例下面是一个启动Elasticsearch集群的docker compose资源定义文件，我们使用注释进行介绍。 一共定义了5个服务容器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495version: '3'services: cerebro: image: lmenezes/cerebro:0.8.3 container_name: hwc_cerebro ports: - "9000:9000" command: - -Dhosts.0.host=http://elasticsearch:9200 networks: - hwc_es7net kibana: image: docker.elastic.co/kibana/kibana:7.1.0 container_name: hwc_kibana7 environment: #- I18N_LOCALE=zh-CN - XPACK_GRAPH_ENABLED=true - TIMELION_ENABLED=true - XPACK_MONITORING_COLLECTION_ENABLED="true" ports: - "5601:5601" networks: - hwc_es7net elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_hot environment: - cluster.name=elasticsearch - node.name=es7_hot - node.attr.box_type=hot - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms1g -Xmx1g" - discovery.seed_hosts=es7_hot,es7_warm,es7_cold - cluster.initial_master_nodes=es7_hot,es7_warm,es7_cold ulimits: memlock: soft: -1 hard: -1 volumes: - hwc_es7data_hot:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - hwc_es7net elasticsearch2: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_warm environment: - cluster.name=elasticsearch - node.name=es7_warm - node.attr.box_type=warm - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms1g -Xmx1g" - discovery.seed_hosts=es7_hot,es7_warm,es7_cold - cluster.initial_master_nodes=es7_hot,es7_warm,es7_cold ulimits: memlock: soft: -1 hard: -1 volumes: - hwc_es7data_warm:/usr/share/elasticsearch/data networks: - hwc_es7net elasticsearch3: image: docker.elastic.co/elasticsearch/elasticsearch:7.1.0 container_name: es7_cold environment: - cluster.name=elasticsearch - node.name=es7_cold - node.attr.box_type=cold - bootstrap.memory_lock=true - "ES_JAVA_OPTS=-Xms1g -Xmx1g" - discovery.seed_hosts=es7_hot,es7_warm,es7_cold - cluster.initial_master_nodes=es7_hot,es7_warm,es7_cold ulimits: memlock: soft: -1 hard: -1 volumes: - hwc_es7data_cold:/usr/share/elasticsearch/data networks: - hwc_es7netvolumes: hwc_es7data_hot: driver: local hwc_es7data_warm: driver: local hwc_es7data_cold: driver: localnetworks: hwc_es7net: driver: bridge 参考文献项目文档：https://yeasy.gitbooks.io/docker_practice/compose/compose_file.html Docker Compose 配置文件详解 https://www.jianshu.com/p/2217cfed29d7 Docker：Docker Compose 详解 https://www.jianshu.com/p/658911a8cff3]]></content>
      <categories>
        <category>docker compose</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch性能调优总结]]></title>
    <url>%2F2019%2F01%2F27%2F2019-01-27-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录[TOC] ##背景 Elasticseach是一个基于Apache Lucene的开源检索引擎，允许用户近实时的存储、检索和分析数据。虽然 Elasticsearch 专为快速查询而设计，但其性能在很大程度上取决于应用使用的场景、索引的数据量以及应用用户查询数据的速度。本文主要参考Elasticseach官方文档和社区在集群调优上的经验，根据个人经验主要分为：集群的性能调优、集群稳定性调优、业务使用调优。 ##第一部分 性能调优 ###1.1 操作系统侧基线调优建议 ####1.1.1 关闭交换分区（或降低Swappiness值） swap空间是一块物理磁盘空间，操作系统使用这块空间保存从内存中换出的不常用内存数据，这样可以分配出更多的内存给当前使用的应用进程。 然而技术都是有适用场景的。相对于内存，磁盘的读写较慢（内存读写速度为纳秒级、磁盘的速度只能达到毫秒级），当系统发生大量的内存数据交换时，将影响系统运行性能。而elasticsearch对读写有较高的性能要求，如果进程使用的内存数据被写入swap磁盘，接着又换出读入内存，将极大影响性能，所以通常建议关闭或减少swap分区。 关于Linux系统swap交换分区的介绍可以参考下面的文章： https://www.linux.com/news/all-about-linux-swap-space 临时禁用swap（操作系统重启失效）： 1swapoff -a 永久禁用swap： 将/etc/fstab 文件中包含swap的行注释掉 1sed -i '/swap/s/^/#/' /etc/fstabswapoff -a 对于应用混用部署环境中，如果不能关闭swap分区的场景，Linux提供swappiness参数（0-100）控制。swappiness=0的时候表示最大限度使用物理内存，swappiness=100表示积极使用swap分区，及时将不用的内存数据搬运到swap中。操作系统默认值为60，即物理内存使用率操作40%（100-60），开始使用swap分区。这个值决定操作系统交换内存的频率。可以预防正常情况下发生交换，但仍允许操作系统在紧急情况下发生交换。 临时调整 1sysctl vm.swappiness=10 永久调整 12345# root用户vi /etc/sysctl.conf# 添加 vm.swappiness = 10# 生效sysctl -p 另外ElasticSearch 自身也提供相关优化参数。打开配置文件中的mlockall开关，它的作用就是运行JVM虚拟机时锁住内存，禁止操作系统将其内存数据交换出去。在elasticsearch.yml配置中修改如下： 1bootstrap.mlockall: true ElasticSearch 提供下面的API可以查询集群是否配置该参数： 12curl -XGET localhost:9200/_nodes?filter_path=**.mlockall##回显：&#123;"nodes":&#123;"VyKDGurkQiygV-of4B1ZAQ":&#123;"process":&#123;"mlockall":true&#125;&#125;&#125;&#125; ####1.1.2 其他操作系统资源限制调整 调整操作系统部分资源限制配置，提高性能。 1234567891011121314151617# 修改系统资源限制# 单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些echo "* - nofile 655360" &gt;&gt;/etc/security/limits.conf# 单用户内存地址空间echo "* - as unlimited" &gt;&gt;/etc/security/limits.conf# 单用户线程数echo "* - nproc 2056474" &gt;&gt;/etc/security/limits.conf# 单用户文件大小echo "* - fsize unlimited" &gt;&gt;/etc/security/limits.conf# 单用户锁定内存echo "* - memlock unlimited" &gt;&gt;/etc/security/limits.conf# 单进程可以使用的最大map内存区域数量echo "vm.max_map_count = 655300" &gt;&gt;/etc/sysctl.conf# TCP全连接队列参数设置， 这样设置的目的是防止节点数较多（比如超过100）的ES集群中，节点异常重启时全连接队列在启动瞬间打满，造成节点hang住，整个集群响应迟滞的情况echo "net.ipv4.tcp_abort_on_overflow = 1" &gt;&gt;/etc/sysctl.confecho "net.core.somaxconn = 2048" &gt;&gt;/etc/sysctl.conf# 降低tcp alive time，防止无效链接占用链接数echo 300 &gt;/proc/sys/net/ipv4/tcp_keepalive_time ####1.1.3 多个path.data路径的设置 使用多个IO设备（设置多个path.data）存储shards，能够增加总的存储空间并提升IO性能。另外查询时shards并行检索，IO会分散负载到各个磁盘，提高查询效率。 ElasticSearch 2.0之前的版本，可以配置多个path.data路径，但是其相当于RAID 0，每个分片（shards）的数据会分布在所有的磁盘上。如果集群中一个节点上有一块盘坏了损坏，该节点上所有的shards都会损坏了。需要恢复该节点上的所有shards。2.0.0版本之后，这个功能得到了优化：每个shards所有的数据只会在一块磁盘上面。这样即使一个节点的一块磁盘损坏了，也只是损失了该磁盘上的shards，其它磁盘上的shards安然无事。只需要恢复该块盘上的shards即可。提高了数据分布式存储的高可用。 集群升级到2.0.0版本时，旧版本一个shard分布到所有磁盘上的数据，会调整拷贝到一块盘上。 参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/master/path-settings.html ####1.1.4 磁盘挂载选项 服务器挂载磁盘时，可以做如下性能参数调优： noatime：禁止记录访问时间戳，提高文件系统读写性能 data=writeback： 不记录data journal，提高文件系统写入性能 barrier=0：barrier保证journal先于data刷到磁盘，上面关闭了journal，这里的barrier也就没必要开启了 nobh：关闭buffer_head，防止内核打断大块数据的IO操作 1mount -o noatime,data=writeback,barrier=0,nobh /dev/sda /es_data_dir ####1.1.5 磁盘设备优化 elasticsearch对读写性能要求较高，当然磁盘性能越高越好，生产建议使用固态SSD。读写速度媲美于内存，另外SSD磁盘采用电梯调度算法，提供了更智能的请求调度算法，不需要内核去做多余的调整 。 使用SSD（PCI-E接口SSD卡/SATA接口SSD盘）通常比机械硬盘（SATA盘/SAS盘）查询速度快5~10倍。但是写入性能提升不明显。 当然一分钱一分货，SSD比SAS贵不少。 ###1.2 elasticsearch 配置文件优化 ####1.2.1 增加写入buffer和bulk队列长度 elasticsearch在处理索引的时候会存在一个索引缓冲buffer，可以为其设置需要分配的缓冲buffer内存大小，这是一个全局设置。 123# 这意味着分配给一个节点的总存储器的10％将被用作索引的缓冲区大小indices.memory.index_buffer_size: 10%thread_pool.bulk.queue_size: 1024 ####1.2.2 计算disk使用量时，不考虑正在搬迁的shard 在规模比较大的集群中，可以防止新建shard时扫描所有的shard的元数据，提升shard的分配速度。 1cluster.routing.allocation.disk.include_relocations: false ##第二部分 集群稳定性调优 ###2.1 操作系统参数调优 主要为资源限制配置，参考1.1.2章节。 ###2.2 Elasticsearch 配置 ####2.2.1 JVM虚拟机参数 Elasticsearch由java语言编写开发。在 Java 中，所有的对象都分配在堆上，并通过一个指针进行引用。 普通对象指针（OOP）指向这些对象，通常为 CPU 字长的大小：32 位或 64 位，取决于你的处理器。指针引用的就是这个 OOP 值的字节位置。 对于 32 位的系统，意味着堆内存大小最大为 4 GB。对于 64 位的系统， 可以使用更大的内存，但是 64 位的指针意味着更大的浪费，因为你的指针本身大了。更大的指针在主内存和各级缓存（例如 LLC，L1 等）之间移动数据的时候，会占用更多的带宽. 配置文件默认堆内存的大小为1G，官方建议将堆大小保持在接近32 GB（最佳建议30.5 GB阈值）。超过32G，jvm会禁用内存对象指针压缩技术，造成内存浪费。 关于堆大小的设置，elasticsearh官网有比较详细的说明： https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html -Xms和-Xmx设置为相同的值，推荐设置为：min（机器内存的一半左右，30.5G)。剩余留给系统cache使用。 另外jvm堆内存建议不要低于2G，否则有可能因为内存不足导致elasticsearch进程无法正常启动或内存溢出（OOM）。 假设你是土豪，机器有 128 GB 的内存，你可以创建两个节点，每个节点内存分配不超过 32 GB。 也就是说不超过 64 GB 内存给 elasticsearch的堆内存，剩下的超过 64 GB 的内存给 Lucene。 ####2.2.2 节点配置参数 设置内存熔断参数，防止写入或查询压力过高导致OOM，具体数值可根据使用场景调整。 123indices.breaker.total.limit: 30% indices.breaker.request.limit: 6% indices.breaker.fielddata.limit: 3% 减少查询使用的cache，避免cache占用过多的jvm内存，具体数值可根据使用场景调整。 12indices.queries.cache.count: 500 indices.queries.cache.size: 5% 单机多节点时，主从shard分配以ip为依据，分配到不同的机器上，避免单机挂掉导致数据丢失。 12cluster.routing.allocation.awareness.attributes: ip node.attr.ip: 1.1.1.1 ###2.3 集群使用方式 ####2.3.1 设置专用master节点Elasticsearch集群的元数据管理、Index的新建、删除操作、节点的加入和隔离、定期将整个集群的健康状态信息广播给各节点等管理任务，均由master节点角色负责。当集群的节点数较大时，集群的管理任务需要消耗大量资源。建议设置专用master节点，master只负责集群管理工作，不负责存储数据（无数据读写压力），提高集群管理的稳定性。 123456789# 专有master节点的配置文件（elasticsearch.yml）增加如下属性：node.master: true node.data: false node.ingest: false# 数据节点的配置文件增加如下属性（与上面的属性相反）：node.master: false node.data: true node.ingest: true 2.3.2 大内存物理机多实例部署之前章节也提到，假如你的机器有128G内存，可以创建两个Data Node实例，使用32G内存。也就是说64G内存给ES的堆内存，剩下的64G给Lucene。如果你选择第二种，你需要配置（默认是false）： 1cluster.routing.allocation.same_shard.host:true 这会防止同一个shard的主副本存在同一个物理机上（因为如果都在一个机器上，副本的高可用性就没有了） ####2.3.3 控制集群中index（索引）和shard（分片）总量 集群中master节点负责集群元数据管理，定期会同步给各节点（集群每个节点都会存储一份）。 Elasticsearch元数据存储在clusterstate中。例如所有节点元信息（indices、节点各种统计参数）、所有index/shard的元信息（mapping, location, size）、元数据ingest等。 elasticsearch在创建新shard时，要根据现有的分片分布情况指定分片分配策略，从而使各个节点上的分片数基本一致，此过程中就需要深入遍历clusterstate。当集群中的index/shard过多时，clusterstate结构会变得过于复杂，导致遍历clusterstate效率低下，集群响应迟滞。 当index/shard数量过多时，可以考虑从以下几方面优化： 降低数据量较小的index的shard数量 把一些有关联的index合并成一个index 数据按某个维度做拆分，写入多个集群 ####2.3.4 Segment Memory优化 elasticsearch底层使用Lucene实现存储，Lucene的一个index由若干segment组成，每个segment都会建立自己的倒排索引用于数据查询。Lucene为了加速查询，为每个segment的倒排做了一层前缀索引，这个索引在Lucene4.0以后采用的数据结构是FST (Finite State Transducer)。Lucene加载segment的时候将其全量装载到内存中，加快查询速度。 这部分内存被称为Segment Memory， 常驻内存，占用heap，无法被GC。 前面提到，为利用JVM的对象指针压缩技术来节约内存，通常建议JVM内存分配不要超过32G。当集群的数据量过大时，Segment Memory会吃掉大量的堆内存，而JVM内存空间有限，此时就需要想办法降低Segment Memory的使用量了。 常用方法有： 定期清理删除不使用的index 对于不常访问的index，可以通过close接口将其关闭，用到时再打开（此时数据仍然保存在磁盘，只是释放掉了内存，无法检索，需要时可以重启open） 通过force_merge api接口强制合并segment，降低segment数量，从而减少Segment Memory 12&gt; curl -XPOST "http://localhost:9200/library/_forcemerge?max_num_segments=1&gt; ###2.4 温热数据分离架构 当Elasticsearch集群用于大量实时数据分析的场景时，elasticsearch官方推荐使用基于时间的索引，并且集群使用三种不同类型的节点：Master、Hot-Node、Warm-Node，进行结构分层。即所谓的“Hot-Warm”集群架构。 参考官方文章： https://www.elastic.co/blog/hot-warm-architecture-in-elasticsearch-5-x 第三部分 业务使用调优###3.1 控制字段的存储选项 elasticsearch底层使用Lucene存储数据，主要包括行存（StoreFiled）、列存（DocValues）和倒排索引（InvertIndex）三部分。 大多数使用场景中，没有必要同时存储这三个部分，可以通过下面的参数来做适当调整： StoreFiled： 行存，其中占比最大的是source字段，它控制doc原始数据的存储。在写入数据时，elasticsearch把doc原始数据的整个json结构体当做一个string，存储为source字段。查询时，可以通过source字段拿到当初写入时的整个json结构体。 所以，如果没有取出整个原始json结构体的需求，可以通过下面的命令，在mapping中关闭source字段或者只在source中存储部分字段，数据查询时仍可通过elasticsearch的docvaluefields获取所有字段的值。 注意：关闭source后， update, updatebyquery, reindex等接口将无法正常使用，所以有update等需求的index不能关闭source。 1234# 关闭 _sourcePUT my_index &#123;"mappings": &#123;"my_type": &#123;"_source": &#123;"enabled": false&#125;&#125;&#125;&#125;# _source只存储部分字段，通过includes指定要存储的字段或者通过excludes滤除不需要的字段，例如PUT my_index&#123;"mappings": &#123;"_doc": &#123;"_source": &#123;"includes": ["*.count","meta.*"], "excludes": ["meta.description","meta.other.*"]&#125;&#125;&#125;&#125; docvalues：控制列存。 ES主要使用列存来支持sorting, aggregations和scripts功能，对于没有上述需求的字段，可以通过下面的命令关闭docvalues，降低存储成本。 1PUT my_index&#123;"mappings": &#123;"my_type": &#123;"properties": &#123;"session_id": &#123;"type": "keyword", "doc_values": false&#125;&#125;&#125;&#125;&#125; index：控制倒排索引。 ES默认对于所有字段都开启了倒排索引，用于查询。对于没有查询需求的字段，可以通过下面的命令关闭倒排索引。 1PUT my_index&#123;"mappings": &#123;"my_type": &#123;"properties": &#123;"session_id": &#123;"type": "keyword", "index": false&#125;&#125;&#125;&#125;&#125; all：ES的一个特殊的字段，ES把用户写入json的所有字段值拼接成一个字符串后，做分词，然后保存倒排索引，用于支持整个json的全文检索。 这种需求适用的场景较少，可以通过下面的命令将all字段关闭，节约存储成本和cpu开销。（ES 6.0+以上的版本不再支持_all字段，不需要设置） 1PUT /my_index&#123;"mapping": &#123;"my_type": &#123;"_all": &#123;"enabled": false&#125;&#125;&#125;&#125; fieldnames：该字段用于exists查询，来确认某个doc里面有无一个字段存在。若没有这种需求，可以将其关闭。 1PUT /my_index&#123;"mapping": &#123;"my_type": &#123;"_field_names": &#123;"enabled": false&#125;&#125;&#125;&#125; ###3.2 开启最佳压缩 对于打开了上述_source字段的index，可以通过下面的命令来把Lucene适用的压缩算法替换成 DEFLATE，提高数据压缩率。 1234PUT /my_index/_settings&#123; "index.codec": "best_compression"&#125; ###3.3 bulk批量写入 在向elasticsearch集群写入数据时，建议尽量使用bulk接口批量写入，提高写入效率。每个bulk请求的doc数量设定区间推荐为1k~1w，具体可根据业务场景选取一个适当的数量。 例如Logstash向elasticsearch集群写数时，其中batch size是一个重要的调优参数（优化的大小需要根据doc的大小和服务器性能来确定）。如果写入过程中遇到大量EsRejectedExecutionException说明elasticsearch集群索引性能已经达到极限了。 12345POST _bulk&#123; "index" : &#123; "_index" : "test", "_type" : "type1" &#125; &#125;&#123; "field1" : "value1" &#125;&#123; "index" : &#123; "_index" : "test", "_type" : "type1" &#125; &#125;&#123; "field1" : "value2" &#125; ###3.4 调整translog同步策略 默认情况下，translog的持久化策略是，对于每个写入请求都做一次flush，刷新translog数据到磁盘上。这种频繁的磁盘IO操作是严重影响写入性能的，如果可以接受一定概率的数据丢失（这种硬件故障的概率很小），可以通过下面的命令调整 translog 持久化策略为异步周期性执行，并适当调整translog的刷盘周期。 1234567891011PUT my_index&#123; "settings": &#123; "index": &#123; "translog": &#123; "sync_interval": "5s", "durability": "async" &#125; &#125; &#125;&#125; ###3.5 适当增加refresh时间间隔elasticsearch为了提高索引性能，数据的写入采用延迟写入机制。数据首先写入内存，当操作一定时间间隔（index.refresh_interval）会统一发起一次写入磁盘的操作。即将内存中的segment数据写入磁盘，也就是这时候我们才能对数据进行访问。所以严格的说elasticsearch提供的近实时检索功能，这个延时大小就是index.refresh_interval参数（默认为1秒）决定的。 默认情况下，elasticsearch每一秒会refresh一次，产生一个新的segment，这样会导致产生的segment较多，从而segment merge较为频繁，系统开销较大。如果对数据的实时检索要求较低，可以提高refresh的时间间隔，降低系统开销。 12345678PUT my_index&#123; "settings": &#123; "index": &#123; "refresh_interval" : "30s" &#125; &#125;&#125; ###3.6 merge并发控制 elasticsearch的一个index由多个shard组成，而一个shard其实就是一个Lucene的index，它又由多个segment组成，且Lucene会不断地把一些小的segment合并成一个大的segment，这个过程被称为merge。 默认值是Math.max(1, Math.min(4, Runtime.getRuntime().availableProcessors() / 2))，当节点配置的cpu核数较高时，merge占用的资源可能会偏高，影响集群的性能，可以通过下面的命令调整某个index的merge过程的并发度： 1234PUT /my_index/_settings&#123; "index.merge.scheduler.max_thread_count": 2&#125; ###3.7 写入数据不指定_id，让elasticsearch自动产生 文档唯一标识由四个元数据字段组成：_id：文档的字符串 ID;_type：文档的类型名;_index：文档所在的索引 _uid：_type 和 _id 连接成的 type#id。 当用户指定_id写入数据时，elasticsearch会先发起查询来确定index中是否已经有相同id的doc存在，若有则先删除原有doc再写入新doc。这样每次写入时，都会耗费一定的资源做查询。如果用户写入数据时不指定id，则通过内部算法产生一个随机的id，并且保证id的唯一性，这样就可以跳过前面查询id的步骤，提高写入效率。 所以，在不需要通过id字段去重、update的使用场景中，写入不指定id可以提升写入速率。 自动生成的id，长度为20个字符，URL安全，base64编码，GUID，分布式系统并行生成时不可能会发生冲突。 12345678# 写入时指定_id，这里_id=1POST _bulk&#123; "index" : &#123; "_index" : "test", "_type" : "type1", "_id" : "1" &#125; &#125;&#123; "field1" : "value1" &#125;# 写入时不指定_idPOST _bulk&#123; "index" : &#123; "_index" : "test", "_type" : "type1" &#125; &#125;&#123; "field1" : "value1" &#125; ###3.8 使用routing Elasticsearch的路由机制与其分片机制直接相关。路由即通过哈希算法，将具有相同哈希值的文档放置到同一个主分片中。这里机制原理和通过哈希算法实现数据库负载均衡原理是相同的。 Elasticsearch有一个默认的路由算法：它会将文档的ID值作为依据将其哈希到相应的主分片上，这种算法基本上会保持所有数据在所有分片上的一个平均分布，而不会产生数据热点。 默认的Routing模式在很多情况下都是能满足需求的。但是在我们更深入地理解我们的数据的特征之后，使用自定义的Routing模式可能会带来更好的性能。 所有的文档API（get，index，delete，update和mget）都能接收一个routing参数，可以用来形成个性化文档分片映射。一个个性化的routing值可以确保相关的文档存储到同样的分片上。 参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html 对于数据量较大的index，一般会配置多个shard来分摊压力。这种场景下，一个查询会同时搜索所有的shard，然后再将各个shard的结果合并后，返回给用户。具体过程如下： 查询的请求会被发送到一个节点 接收到这个请求的节点，将这个查询广播到这个索引的每个分片上（可能是主分片，也可能是复制分片） 每个分片执行这个搜索查询并返回结果 结果在通道节点上合并、排序并返回给用户 对于高并发的小查询场景，每个分片通常仅抓取极少量数据，此时查询过程中的调度开销远大于实际读取数据的开销，且查询速度取决于最慢的一个分片。 开启routing功能后，会将routing相同的数据写入到同一个分片中（也可以是多个，由index.routingpartitionsize参数控制）。如果查询时指定routing，那么只会查询routing指向的那个分片，可显著降低调度开销，提升查询效率。 routing的使用方式如下： 123456789101112131415# 写入PUT my_index/my_type/1?routing=user1&#123; "title": "This is a document"&#125;# 查询GET my_index/_search?routing=user1,user2 &#123; "query": &#123; "match": &#123; "title": "document" &#125; &#125;&#125; ###3.9 为string类型的字段选取合适的存储方式 存为text类型的字段（string字段默认类型为text）： 做分词后存储倒排索引，支持全文检索，可以通过下面几个参数优化其存储方式： norms：用于在搜索时计算该doc的_score（代表这条数据与搜索条件的相关度），如果不需要评分，可以将其关闭。 indexoptions：控制倒排索引中包括哪些信息（docs、freqs、positions、offsets）。对于不太注重score/highlighting的使用场景，可以设为 docs来降低内存/磁盘资源消耗。 fields: 用于添加子字段。对于有sort和聚合查询需求的场景，可以添加一个keyword子字段以支持这两种功能。 12345678910111213141516171819PUT my_index&#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "title": &#123; "type": "text", "norms": false, "index_options": "docs", "fields": &#123; "raw": &#123; "type": "keyword" &#125; &#125; &#125; &#125; &#125; &#125;&#125; 存为keyword类型的字段： 不做分词，不支持全文检索。text分词消耗CPU资源，冗余存储keyword子字段占用存储空间。如果没有全文索引需求，只是要通过整个字段做搜索，可以设置该字段的类型为keyword，提升写入速率，降低存储成本。 设置字段类型的方法有两种：一是创建一个具体的index时，指定字段的类型；二是通过创建template，控制某一类index的字段类型。 123456789101112131415161718192021222324252627282930313233343536# 1. 通过mapping指定 tags 字段为keyword类型PUT my_index&#123; "mappings": &#123; "my_type": &#123; "properties": &#123; "tags": &#123; "type": "keyword" &#125; &#125; &#125; &#125;&#125;# 2. 通过template，指定my_index*类的index，其所有string字段默认为keyword类型PUT _template/my_template&#123; "order": 0, "template": "my_index*", "mappings": &#123; "_default_": &#123; "dynamic_templates": [ &#123; "strings": &#123; "match_mapping_type": "string", "mapping": &#123; "type": "keyword", "ignore_above": 256 &#125; &#125; &#125; ] &#125; &#125;, "aliases": &#123;&#125; &#125; ###3.10 查询时，使用query-bool-filter组合取代普通query 默认情况下，ES通过一定的算法计算返回的每条数据与查询语句的相关度，并通过score字段来表征。但对于非全文索引的使用场景，用户并不care查询结果与查询条件的相关度，只是想精确的查找目标数据。此时，可以通过query-bool-filter组合来让ES不计算score，并且尽可能的缓存filter的结果集，供后续包含相同filter的查询使用，提高查询效率。 12345678910111213141516171819# 普通查询POST my_index/_search&#123; "query": &#123; "term" : &#123; "user" : "Kimchy" &#125; &#125;&#125;# query-bool-filter 加速查询POST my_index/_search&#123; "query": &#123; "bool": &#123; "filter": &#123; "term": &#123; "user": "Kimchy" &#125; &#125; &#125; &#125;&#125; ###3.11 index命名按日期滚动 写入elasticsearch的数据最好通过某种方式分割，存入不同的index。常见的做法是将数据按模块、功能分类，写入不同的index，然后按照时间去滚动生成index。这样做的好处是各种数据分开管理不会混淆，也易于提高查询效率。同时index按时间滚动，数据过期时删除整个index，要比一条条删除数据效率高很多，因为删除整个index是直接删除底层文件，而delete by query是查询-标记-删除。 举例说明，假如有两个source1和source2两个数据生产者产生的数据，那么index规划可以是这样的：一类index名称是source1 + {日期}，另一类index名称是source2+ {日期}。 对于名字中的日期，可以在写入数据时自己指定精确的日期，也可以通过elasticsearch的ingest pipeline中的index-name-processor实现（会有写入性能损耗）。参考官方文档： https://www.elastic.co/guide/en/elasticsearch/reference/5.6/date-index-name-processor.html 123456789101112131415161718192021222324# module_a 类index- 创建index：PUT module_a@2018_01_01&#123; "settings" : &#123; "index" : &#123; "number_of_shards" : 3, "number_of_replicas" : 2 &#125; &#125;&#125;PUT module_a@2018_01_02&#123; "settings" : &#123; "index" : &#123; "number_of_shards" : 3, "number_of_replicas" : 2 &#125; &#125;&#125;...- 查询数据：GET module_a@*/_search ###3.12 按需控制index的分片数和副本数 分片（shard）：Elasticsearch中的一个索引（index）由多个分片（shard）组成，每个shard承载index的一部分数据。 副本（replica）：index也可以设定副本数（numberofreplicas），也就是同一个shard有多少个备份。对于查询压力较大的index，可以考虑提高副本数（numberofreplicas），通过多个副本均摊查询压力。 Elasticsearch默认副本数量为3，能提高集群的业务检索性能，但是会影响索引的写入性能（写入过程需要等待所有副本写完，才算完成一次更新写入）。 shard数量设置过多或过低都会引发一些问题： shard数量过多，则批量写入、查询请求被分割为过多的子写入、查询，导致该index的写入、查询拒绝率上升。 对于数据量较大的index，当其shard数量过小时，无法充分利用节点资源，造成机器资源利用率不高或不均衡，影响写入、查询的效率。 对于每个index的shard数量，可以根据数据总量、写入压力、节点数量等综合考量后设定，然后根据数据增长状态定期检测下shard数量是否合理，推荐方案是： 对于数据量较小（100GB以下）的index，往往写入压力查询压力相对较低，一般设置3~5个shard，副本设置为1（也就是一主一从） 。 对于数据量较大（100GB以上）的index： 一般把单个shard的数据量控制在（20GB~50GB） 让index压力分摊至多个节点：可通过index.routing.allocation.totalshardsper_node参数，强制限定一个节点上该index的shard数量，让shard尽量分配到不同节点上 综合考虑整个index的shard数量，如果shard数量（不包括副本）超过50个，就很可能引发拒绝率上升的问题，此时可考虑把该index拆分为多个独立的index，分摊数据量，同时配合routing使用，降低每个查询需要访问的shard数量。 所以有一种优化建议：在index创建写入过程中将副本数设为0，待index完成后将副本数按需量改回来，这样也可以提高效率。 3.13 增加负载均衡（查询）节点负载均衡节点需要如下配置： 12node.master: falsenode.data: false 该节点服务器即不会被选作主节点，也不会存储任何索引数据，节点只能处理路由请求，处理搜索，分发索引操作等，从本质上来说该客户节点主要作用是负载均衡。 在查询的时候，通常会涉及到从多个节点服务器上查询数据，并请求分发到多个指定的节点服务器，并对各个节点服务器返回的结果进行一个汇总处理，最终返回给客户端。 性能方面对该节点要求是内存越大越好。 ###3.14 关闭data节点服务器中的http功能 针对ElasticSearch集群中的所有data节点，不用开启http服务。将其中的配置参数这样设置： 1http.enabled: false 同时不安装head, marvel等监控插件，这样保证data节点服务器只需处理创建/更新/删除/查询索引数据等操作。http功能可以在非数据节点服务器上开启，上述相关的监控插件也安装到这些服务器上，用于监控ElasticSearch集群状态等数据信息。这样做一来出于数据安全考虑，二来出于服务性能考虑。 ###3.15 调整集群热数据分布 集群在分配数据分片的时候，如果不指定路由，分片是均匀分派到各个节点上。当节点上存在太多热点数据分片时候，我们可以通过API手动进行调整分片的存储位置。 下面的API中，indexname：需要移动的索引名称，shard：分片编号，from_node:分片的原归属节点名称，to_node:分片的移动目的节点名称。 123456curl -XPOST 'http://localhost:9200/_cluster/reroute' -d '&#123;"commands" : [&#123;"move" :&#123;"index" : "indexname", "shard" : 1,"from_node" : "nodename", "to_node" : "nodename"&#125;&#125;]&#125;' ##总结 Elasticsearch 的性能取决于很多因素，包括文档结构、文档大小、索引设置 / 映射、请求率、数据集大小和查询命中次数等等。每个优化策略都自身的优化需求场景，没有技术银弹。因此，针对需求场景进行性能测试、收集数据，然后调优非常重要。 ##参考文献 1、A Heap of Trouble: Managing Elasticsearch’s Managed Heap 链接：https://www.elastic.co/blog/a-heap-of-trouble 2、elasticsearch官方网站 链接：https://www.elastic.co/ 3、《深入理解Elasticsearch》]]></content>
      <categories>
        <category>Elasticsearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch温热集群架构]]></title>
    <url>%2F2019%2F01%2F27%2F2019-01-27-Elasticsearch%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E5%86%B7%E7%83%AD%E8%8A%82%E7%82%B9%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 温热集群整体架构 第二部分 架构配置 第三部分 架构维护相关 第四部分 基于hot-warm架构的读写分离实现 总结 参考文献及资料 背景Elasticsearch集群通常有下面两种常见组织架构： 同质集群架构。即所有数据节点会负载所有的index存储和查询，节点配置均相同。 温热集群架构。此架构中数据节点有两种类型：温节点、热节点。 特别针对日志型数据和时间序列指标类数据处理，温热架构较为常见。使用该架构通常有这样的前提：数据通常是不可更改的。每个index仅包含特定时间段的数据，用户可通过删除整个index来管理数据生命周期。 当Elasticsearch用于大量实时数据分析的场景时，推荐使用基于时间的索引然后使用三种不同类型的节点（Master, Hot-Node 和 Warm-Node）进行结构分层。 第一部分 温热集群整体架构1.1 Master 节点通常推荐每个集群运行三个专用的Master节点来提供最好的弹性。三个专用的Master节点，专门负责处理集群的管理以及加强状态的整体稳定性。Master节点不存储实际数据、不实际参与搜索以及索引操作。因此不太可能会因为垃圾回收而导致停顿。所以在服务器资源规划上，Master节点的CPU，内存以及磁盘配置可以比Data节点少很多。 建议将 discovery.zen.minimum_master_nodes setting 参数设置为2，避免集群出现脑裂。 1.2 Hot 节点Hot节点会完成集群内所有的索引工作。这些节点同时还会保存近期的一些频繁被查询的索引。由于进行索引非常耗费CPU和I/O，因此这些节点的服务器建议配置SSD存储来支撑。 1.3 Warm 节点这种类型的节点是为了处理大量的而且不经常访问的只读索引而设计的。由于这些索引是只读的，Warm节点倾向于挂载大量磁盘（普通磁盘）来替代SSD。但是CPU和内存配置建议和Hot节点保持一致。 第二部分 架构配置Hot-Warm架构实现主要有以下几点： 有3台机器作为master节点 有若干台实例tag设置为hot（hot节点），有若干实例tag设置为warm（warm节点） hot节点中只存最近查询需求较高的索引 设定定时任务每天将前一天的索引标记为warm elasticsearch根据tag将hot数据迁移到warm 2.1 实例配置2.1.1 索引级路由配置实例标签设置在 elasticsearch.yml 配置文件中，参数名称为： node.attr.box_type 1234#设置该节点的tag为hotnode.attr.box_type：hot# 设置该节点的tag为warmnode.attr.box_type：warm 也可以在启动实例时使用 ./bin/elasticsearch -Enode.attr.box_type=hot 参数指定 另外box_type名称也是自定义的，例如也可以为：zone 通过以下配置（索引路由分布策略）创建索引，索引分片只会分配写入hot实例节点。 123456PUT /logs_2016-12-26&#123; "settings": &#123; "index.routing.allocation.require.box_type": "hot" &#125;&#125; 当索引不再需要大量检索时，可以将索引迁移到warm实例节点。通过API更新索引配置如下： 123456PUT /logs_2016-12-26/_settings &#123; "settings": &#123; "index.routing.allocation.require.box_type": "warm" &#125; &#125; 2.1.2 集群路由分布策略设置对于一个物理机上多个elasticsearch实例的场景，多个实例可能是hot和warm节点。这是就会存在索引主从分片分配在同一个物理机上，这不满足数据的高可用。所以需要设置集群路由分布策略。 12"cluster.routing.allocation.awareness.attributes": "box_type""cluster.routing.allocation.awareness.force.box_type.values": "hot,warm" 集群路由分布策略高于索引级路由策略 2.2 模板设置在hot-warm架构下，需要对索引模板（index template）进行设置。如果索引模板在logstash或者beats中管理，那么索引模板需要做一些更新，包括分配过滤器。”index.routing.allocation.require.box_type” : “hot” 这个配置会使新的索引创建在Hot节点上。例如： 12345&#123; "template" : "indexname-*", "version" : 50001, "settings" : &#123; "index.routing.allocation.require.box_type": "hot" 或者给集群中的所有索引设置一个普通模板，所有新增索引首先写入hot实例节点： 12345&#123; "template" : "*", "version" : 50001, "settings" : &#123; "index.routing.allocation.require.box_type": "hot" 后续可以通过更新它的索引配置：”index.routing.allocation.require.box_type” : “warm”，完成迁移。 2.3 其他优化设置warm节点开启压缩配置。在elasticsearch.yml配置文件中： 1index.codec: best_compression warm节点合并分段。当数据迁移到Warm节点后，可以调用 _forcemerge API 来合并分段。可以节约内存, 磁盘空间以及更少的文件句柄。 第三部分 架构维护相关在日常集群维护中，我们希望重复的维护工作，能通过工具或自动化手段完成。例如如何使用Curator这个工具来自动处理这些事情。在Elasticsearch6.8版本开始，可以通过索引生命周期管理（ILM）模块完成这些自动化管理。 下面的例子中我们使用curator 4.2从Hot节点移动三天前的索引到Warm节点： 12345678910111213141516171819202122actions: 1: action: allocation description: "Apply shard allocation filtering rules to the specified indices" options: key: box_type value: warm allocation_type: require wait_for_completion: true timeout_override: continue_if_exception: false disable_action: false filters: - filtertype: pattern kind: prefix value: logstash- - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 3 最后我们可以使用curator来强制合并索引。执行优化之前要确保等待足够长的时间进行索引重新分配。你可以设置操作1中 wait_for_completion，或者修改操作2中的 unit_count 来选择4天前的索引.这样就有机会在强制合并之前完全合并。 123456789101112131415161718192: action: forcemerge description: "Perform a forceMerge on selected indices to 'max_num_segments' per shard" options: max_num_segments: 1 delay: timeout_override: 21600 continue_if_exception: false disable_action: false filters: - filtertype: pattern kind: prefix value: logstash- - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 3 注意 timeout_override 要比默认值 21600 秒大，不过它可能会更快或者慢一点，这取决于你的配置。 从Elasticsearch 5.0开始我们还可以使用 Rollover 和 shrink api 来减少分片数量，可以以更简单高效的方式来管理基于时间的索引。你可以在这个博客中找到更多细节。 3.2 数据温热迁移shell脚本日常运维中可以定时执行shell脚本完成hot数据的迁移。 12345678910111213#!/bin/bashTime=$(date -d "1 week ago" +"%Y.%m.%d")Hostname=$(hostname)arr=("cron" "messages" "secure" "tomcat" "nginx-access" "nginxtcp" "nginxerror" "windows" ".monitoring-es-6" ".monitoring-beats-6" ".monitoring-kibana-6" ".monitoring-logstash-6" "metricbeat-6.5.3")for var in $&#123;arr[@]&#125;do curl -H "Content-Type: application/json" -XPUT http://$Hostname:9200/$var-$Time/_settings?pretty -d' &#123; "settings": &#123; "index.routing.allocation.require.box_type": "cold" &#125; &#125;'done 3.3 warm数据的清理对于warm节点的数据，可以用下面的shell脚本完成历史数据的定期清理。 123456#! /bin/bash# 清理7天前的indexecho "Begin @ `date +%Y%m%d%H%M%S`"d7=$(date +%Y.%m.%d --date '7 days ago')curl -XDELETE "http://192.168.1.30:9200/logstash-$d7?pretty"echo "End @ `date +%Y%m%d%H%M%S`" 第四部分 基于hot-warm架构的读写分离实现目标：使主分片分配在SSD磁盘上，副本落在SATA磁盘上，读取时优先从副本中查询数据，SSD节点只负责写入数据。 实现步骤： 修改集群路由分配策略配置 增加集群路由配置 12"allocation.awareness.attributes": "box_type","allocation.awareness.force.box_type.values": "hot,cool" 提前创建索引 提前创建下一天的索引，索引配置如下（可写入模板中）： 1234567PUT log4x_trace_2018_08_11&#123; "settings": &#123; "index.routing.allocation.require.box_type": "hot", "number_of_replicas": 0&#125;&#125; 此操作可使索引所有分片都分配在SSD磁盘中。 修改索引路由分配策略配置 索引创建好后，动态修改索引配置 12345PUT log4x_trace_2018_08_11/_settings&#123; "index.routing.allocation.require.box_type": null, "number_of_replicas": 1&#125; 4、转为冷数据 动态修改索引配置，并取消副本数 12345PUT log4x_trace_2018_08_11/_settings&#123; "index.routing.allocation.require.box_type": "cool", "number_of_replicas": 0&#125; 总结参考文献和资料1、Elasticsearch 主节点和暖热节点 https://dongbo0737.github.io/2017/06/06/elasticsearch-hot-warm/]]></content>
      <categories>
        <category>ElasticSearch，hot-warm</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive介绍及部署]]></title>
    <url>%2F2018%2F08%2F14%2F2018-08-13-Hive%E4%BB%8B%E7%BB%8D%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[背景Hive（蜂巢）是Hadoop的组件，官方介绍为： Hive™: A data warehouse infrastructure that provides data summarization and ad hoc querying. Hive有三种部署方式（本质是Hive Metastore的三种部署方式）： Embedded Metastore Database (Derby) 内嵌模式 内嵌模式使用的是内嵌的Derby数据库来存储元数据，也不需要额外起Metastore服务。这个是默认的，配置简单，但是一次只能一个客户端连接（Derby只提供单进程存储），适用于用来实验，不适用于生产环境。 Local Metastore Server 本地元存储 采用外部数据库来存储元数据 。本地元存储不需要单独起metastore服务，用的是跟hive在同一个进程里的metastore服务 。 目前支持：Derby，Mysql，微软SQLServer，Oracle和Postgres Remote Metastore Server 远程元存储 采用外部数据库来存储元数据 。远程元存储需要单独起metastore服务，然后每个客户端都在配置文件里配置连接到该metastore服务。远程元存储的metastore服务和hive运行在不同的进程里。 远程元存储是生产环境部署方式。 本地部署过程 由于设备资源限制，没有太多机器配置类似生产环境的集群环境。所以通过docker搭建大集群环境。 搭建目标： 集群中hadoop集群由3台构成（1台master，2台slaves） Hive的元数据库使用Mysql，并且单独包裹在一个docker环境中。 环境准备准备hadoop集群环境。启docker集群： 1234CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc27312e13270 kiwenlau/hadoop:1.0 "sh -c 'service ssh …" 2 hours ago Up 2 hours hadoop-slave2f8b69885f3ef kiwenlau/hadoop:1.0 "sh -c 'service ssh …" 2 hours ago Up 2 hours hadoop-slave1439b359d230e kiwenlau/hadoop:1.0 "sh -c 'service ssh …" 2 hours ago Up 2 hours 0.0.0.0:8088-&gt;8088/tcp, 0.0.0.0:50070-&gt;50070/tcp hadoop-master Hive部署下载安装包：1234# 进入hadoop-master主机，进入hadoop目录：/use/local/hadoopwget http://apache.claz.org/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gztar -zxvf apache-hive-2.3.3-bin.tar.gzmv apache-hive-2.3.3-bin hive 配置Hive环境变量：123456vi /etc/profile#hiveexport HIVE_HOME=/usr/local/hadoop/hivePATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$PATH# 生效环境变量source /etc/profile 调整Hive的配置文件：12345# 进入hive 配置文件目录：cd confcp hive-default.xml.template hive-site.xml# 修改配置文件vim hive-site.xml 新建HDFS分布式文件目录：12345678# hadoop已经设置好环境变量，新建下面目录hadoop fs -mkdir -p /user/hive/warehouse hadoop fs -mkdir -p /user/hive/tmp hadoop fs -mkdir -p /user/hive/log # 设置目录权限hadoop fs -chmod -R 777 /user/hive/warehouse hadoop fs -chmod -R 777 /user/hive/tmp hadoop fs -chmod -R 777 /user/hive/log 可以用下面命令进行检查： 12345root@hadoop-master:/usr/local/hadoop/hive/conf# hadoop fs -ls /user/hiveFound 3 itemsdrwxrwxrwx - root supergroup 0 2018-08-14 07:34 /user/hive/logdrwxrwxrwx - root supergroup 0 2018-08-14 07:34 /user/hive/tmpdrwxrwxrwx - root supergroup 0 2018-08-14 07:34 /user/hive/warehouse 修改配置文件（hive-site.xml）：hive数据仓库数据路径：/user/hive/warehouse 需要使用hdfs新建文件目录。 12345&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt; 配置查询日志存放目录： 12345&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/user/hive/log/hadoop&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt; 数据库JDBC连接配置（172.18.0.5为mysql的ip地址，暴露3306端口）： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;&lt;/property&gt; 数据库驱动： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt; 数据库用户名： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt; 数据库密码： 12345&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt; 配置Hive临时目录： 1mkdir /usr/local/hadoop/hive/tmp 并在 hive-site.xml 中修改: 把${system:java.io.tmpdir} 改成真实物理绝对路径 /usr/local/hadoop/hive/tmp 把 ${system:user.name} 改成 ${user.name} 可以在外面编辑好配置文件，拷贝进docke： 12&gt; docker cp hive-site.xml 439b359d230e:/usr/local/hadoop/hive/conf/hive-site.xml&gt; 配置hive-env.sh文件：尾部加上下面的配置（或者修改注释部分的配置亦可）： 123HADOOP_HOME=/usr/local/hadoopexport HIVE_CONF_DIR=/usr/local/hadoop/hive/confexport HIVE_AUX_JARS_PATH=/usr/local/hadoop/hive/lib 配置Mysql启mysql容器，容器名：first-mysql，使用和hadoop一个桥接网络hadoop，密码为123456 1docker run --name first-mysql --net=hadoop -p 3306:3306 -e MYSQL\_ROOT\_PASSWORD=123456 -d mysql:5.7 回显： 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES84ae224cee53 mysql:5.7 "docker-entrypoint.s…" 32 minutes ago Up 32 minutes 0.0.0.0:3306-&gt;3306/tcp first-mysql 在Hadoop-master中配置mysql客户端（用来访问mysql服务器）： 1apt-get install mysql-client-core-5.6 测试远程连接： 1mysql -h172.18.0.5 -P3306 -uroot -p123456 新建数据库，数据库名为hive： 12mysql&gt; CREATE DATABASE hive;Query OK, 1 row affected (0.00 sec) 初始化（Hive主机上）： 12cd /usr/local/hadoop/hive/bin./schematool -initSchema -dbType mysql 回显： 12345root@hadoop-master:/usr/local/hadoop/hive/bin# ./schematool -initSchema -dbType mysqlSLF4J: Class path contains multiple SLF4J bindings.。。。（略）schemaTool completed# 初始化成功 下载配置mysql驱动包，放在Hive的lib路径下面： 12cd /usr/local/hadoop/hive/libwget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar 启动Hive做完上面准备工作后，开始启动hive： 12345678910root@hadoop-master:/usr/local/hadoop/hive/bin# ./hiveSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/hadoop/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Logging initialized using configuration in jar:file:/usr/local/hadoop/hive/lib/hive-common-2.3.3.jar!/hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt; 最后进入hive的命令界面。 踩坑备注1、Hive提示SSL连接警告 1Tue Aug 14 10:53:12 UTC 2018 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 虽然Hive SQL执行成功，但是报上面的错误。产生的原因是使用JDBC连接MySQL服务器时为设置useSSL参数 。 解决办法：javax.jdo.option.ConnectionURL 配置的value值进行调整，设置useSSL=false ，注意xml中的语法。 12345678&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.18.0.5:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt; 重启Hive，不再有警告。 远程部署对于远程部署需要单独启metastore服务，具体需要调整下面的配置文件（hive-site.xml）： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://hadoop-master:9083&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动metastore服务： 1nohup hive --service metastore &amp; 当然这属于简单方式将Hive都扎堆部署在一个容器中。可以在集群其他几点启metastore服务，提升架构的高可用性，避免单点问题。 参考文献1、Apache Hive-2.3.0 快速搭建与使用，https://segmentfault.com/a/1190000011303459 2、Hive提示警告SSL，https://blog.csdn.net/u012922838/article/details/73291524]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Minikube上运行Flink集群]]></title>
    <url>%2F2018%2F06%2F25%2F2019-04-23-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CFlink%E9%9B%86%E7%BE%A4%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 部署准备 第二部分 验证 总结 参考文献及资料 背景第一部分 部署准备首先当然需要部署minikube集群。启动minikube集群： 1234567891011# minikube startStarting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file. 上面的回显表明minikube已经启动成功。执行下面网络配置： 1# minikube ssh 'sudo ip link set docker0 promisc on' 第二部分 部署Flink集群一个基本的Flink集群运行在minikube需要三个组件： Deployment/Job：运行 JobManager Deployment for a pool of TaskManagers Service exposing the JobManager’s REST and UI ports 1.1 创建命名空间12# kubectl create -f namespace.yamlnamespace/flink created 其中namespace.yaml文件为： 123456kind: NamespaceapiVersion: v1metadata: name: flink labels: name: flink 查询minikube集群的的命名空间： 12345# kubectl get namespacesNAME STATUS AGEflink Active 1mkube-public Active 254dkube-system Active 254d 1.2 集群组件资源定义1.2.1 启动flink-jobmanager组件Job Manager 服务是Flink集群的主服务，使用jobmanager-deployment.yaml创建。 123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: flink-jobmanager namespace: flinkspec: replicas: 1 template: metadata: labels: app: flink component: jobmanager spec: containers: - name: jobmanager image: flink:latest args: - jobmanager ports: - containerPort: 6123 name: rpc - containerPort: 6124 name: blob - containerPort: 6125 name: query - containerPort: 8081 name: ui env: - name: JOB_MANAGER_RPC_ADDRESS value: flink-jobmanager 1.2.2 启动flink-taskmanager组件使用taskmanager-deployment.yaml创建。 12345678910111213141516171819202122232425262728apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: flink-taskmanager namespace: flinkspec: replicas: 2 template: metadata: labels: app: flink component: taskmanager spec: containers: - name: taskmanager image: flink:latest args: - taskmanager ports: - containerPort: 6121 name: data - containerPort: 6122 name: rpc - containerPort: 6125 name: query env: - name: JOB_MANAGER_RPC_ADDRESS value: flink-jobmanager 1.2.3 启用flink服务使用jobmanager-service.yaml创建服务，并且将端口映射到minikube主机响应端口。 1234567891011121314151617181920212223apiVersion: v1kind: Servicemetadata: name: flink-jobmanager namespace: flinkspec: type: NodePort ports: - name: rpc port: 6123 nodePort: 30123 - name: blob port: 6124 nodePort: 30124 - name: query port: 6125 nodePort: 30125 - name: ui port: 8081 nodePort: 30081 selector: app: flink component: jobmanager 1.3 端口映射到虚拟机主机minikube虚拟机停止的情况下的端口转发命令如下： 1234# VBoxManage modifyvm "minikube" --natpf1 "30123_6123,tcp,,6123,,30123"# VBoxManage modifyvm "minikube" --natpf1 "30124_6123,tcp,,6124,,30124"# VBoxManage modifyvm "minikube" --natpf1 "30125_6125,tcp,,6125,,30125"# VBoxManage modifyvm "minikube" --natpf1 "30081_8081,tcp,,8081,,30081" 格式说明：vboxmanage modifyvm 宿主机名称 natpf “映射别名,tcp,,本机端口,,虚拟机端口” minikube虚拟机运行的情况下的端口转发命令如下： 1234# VBoxManage controlvm "minikube" --natpf1 "30123_6123,tcp,,6123,,30123"# VBoxManage controlvm "minikube" --natpf1 "30124_6123,tcp,,6124,,30124"# VBoxManage controlvm "minikube" --natpf1 "30125_6125,tcp,,6125,,30125"# VBoxManage controlvm "minikube" --natpf1 "30081_8081,tcp,,8081,,30081" 格式说明：vboxmanage controlvm 宿主机名称 natpf “映射别名,tcp,,本机端口,,宿主机端口” 另外如果要删除上面转发规则： vboxmanage controlvm 宿主机名称 natpf delete 映射别名 vboxmanage modifyvm 宿主机名称 natpf delete 映射别名 第三部分 验证3.1 minikube控制台界面为了是主机局域网类服务器都能访问minikube控制台，需要将端口映射出去。 1# VBoxManage modifyvm "minikube" --natpf1 "kubedashboard,tcp,,30000,,30000" 3.2 Flink控制台 总结（1）部署前提前拉取镜像到本地镜像库。 （2）需要将服务端口映射到本地机器端口，供局域网服务访问，为后续访问Flink提供方便。 参考文献1、Kubernetes Setup ：https://ci.apache.org/projects/flink/flink-docs-stable/ops/deployment/kubernetes.html 2、How to Deploy Flink Cluster &amp; Flink-exporter in Kubernetes Cluster：https://medium.com/pharos-production/how-to-deploy-flink-cluster-flink-exporter-in-kubernetes-cluster-48e24b440446 3、melentye/flink-kubernetes https://github.com/melentye/flink-kubernetes 4、Set up Ingress on Minikube with the NGINX Ingress Controller https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于正则表达式使用和总结]]></title>
    <url>%2F2018%2F06%2F25%2F2018-08-06-%E5%85%B3%E4%BA%8E%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景正则表达式（regular Expression）是计算机中的概念。regular这里是规则、规律的意思，字面翻译指的是：规则的表达式。正则表达式主要用来处理字符串的工具，拥有自己独特的语法。 计算机常用语言均支持正则表达式，语法都是相同的，区别在于不同的语言支持的语法略有差异。 在数据科学实践中，特别是文本数据分析中，由于实际文本数据的纷繁复杂，正则表达式就成为文本检索分析的重要工具。本篇介绍以python语言背景来介绍正则表达式的实现。 https://jex.im/regulex/#!flags=&amp;re=%5E(a%7Cb)*%3F%24 第一章 正则表达式字符匹配正则表达式可以理解为一种字符串模式识别，识别对象有两个：子字符串、子字符串位置。 python中内置正则表达式包，直接import re。比如下面实现一个精确匹配： 1234import repattern = re.compile('hello')print(pattern.findall('hello'))# ['hello'] 1.1 两种模糊匹配正则表达式的强大主要实现模糊匹配，主要有：横向模糊匹配、纵向模糊匹配。 1、横向模糊匹配定义：正则匹配的字符串的长度不是固定的。 举一个栗子： 我们需要匹配字符串中具有特有的模式子串：第一个字符是’a’，最后一个字符是‘c’，中间部分长度可变，但是均为’b’，数量范围为[2,3]。具体代码实现如下： 1234import repattern = re.compile('ab&#123;2,3&#125;c')print(pattern.findall('abbcabbbbcaaabcaabbbc'))# ['abbc', 'abbbc'] 注意findall函数是全局匹配，即按顺序遍历字符串进行匹配。 2、纵向模糊匹配定义：正则匹配的字符串，存在一些位置的值不固定。 举个栗子： 我们需要匹配字符串具有的模式为：第一个和最后一个字符为‘a’、’b’，中间部分长度为1，但是字符可选，备选集合为：{b,c,d} 。代码实现如下： 1234import repattern = re.compile('a[bcd]e')print(pattern.findall('abeaaacdeade'))# ['abe', 'ade'] 正则表达式匹配的主要模式就是横向模糊、纵向模糊及两种模式的组合。 1.2 字符组 这里字符组其实匹配的是单个字符，并不是“组”哈。 范围表示方法例如在纵向模糊中我们举的例子，用[b,c,d]表示这三个字符的其中一个。在实际中如果备选集合比较大怎么办？有简略的表达式，比如： 123# [0,1,2,3,4,5,6,7,8,9] 等价表示为[0-9]# [a,b,c,d,e,f,g,h] 等价表示为[a-h]# 还可以各种类型字符合并写：[0-9a-h] 上面范围表达式，会被自动解析为连续字符。 有些杠精要问了假如备选集合中有横杆字符’-‘字符咋办，比如{‘a’,’-‘,’c’}，这时候我们不能写写成这样了。要这样写：[-ac]、[ac-]、[a\-c]，注意这里的转义符: ‘\’，后续我们再介绍。 排除字符组（反义字符组）在纵向模糊匹配中，还有一种匹配模式：某个字符除了在排除集中的字符，可以是任意字符。 1# 例如[^abc]，表示一个除"a"、"b"、"c"之外的任意一个字符。字符组的第一位放^（脱字符），表示求反的概念。 最后我们介绍简写形式，即解析语义约定俗成的简短写法。 \d就是[0-9]。表示是一位数字。记忆方式：其英文是digit（数字）。 \D就是[^0-9]。表示除数字外的任意字符。 \w就是[0-9a-zA-Z_]。表示数字、大小写字母和下划线。记忆方式：w是word的简写，也称单词字符。 \W是[^0-9a-zA-Z_]。非单词字符。 \s是[ \t\v\n\r\f]。表示空白符，包括空格、水平制表符、垂直制表符、换行符、回车符、换页符。记忆方式：s是space character的首字母。 \S是[^ \t\v\n\r\f]。 非空白符。 .就是[^\n\r\u2028\u2029]。通配符，表示几乎任意字符。换行符、回车符、行分隔符和段分隔符除外。 1.3 量词简写形式量词即连续重复的字符模式。 {m,n} 表示至少出现次数范围为：大于等于m，小于等于n。 {m,} 表示至少出现m次，即{m,+infinity}。{m} 等价于{m,m}，表示出现m次。 ? 等价于{0,1}，表示出现或者不出现。记忆方式：问号的意思表示，有吗？ +等价于{1,}，表示出现至少一次，即{1,+infinity}。 * 等价于{0,}，表示出现任意次，即{0,+infinity}。 贪婪匹配和惰性匹配先看一个栗子： 1234import repattern = re.compile('\d&#123;2,5&#125;')print(pattern.findall('123a1234b12345c123456d'))# ['123', '1234', '12345', '12345'] 上面的正则表达式会匹配连续出现的数值型字符串，长度范围为：2，3，4，5。这种模式是贪婪模式，即尽可能的匹配到最长范围。 而对应的由惰性匹配，即尽可能少的匹配： 1234import repattern = re.compile('\d&#123;2,5&#125;?')print(pattern.findall('123a1234b12345c123456d'))# ['12', '12', '34', '12', '34', '12', '34', '56'] 量词组通过问号实现惰性匹配，还有其他情况： {m,n}? {m,}? ?? +? ? 记忆技巧：问号的含义是反问：还不知足吗？意思就是要惰性一点。。。。 1.4 多选分支一个模式可以实现横向和纵向模糊匹配。而多选分支可以支持多个子模式任选其一。 具体形式如下：(p1|p2|p3)，其中p1、p2和p3是子模式，用|（管道符）分隔，表示其中任何之一。 例如要匹配”good”和”nice”可以使用/good|nice/。测试如下： 1234import repattern = re.compile('goodby|good')print(pattern.findall('googbygood'))# ['good'] 也就是说，分支结构也是惰性的，即当前面的匹配上了，后面的就不再尝试了。 1.5 案例分析 匹配时间字符串 例如需要匹配字符串中24小时制的时间字符串，比如：10:30，23:59 12345import re# 错误：pattern = re.compile('\d&#123;2&#125;:\d&#123;2&#125;')pattern = re.compile('[01]\d:[0-5]\d|2[0-3]:[0-5]\d')print(pattern.findall('ahjs10:21s7:2sd23:59wi01:01iis09:02w'))# ['10:21', '23:59', '01:01', '09:02'] 第二章 正则表达式位置匹配正则表达式属于模式匹配，主要两种匹配目标：字符和位置。 字符串的位置位置定义：相邻字符之间的位置。 锚字符Python中的锚字符 ^(脱字符)和$(美元符) ^(脱字符)，用来匹配字符串开头。多行模式下，匹配每行的开头位置。 $(美元符)，用来匹配字符串结尾。多行模式下，匹配每行的结尾位置。 举个栗子： 第三章 正则表达式括号的作用第四章 正则表达式回溯原理第五章 正则表达式的拆分第六章 正则表达式的构建第七章 正则表达式的性能调优第八章 Python的正则表达式处理函数写在最后参考文献1、]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-聚类算法总结]]></title>
    <url>%2F2018%2F06%2F25%2F2018-06-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景根据事物的特征差异，对事物进行分类是一个基本问题。这个问题在数据科学中进行抽象：（1）“特征”指的是事物对应的特征向量，（2）“差异”对应的为特征向量之间的距离（度量）。 对于这个基本问题，根据已知数据是否具有标签，数据科学中使用两种不同的学习方法来处理： 分类问题。样本抽样数据具有标签信息。监督学习。 聚类问题。样本抽样数据不具有标签信息。无监督学习。 形式上，抽样训练数据集$D={x_1,x_2,…,x_N}$ ，包含了$N$ 个样本点，每个样本点具有$m$维的特征向量$x_i=(x_{i,1},x_{i,2},…,x_{i,m})$ 。聚类算法目标将训练集$D$ 划分为若干个子集${C_i}_{i=1}^K$ 。 那么很自然有下面两个问题： （1）怎样分类能区别样本点的差异？使得相似样本点属于同一个分类子集。 （2）怎样的分类方法是最优的？即量化评价分类方法。 对于第一个问题，首先我们要量化“相似”这个概念。这样我们需要对数据集（点集）嵌入到带有度量的空间中，用特征向量的空间度量来刻画相似，比如常用的度量有欧几里得距离。 那么又引出另外一个问题：空间度量有很多，什么度量是最优的？ 对于数据集的划分 聚类算法 SKLearn数据包数据工程实现中，常用python包有：sklearn，对于聚类算法有很好的封装。接下来介绍的聚类算法，工程实现上主要使用sklearn。 算法集原型聚类K-means算法K-mearn算法应该是聚类算法中名气最大的，几乎是聚类算法的同义词。 二分K均值算法学习向量量化LVQ 高斯混合聚类 密度聚类层次聚类### 参考文献1、数据科学家需要了解的5种聚类算法。https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68 2、sklearn 聚类算法。http://sklearn.apachecn.org/cn/0.19.0/modules/clustering.html]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Cloudera Quickstart Docker镜像快速部署hadoop集群]]></title>
    <url>%2F2018%2F06%2F25%2F2019-04-30-%E4%BD%BF%E7%94%A8Cloudera%20Quickstart%20Docker%E9%95%9C%E5%83%8F%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2hadoop%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Docker镜像准备 第二部分 运行容器 第三部分 cloudera-manager管理 第四部分 组件使用测试 第五部分 总结 参考文献及资料 背景通常在个人笔记本上部署Hadoop测试集群（含生态圈各组件）是个很耗时的工作。Cloudera公司提供一个快速部署的Docker镜像，可以快速启动一个测试集群。 测试环境为Ubuntu服务器 第一部分 Docker镜像准备首先本机需要部署有docker环境，如果没有需要提前部署。 1.1 拉取Docker镜像可以从DockerHub上拉取cloudera/quickstart镜像。 镜像项目地址为：https://hub.docker.com/r/cloudera/quickstart 1# docker pull cloudera/quickstart:latest 如果不具备联网环境，可以通过镜像介质包安装。介质可以在官网（需要注册用户）下载：https://www.cloudera.com/downloads/quickstart_vms/5-13.html 由于墙的原因下载会很慢 123# wget https://downloads.cloudera.com/demo_vm/docker/cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz# tar xzf cloudera-quickstart-vm-5.13.0-0-beta-docker.tar.gz# docker import - cloudera/quickstart:latest &lt; cloudera-quickstart-vm-5.13.0-0-beta-docker/*.tar 1.2 检查镜像库12# docker images|grep clouderacloudera/quickstart latest 4239cd2958c6 3 years ago 6.34GB 说明镜像准备好了，下面基于镜像启动容器。 第二部分 运行容器2.1 使用镜像启动容器启动CDH集群的命令格式为： 1# docker run --hostname=quickstart.cloudera --privileged=true -t -i [OPTIONS] [IMAGE] /usr/bin/docker-quickstart 官方提示的参数介绍如下： Option Description –hostname=quickstart.cloudera Required: Pseudo-distributed configuration assumes this hostname.容器主机名（/etc/hosts中指定hostname）。 –privileged=true Required: For HBase, MySQL-backed Hive metastore, Hue, Oozie, Sentry, and Cloudera Manager.这是Hbase组件需要的模式。 -t Required: Allocate a pseudoterminal. Once services are started, a Bash shell takes over. This switch starts a terminal emulator to run the services. -i Required: If you want to use the terminal, either immediately or connect to the terminal later. -p 8888 Recommended: Map the Hue port in the guest to another port on the host.端口映射参数。 -p [PORT] Optional: Map any other ports (for example, 7180 for Cloudera Manager, 80 for a guided tutorial). -d Optional: Run the container in the background.容器后台启动。 –name 容器的名字 -v host_path:container_path 主机上目录挂载到容器中目录上，主机上该放入任何东西，Docker容器中对于目录可以直接访问。 当然还可以自定义其他docker启动参数。最后启动命令整理为： 1234567docker run -t -i -d \--name cdh \--hostname=quickstart.cloudera \--privileged=true \-v /data/CDH:/src \-p 8020:8020 -p 8042:8042 -p 8022:8022 -p 7180:7180 -p 21050:21050 -p 50070:50070 -p 50075:50075 -p 50010:50010 -p 50020:50020 -p 8890:8890 -p 60010:60010 -p 10002:10002 -p 25010:25010 -p 25020:25020 -p 18088:18088 -p 8088:8088 -p 19888:19888 -p 7187:7187 -p 11000:11000 -p 10000:10000 -p 88:88 -p 8888:8888 cloudera/quickstart \/bin/bash -c '/usr/bin/docker-quickstart' Cloudera 本身的 manager 是 7180 端口，提前配置端口映射。 启动容器我们使用了-d后台启动参数，如果没有指定后台启动，终端将自动连接到容器，退出shell后容器会中止运行（可以通过使用Ctrl + P + Q命令退出，这样容器会继续保持运行）。 对于已经后台运行的容器，我们使用下面的命令进入容器shell： 1# docker attach [CONTAINER HASH] 建议使用下面的命令： 1# docker exec -it 容器名 /bin/bash 2.2 时钟同步问题容器内部使用的时间时区为UTC，和主机（宿主机通常为CST（东八区））不同时区，会提示时钟同步问题。解决的办法是： 在环境变量中添加时区变量,并source生效: 123$ vi /etc/profile文件末尾添加一行：TZ='Asia/Shanghai'; export TZ$ source /etc/profile 最后启动时钟同步服务： 12345[root@quickstart home]# service ntpd startStarting ntpd: [ OK ]# 检查服务状态[root@quickstart home]# service ntpd statusntpd (pid 13536) is running... 这样完成时钟同步。 2.3 使用集群服务这样集群的大部分服务组件均可使用。 第三部分 cloudera-manager管理3.1 启动管理服务CDH在该镜像中提供cloudera-manager组件，用户集群web管理界面，可以通过下面的命令启动。 需要注意的是，启动后CDH会停止其他组件服务。 12345678910111213141516171819202122232425#./home/cloudera/cloudera-manager --express [--force][QuickStart] Shutting down CDH services via init scripts...kafka-server: unrecognized serviceJMX enabled by defaultUsing config: /etc/zookeeper/conf/zoo.cfg[QuickStart] Disabling CDH services on boot...error reading information on service kafka-server: No such file or directory[QuickStart] Starting Cloudera Manager server...[QuickStart] Waiting for Cloudera Manager API...[QuickStart] Starting Cloudera Manager agent...[QuickStart] Configuring deployment...Submitted jobs: 14[QuickStart] Deploying client configuration...Submitted jobs: 16[QuickStart] Starting Cloudera Management Service...Submitted jobs: 24[QuickStart] Enabling Cloudera Manager daemons on boot...________________________________________________________________________________Success! You can now log into Cloudera Manager from the QuickStart VM's browser: http://quickstart.cloudera:7180 Username: cloudera Password: cloudera 集群控制台的地址为：http://quickstart.cloudera:7180，需要注意的是这里quickstart.cloudera是主机名，需要客户端hosts中配置，否则使用实IP或者容器端口映射后使用宿主机IP（例如：192.168.31.3）。 用户名和密码为：cloudera/cloudera,登录界面如下： 登录后，下图是集群控制台： 从管理界面上可以看到除了主机和 manager ，其他服务组件均未启动。 3.2 启动集群组件服务在控制台上，我们按照顺序启动HDFS、Hive、Hue、Yarn服务。 如果服务启动异常，可以尝试重启服务组件。注意需要先启动HDFS后启动Hive，否则需要重启Hive。 第四部分 组件使用测试4.1 HDFS组件使用我们使用HDFS的Python API与集群hdfs文件系统进行交互测试： 4.1.1 查看文件系统123456from hdfs.client import Clientclient = Client("http://192.168.31.3:50070", root="/", timeout=100)print(client.list("/"))# ['benchmarks', 'hbase', 'tmp', 'user', 'var']# 返回一个list记录主目录 4.1.2 上传新增文件注意这里需要在宿主机（客户端机器）配置hosts文件： 1192.168.31.3 quickstart.cloudera 然后执行： 123client.upload("/tmp", "/root/jupyter/nohup.out")# '/tmp/nohup.out'# 返回路径信息 4.1.3 下载hdfs文件12client.download("/tmp/nohup.out", "/tmp")# 返回路径'/tmp/nohup.out' 第五部分 启用安全模式（Kerberos）默认情况下集群是非安全模式，如果测试需要安全集群模式，可以在菜单：管理-安全中开始Kerberos。需要注意的是需要在容器中安装Kerberos。安装步骤如下： 注释到无关源（docker容器执行）； 12# cd /etc/yum.repos.d# mv cloudera-manager.repo cloudera-manager.repo.bak 部署yum源（宿主机执行） 12# wget https://www.xmpan.com/Centos-6-Vault-Aliyun.repo# docker cp Centos-6-Vault-Aliyun.repo 容器名:/etc/yum.repos.d 然后安装Kerberos（docker容器执行） 1./home/cloudera/kerberos 等待安装完成即可。 接下来就是配置kerberos，主要涉及的配置文件有： 123/etc/krb5.conf/var/kerberos/krb5kdc/kdc.conf/var/kerberos/krb5kdc/kdc.conf 主要调整是将EXAMPLE.COM调整为HADOOP.COM。另外配置文件krb5.conf需要个性化指定KDC服务器： 1234HADOOP.COM = &#123;kdc = 服务器hostname或者IPadmin_server = 服务器hostname或者IP&#125; 然后创建Kerberos数据库： 1$ kdb5_util create –r HADOOP.COM -s 创建Kerberos管理账户： 12$ kadmin.localkadmin.local: addprinc admin/admin@HADOOP.COM 最后启动服务： 1234[root@quickstart cloudera]# service krb5kdc startStarting Kerberos 5 KDC: [ OK ][root@quickstart cloudera]# service kadmin startStarting Kerberos 5 Admin Server: [ OK ] 查看票据： 123456789[root@quickstart cloudera]# kinit admin/admin@HADOOP.COMPassword for admin/admin@HADOOP.COM: [root@quickstart cloudera]# klistTicket cache: FILE:/tmp/krb5cc_0Default principal: admin/admin@HADOOP.COMValid starting Expires Service principal03/16/21 16:30:57 03/17/21 16:30:57 krbtgt/HADOOP.COM@HADOOP.COM renew until 03/16/21 16:30:57 生成keytab文件： 123[root@quickstart keytab]# kadmin.localAuthenticating as principal admin/admin@HADOOP.COM with password.kadmin.local: xst -norandkey -k admin.keytab admin/admin@HADOOP.COM 这样就生成了admin.keytab文件。然后使用下面的命令测试一下： 1kinit -kt admin.keytab admin/admin@HADOOP.COM 第六部分 总结1、Cloudera 的 docker 版本分成两部分启动。(1)启动各组件启动,使用命令为： /usr/bin/docker-quickstart，(2) 启动Cloudera manager 管理服务，启动命令为：/home/cloudera/cloudera-manager。docker启动时选择启动一项。 参考文献及材料1、cloudera/quickstart镜像地址：https://hub.docker.com/r/cloudera/quickstart 2、cloudera/quickstart镜像部署指引：https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html 3、利用 Docker 搭建单机的 Cloudera CDH 以及使用实践]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Minikube上运行Spark集群]]></title>
    <url>%2F2018%2F06%2F25%2F2018-08-06-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CSpark%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[[TOC] 背景​ Spark2.3版本开始支持使用spark-submit直接提交任务给Kubernetes集群。执行机制原理： Spark创建一个在Kubernetes pod中运行的Spark驱动程序。 驱动程序创建执行程序，这些执行程序也在Kubernetes pod中运行并连接到它们，并执行应用程序代码。 当应用程序完成时，执行程序窗格会终止并清理，但驱动程序窗格会保留日志并在Kubernetes API中保持“已完成”状态，直到它最终被垃圾收集或手动清理。 第一部分 环境准备1.1 minikube虚拟机准备由于spark集群对内存和cpu资源要求较高，在minikube启动前，提前配置较多的资源给虚拟机。 当minikube启动时，它以单节点配置开始，默认情况下占用1Gb内存和2CPU内核，但是，为了运行spark集群，这个资源配置是不够的，而且作业会失败。 12345# minikube config set memory 8192These changes will take effect upon a minikube delete and then a minikube start# minikube config set cpus 2These changes will take effect upon a minikube delete and then a minikube start 或者用下面的命令启集群 1# minikube start --cpus 2 --memory 8192 1.2 Spark环境准备第一步 下载saprk2.3 1# wget http://apache.mirrors.hoobly.com/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz 解压缩： 1# tar xvf spark-2.3.0-bin-hadoop2.7.tgz 制作docker镜像 12# cd spark-2.3.0-bin-hadoop2.7# docker build -t rongxiang/spark:2.3.0 -f kubernetes/dockerfiles/spark/Dockerfile . 查看镜像情况： 123# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZErongxiang1986/spark 2.3.0 c5c806314f25 5 days ago 346MB 登录docker 账户： 12345# docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: Password: Login Succeeded 将之前build好的镜像pull到docker hub上： 1# docker push rongxiang1986/spark:2.3.0 注意这里的格式要求（我踩坑了）：docker push 注册用户名/镜像名 在https://hub.docker.com/上查看，镜像确实push上去了。 第二部分 提交Spark作业2.1 作业提交提前配置serviceaccount信息。 1234# kubectl create serviceaccount sparkserviceaccount/spark created# kubectl create clusterrolebinding spark-role --clusterrole=admin --serviceaccount=default:spark --namespace=defaultclusterrolebinding.rbac.authorization.k8s.io/spark-role created 提交作业： 12345678910# ./spark-submit \--master k8s://https://192.168.99.100:8443 \--deploy-mode cluster \--name spark-pi \--class org.apache.spark.examples.SparkPi \--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \--conf spark.kubernetes.authenticate.executor.serviceAccountName=spark \--conf spark.executor.instances=2 \--conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0 \local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar 提交命令的参数含义分别是： --class：应用程序的入口点（命令中使用：org.apache.spark.examples.SparkPi）； --master：Kubernetes集群的URL（k8s://https://192.168.99.100:8443）； --deploy-mode：驱动程序部署位置（默认值：客户端），这里部署在集群中； --conf spark.executor.instances=2：运行作业启动的executor个数； --conf spark.kubernetes.container.image=rongxiang1986/spark:2.3.0：使用的docker镜像名称； local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar：应用程序依赖jar包路径； 注意：目前deploy-mode只支持cluster模式，不支持client模式。 Error: Client mode is currently not supported for Kubernetes. 作业运行回显如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677782018-08-12 15:51:17 WARN Utils:66 - Your hostname, deeplearning resolves to a loopback address: 127.0.1.1; using 192.168.31.3 instead (on interface enp0s31f6)2018-08-12 15:51:17 WARN Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address2018-08-12 15:51:18 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: N/A start time: N/A container images: N/A phase: Pending status: []2018-08-12 15:51:18 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: N/A container images: N/A phase: Pending status: []2018-08-12 15:51:18 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: 2018-08-12T07:51:18Z container images: rongxiang1986/spark:2.3.0 phase: Pending status: [ContainerStatus(containerID=null, image=rongxiang1986/spark:2.3.0, imageID=, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=null, waiting=ContainerStateWaiting(message=null, reason=ContainerCreating, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]2018-08-12 15:51:18 INFO Client:54 - Waiting for application spark-pi to finish...2018-08-12 15:51:51 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: 2018-08-12T07:51:18Z container images: rongxiang1986/spark:2.3.0 phase: Running status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=true, restartCount=0, state=ContainerState(running=ContainerStateRunning(startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), terminated=null, waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]2018-08-12 15:51:57 INFO LoggingPodStatusWatcherImpl:54 - State changed, new state: pod name: spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver namespace: default labels: spark-app-selector -&gt; spark-8be4d909d85148bc9f1f91d511c275c6, spark-role -&gt; driver pod uid: 7f6dd84d-9e04-11e8-b58f-080027b3a6c0 creation time: 2018-08-12T07:51:18Z service account name: spark volumes: spark-token-rzrgk node name: minikube start time: 2018-08-12T07:51:18Z container images: rongxiang1986/spark:2.3.0 phase: Succeeded status: [ContainerStatus(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, image=rongxiang1986/spark:2.3.0, imageID=docker-pullable://rongxiang1986/spark@sha256:3e93a2d462679015a9fb7d723f53ab1d62c5e3619e3f1564d182c3d297ddf75d, lastState=ContainerState(running=null, terminated=null, waiting=null, additionalProperties=&#123;&#125;), name=spark-kubernetes-driver, ready=false, restartCount=0, state=ContainerState(running=null, terminated=ContainerStateTerminated(containerID=docker://d43089c8340affc4534f796b94a90ae080670c36c095176575fbeebacaab648e, exitCode=0, finishedAt=Time(time=2018-08-12T07:51:57Z, additionalProperties=&#123;&#125;), message=null, reason=Completed, signal=null, startedAt=Time(time=2018-08-12T07:51:51Z, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;), waiting=null, additionalProperties=&#123;&#125;), additionalProperties=&#123;&#125;)]2018-08-12 15:51:57 INFO LoggingPodStatusWatcherImpl:54 - Container final statuses: Container name: spark-kubernetes-driver Container image: rongxiang1986/spark:2.3.0 Container state: Terminated Exit code: 02018-08-12 15:51:57 INFO Client:54 - Application spark-pi finished.2018-08-12 15:51:57 INFO ShutdownHookManager:54 - Shutdown hook called2018-08-12 15:51:57 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-6dd1c204-4ad7-40c4-b47f-a34f18e1995d 2.2 日志查询可以通过命令查看容器执行日志，或者通过kubernetes-dashboard提供web界面查看。 1# kubectl logs spark-pi-709e1c1b19813e7cbc1aeff45200c64e-driver 1234567891011121314152018-08-12 07:51:57 INFO DAGScheduler:54 - Job 0 finished: reduce at SparkPi.scala:38, took 0.576528 sPi is roughly 3.13367566837834182018-08-12 07:51:57 INFO AbstractConnector:318 - Stopped Spark@9635fa&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:4040&#125;2018-08-12 07:51:57 INFO SparkUI:54 - Stopped Spark web UI at http://spark-pi-7314d819cd3730b4bf7d02bfedd21373-driver-svc.default.svc:40402018-08-12 07:51:57 INFO KubernetesClusterSchedulerBackend:54 - Shutting down all executors2018-08-12 07:51:57 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint:54 - Asking each executor to shut down2018-08-12 07:51:57 INFO KubernetesClusterSchedulerBackend:54 - Closing kubernetes client2018-08-12 07:51:57 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!2018-08-12 07:51:57 INFO MemoryStore:54 - MemoryStore cleared2018-08-12 07:51:57 INFO BlockManager:54 - BlockManager stopped2018-08-12 07:51:57 INFO BlockManagerMaster:54 - BlockManagerMaster stopped2018-08-12 07:51:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!2018-08-12 07:51:57 INFO SparkContext:54 - Successfully stopped SparkContext2018-08-12 07:51:57 INFO ShutdownHookManager:54 - Shutdown hook called2018-08-12 07:51:57 INFO ShutdownHookManager:54 - Deleting directory /tmp/spark-435d5ab2-f7b4-45d0-a00f-0bd9f162f9db 执行结束后executor pod被自动清除。计算得到pi的值为： 1Pi is roughly 3.1336756683783418 如果作业通过cluster提交，driver容器会被保留，可以查看： 123456789# minikube service list|-------------|------------------------------------------------------|-----------------------------|| NAMESPACE | NAME | URL ||-------------|------------------------------------------------------|-----------------------------|| default | kubernetes | No node port || default | spark-pi-27fcc168740e372292b27185d124ad7b-driver-svc | No node port || kube-system | kube-dns | No node port || kube-system | kubernetes-dashboard | http://192.168.99.100:30000 ||-------------|------------------------------------------------------|-----------------------------| 第三部分 常见报错异常处理1、如果遇到下面的报错信息，可能是Spark版本太低，建议升级大于2.4.5+以上版本后重试。 1234567891011121314151617181920212020-08-09 10:13:14 WARN KubernetesClusterManager:66 - The executor's init-container config map is not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.2020-08-09 10:13:14 WARN KubernetesClusterManager:66 - The executor's init-container config map key is not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.2020-08-09 10:13:15 WARN WatchConnectionManager:185 - Exec Failure: HTTP 403, Status: 403 -java.net.ProtocolException: Expected HTTP 101 response but was '403 Forbidden'at okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:216)at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:183)at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141)at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)2020-08-09 10:13:15 ERROR SparkContext:91 - Error initializing SparkContext.io.fabric8.kubernetes.client.KubernetesClientException: at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$2.onFailure(WatchConnectionManager.java:188)at okhttp3.internal.ws.RealWebSocket.failWebSocket(RealWebSocket.java:543)at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:185)at okhttp3.RealCall$AsyncCall.execute(RealCall.java:141)at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748) 2、如果发现拉取镜像的比较慢，或者任务状态一直停留也pulling。这是由于minikube主机中本地是没有这个私有镜像，需要进入minikube组件提前将dockerhub中的远程镜像拉取到本地。否则每次提交任务，都会从远程拉取，由于网络条件的限制（你懂的），导致每次拉取都很慢或者超时。 1234567891011root@deeplearning:~# minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\| |/' _ `\| || , &lt; ( ) ( )| '_`\ /'__`\| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )( ___/(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)$ docker images#显示本地没有，然后手动提前拉取到本地$ docker pull rongxiang1986/spark:2.4.6 2、spark 任务每个容器至少需要一个CPU，如果启动的minikube集群的资源是默认的2CPU，如果单个任务申请多个执行器就会报资源不足。所以在创建minikube集群时，提前分配足够的资源。 参考文献1、Running Spark on Kubernetes ：https://spark.apache.org/docs/latest/running-on-kubernetes.html 2、在Minikube Kubernetes集群上运行Spark工作：https://iamninad.com/running-spark-job-on-kubernetes-minikube/]]></content>
      <categories>
        <category>Minikube spark Kubernetes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Ubuntu上部署Minikube]]></title>
    <url>%2F2018%2F06%2F25%2F2018-06-25-%E5%9C%A8Ubuntu%E4%B8%8A%E9%83%A8%E7%BD%B2Minikube%2F</url>
    <content type="text"><![CDATA[[TOC] 背景Kubernetes是Google推出的容器编排工具，这是Google保密十几年的强大武器Borg的开源版本。Kubernetes这个名字源于古希腊，意思是舵手。既然docker被比喻成大海上驮着集装箱的鲸鱼，那么Kubernetes就是舵手，掌握鲸鱼的游弋方向，寓意深刻。 Kubernetes第一个正式版本于2015年7月发布。从Kubernetes 1.3开始提供了一个叫Minikube的强大测试工具，可以在主流操作系统平台（win、os、linux）上运行单节点的小型集群，这个工具默认安装和配置了一个Linux VM，Docker和Kubernetes的相关组件，并且提供Dashboard。 本篇主要介绍Ubuntu平台上部署Minikube。Minikube利用本地虚拟机环境部署Kubernetes，其基本架构如下图所示。 Minitube项目地址：https://github.com/kubernetes/minikube 第一部分 准备Minikube在OS X和Windows上部署需要安装虚拟机实现（用虚拟机来初始化Kubernetes环境），但是Linux例外可以使用自己的环境。参见：https://github.com/kubernetes/minikube#quickstart 1.1 准备工作检查CPU是否支持虚拟化，即BIOS中参数（VT-x/AMD-v ）设置为enable。 1.2 安装虚拟机Minikube在不同操作系统上支持不同的虚拟驱动： macOS xhyve driver, VirtualBox 或 VMware Fusion Linux VirtualBox 或 KVM 注意: Minikube 也支持 --vm-driver=none 选项来在本机运行 Kubernetes 组件，这时候需要本机安装了 Docker。 Windows VirtualBox 或 Hyper-V 本篇在Ubuntu部署VirtualBox虚拟驱动。 123# wget https://download.virtualbox.org/virtualbox/5.1.38/virtualbox-5.1_5.1.38-122592~Ubuntu~xenial_i386.deb#dpkg -i virtualbox-5.1_5.1.38-122592~Ubuntu~xenial_i386.deb 第二部分 安装minikube从阿里云下载最新版本的minikube： 最新版本在这个网址获取：https://github.com/AliyunContainerService/minikube 1234# curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v1.12.1/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 40.8M 100 40.8M 0 0 4671k 0 0:00:08 0:00:08 --:--:-- 7574k 第三部分 安装Kubectlkubectl即kubernetes的客户端，通过他可以进行类似docker run等容器管理操作 。 下载： 1234567# curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 52.5M 100 52.5M 0 0 6654k 0 0:00:08 0:00:08 --:--:-- 10.3M# chmod +x kubectl# mv kubectl /usr/local/bin/ 第四部分 启集群1234567891011121314151617root@deeplearning:~# minikube start --vm-driver=virtualbox --registry-mirror=https://registry.docker-cn.com --image-mirror-country=cn --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --force=true --kubernetes-version="v1.15.3"Starting local Kubernetes v1.10.0 cluster...Starting VM...Downloading Minikube ISO 153.08 MB / 153.08 MB [============================================] 100.00% 0sGetting VM IP address...Moving files into cluster...Downloading kubeadm v1.10.0Downloading kubelet v1.10.0Finished Downloading kubelet v1.10.0Finished Downloading kubeadm v1.10.0Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file. 为了访问海外的资源，阿里云提供了一系列基础设施，请按照如下参数进行配置。其中常见参数 --driver=*** 从1.5.0版本开始，Minikube缺省使用本地最好的驱动来创建Kubernetes本地环境，测试过的版本 docker, kvm --image-mirror-country cn 将缺省利用 registry.cn-hangzhou.aliyuncs.com/google_containers 作为安装Kubernetes的容器镜像仓库 （阿里云版本可选） --iso-url=*** 利用阿里云的镜像地址下载相应的 .iso 文件 （阿里云版本可选） --registry-mirror=***为了拉取Docker Hub镜像，需要为 Docker daemon 配置镜像加速，参考阿里云镜像服务 --cpus=2: 为minikube虚拟机分配CPU核数 --memory=2048mb: 为minikube虚拟机分配内存数 --kubernetes-version=***: minikube 虚拟机将使用的 kubernetes 版本 进入minikube虚拟机： 123456789101112root@deeplearning:~# minikube ssh _ _ _ _ ( ) ( ) ___ ___ (_) ___ (_)| |/') _ _ | |_ __ /' _ ` _ `\| |/' _ `\| || , &lt; ( ) ( )| '_`\ /'__`\| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )( ___/(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)$ # 通过exit退出集群$ exitlogout 虚拟机地址： 12# minikube ip192.168.99.100 启停集群： 1# minikube start/stop 获取集群信息： 123root@deeplearning:/home/rongxiang# kubectl get node NAME STATUS ROLES AGE VERSIONminikube Ready master 7h v1.10.0 删除集群： 123# minikube delete # rm -rf ~/.minikube # kubeadm reset 第五部分 心酸踩坑如果在启集群时遇到下面类似的错误，不要慌。国内环境99%的原因是GFW的原因，集群在抓取Google站点docker镜像时候被墙咔嚓了，然后time out。 WTF！我开始不知道呀。网上的部署指引都那么轻松！！然后重复删除集群，重新装，配参数，给docker配代理。尼玛，最后代理都被咔嚓了。终于撞了南墙，去阿里云拉取镜像，几秒钟搞定。 1234567891011121314151617# minikube startStarting local Kubernetes v1.10.0 cluster...Starting VM...Getting VM IP address...Moving files into cluster...Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...E0626 12:57:46.868961 26526 start.go:299] Error restarting cluster: restarting kube-proxy: waiting for kube-proxy to be up for configmap update: timed out waiting for the condition================================================================================An error has occurred. Would you like to opt in to sending anonymized crashinformation to minikube to help prevent future errors?To opt out of these messages, run the command: minikube config set WantReportErrorPrompt false================================================================================Please enter your response [Y/n]: 第六部分 远程访问 minikube dashboard6.1旧版本dashboard 1.0在虚拟机启动前，设置端口转发。注意这里使用tcp而不是http。 1# VBoxManage modifyvm "minikube" --natpf1 "kubedashboard,tcp,,30000,,30000" 然后启动虚拟机，这时候局域网上的其他服务器就可以通过宿主机的IP:30000访问web UI。或者： 1# kubectl proxy --address='0.0.0.0' --disable-filter=true 然后启动： 12# minikube dashboard --urlhttp://192.168.99.102:30000 6.2 新版本dashboard 2.0在新版本中，dashboard 2.0 默认会启用 https 的认证，具体认证方式有：TLS、token 和 username/passwd 首先地址映射： 12root@deeplearning:~# kubectl proxy &amp;Starting to serve on 127.0.0.1:8001 然后生成token： 1234567891011121314151617181920212223242526272829303132root@deeplearning:~# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yamlroot@deeplearning:~# kubectl create serviceaccount dashboard-admin -n kube-system#创建用于登录dashborad的serviceaccount账号serviceaccount "dashboard-admin" createdroot@deeplearning:~# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminclusterrolebinding.rbac.authorization.k8s.io/dashboard-cluster-admin created #创建一个clusterrolebingding，将名称为cluster-admin的clusterrole绑定到我们刚刚从的serviceaccount上，名称空间和sa使用:作为间隔root@deeplearning:~# kubectl get secret -n kube-system#创建完成后系统会自动创建一个secret，名称以serviceaccount名称开头NAME TYPE DATA AGE......dashboard-admin-token-twwfl kubernetes.io/service-account-token 3 46s.....#使用describe查看该secret的详细信息，主要是token一段root@deeplearning:~# kubectl describe secret dashboard-admin-token-twwfl -n kube-systemName: dashboard-admin-token-twwflNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: ae20b337-3729-4c97-9852-19e160b37427Type: kubernetes.io/service-account-tokenData====token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjZGSnZhZ2FiTUhlMTBoN1dYWFB6cmwzSkphNWVNQ0ZLWGZ0NEhOOUZST1UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdHd3ZmwiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWUyMGIzMzctMzcyOS00Yzk3LTk4NTItMTllMTYwYjM3NDI3Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.lOL6l1uWwEBfw4bbsdRyhvRnLzeOakz-kf38rc2RXPzBgMHqeLgsLw2o7TwRRp84XVNPoDaIa6HwW_6StxYZG5dfiCdUlClWHgepl-z3dq9r49IPqh-ZJLA56D1BZP-iRptmFjJHy5uAXPOyRRg-a43FwM0VZa4aaSe-NKdh7eceJxig0t_mJJbSYdIG_PqZS-JiBsJPb8KYl_GEWSNB4jzQ0SA8CZrB9Yl_ifhTL3vAie6FgLawPXENjz1puufene2Kymo-fVEmGK6KedRdIm4gCpg5NGmuEdJB3ikEzObN_Mv5JbK9wTmGW0s1-6m5ogdshpvKmSBQZ9Vuik_Qfwca.crt: 1066 bytesnamespace: 11 bytes 打开下面url，使用token方式登录： http://127.0.0.1:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login 附录 补充VBoxManage管理查询虚拟机： 123# VBoxManage list vms"&lt;inaccessible&gt;" &#123;4a3cefe1-11d1-45d2-91c5-1e39fccb6a8d&#125;"minikube" &#123;dfcd1bdf-afc1-49e6-a270-9d8ff14bf167&#125; 删除虚拟机： 1# VBoxManage unregistervm --delete 4a3cefe1-11d1-45d2-91c5-1e39fccb6a8d 参考文献及材料1、Minitube项目地址：https://github.com/kubernetes/minikube 2、kubernetes学习笔记之十一：kubernetes dashboard认证及分级授权]]></content>
      <categories>
        <category>Minikube Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Minikube上运行Kafka集群]]></title>
    <url>%2F2018%2F06%2F25%2F2019-07-27-%E5%9C%A8Minikube%E4%B8%8A%E8%BF%90%E8%A1%8CKafka%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Minikube集群启动 第一部分 Kubernetes中StatefulSet介绍 第三部分 部署Zookeeper集群 第四部分 部署Kafka集群 第五部分 总结 参考文献及资料 背景Kafka和zookeeper是在两种典型的有状态的集群服务。首先kafka和zookeeper都需要存储盘来保存有状态信息，其次kafka和zookeeper每一个实例都需要有对应的实例Id(Kafka需要broker.id,zookeeper需要my.id)来作为集群内部每个成员的标识，集群内节点之间进行内部通信时需要用到这些标识。 对于这类服务的部署，需要解决两个大的问题，一个是状态保存，另一个是集群管理(多服务实例管理)。kubernetes中提的StatefulSet(1.5版本之前称为Petset)方便了有状态集群服务在上的部署和管理。具体来说是通过Init Container来做集群的初始化工 作，用 Headless Service来维持集群成员的稳定关系，用Persistent Volume和Persistent Volume Claim提供网络存储来持久化数据，从而支持有状态集群服务的部署。 StatefulSet 是Kubernetes1.9版本中稳定的特性，本文使用的环境为 Kubernetes 1.10.0。 第一部分 Minikube集群启动12345678910111213141516171819202122root@deeplearning:~# minikube startThere is a newer version of minikube available (v1.2.0). Download it here:https://github.com/kubernetes/minikube/releases/tag/v1.2.0To disable this notification, run the following:minikube config set WantUpdateNotification falseStarting local Kubernetes v1.10.0 cluster...Starting VM...Downloading Minikube ISO 153.08 MB / 153.08 MB [============================================] 100.00% 0sGetting VM IP address...Moving files into cluster...Downloading kubeadm v1.10.0Downloading kubelet v1.10.0Finished Downloading kubeadm v1.10.0Finished Downloading kubelet v1.10.0Setting up certs...Connecting to cluster...Setting up kubeconfig...Starting cluster components...Kubectl is now configured to use the cluster.Loading cached images from config file. 第二部分 Kubernetes中StatefulSet介绍使用Kubernetes来调度无状态的应用较为简单 StatefulSet 这个对象是专门用来部署用状态应用的，可以为Pod提供稳定的身份标识，包括hostname、启动顺序、DNS名称等。 在最新发布的 Kubernetes 1.5 我们将过去的 PetSet 功能升级到了 Beta 版本，并重新命名为StatefulSet 第三部分 部署Zookeeper集群第四部分 部署Kafka集群参考文献及材料1、kubernetes 中 kafka 和 zookeeper 有状态集群服务部署实践 (一) https://cloud.tencent.com/developer/article/1005492 2、https://cloud.tencent.com/developer/article/1005491 3、https://www.bogotobogo.com/DevOps/Docker/Docker_Kubernetes_StatefulSet.php https://technology.amis.nl/2018/04/19/15-minutes-to-get-a-kafka-cluster-running-on-kubernetes-and-start-producing-and-consuming-from-a-node-application/ 4、https://kubernetes.io/zh/docs/tutorials/stateful-application/basic-stateful-set/ 5、https://jimmysong.io/kubernetes-handbook/guide/using-statefulset.html 6、Kubernetes部署Kafka集群 https://blog.usejournal.com/kafka-on-kubernetes-a-good-fit-95251da55837 https://www.cnblogs.com/cocowool/p/kubernetes_statefulset.html]]></content>
      <categories>
        <category>Minikube Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-集成学习综述]]></title>
    <url>%2F2018%2F06%2F09%2F2018-06-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[文章分为两个部分： 第一部分，介绍集成学习的理论原理。 第二部分，实践部分。主要使用sklearn封装的包。 第一部分 理论原理什么是集成学习​ 首先集成学习（ensemble learning）不是具体的算法，应该算一种算法思想（Algorithm Framework，类比EM算法）。在机器学习中，通过一定组合策略，将多种学习器组合起来，以此提升泛化能力的框架统称为集成学习。 ​ 集成学习框架可以分成两个部分：基分类器集合、组合策略。 ​ 最常用的集成学习框架有：Bagging、Boosting、Stacking，在机器学习的比赛中被广泛使用。 BaggingBootstrap aggregating 简称Bagging，下面的流程图很清楚的说明了Bagging的框架流程。 算法流程 算法输入：$TrainData \ D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$ ​ 基学习器： $L$ ​ 训练迭代的次数： $T$ 算法过程： 1、For t =1,2,3，…,T DO: 2、对训练集$D$进行第t次随机采样，共采集m次，得到包含m个样本的采样集$Dt$ 3、$h_t=L(D,D_t)$ 4、end for 算法输出：$H(x)=argmax_{y \in Y}{\sum_{t=1}^{T}I(h_t(x)=y)}$ 关键步骤： 随机采样：每次采样，从训练集中采样固定数量的样本。并且是有放回的。每次未被采样的数据称为袋外数据（Out Of Bag, 简称OOB ），作为测试集，检测基学习器的泛化能力。 Bagging的基学习器可以是“异质”的。 Bagging的组合策略有：投票法（分类问题）、平均值（回归问题）。 随机森林（Random Forest）是最常用的Bagging算法架构，基学习器使用CART决策树。当然RF对原架构有部分改动： RF中每个基分类器（决策树），每个节点对特征进行部分采样，然后在采样集中选择最优特征。提高基分类器的泛化能力。 普通的CART并不对特征进行采样。 Boosting（Adaboost）在Bagging框架下，基学习器是相互独立的学习过程。而在boosing框架中，基学习器存在前后依赖关系。可以将弱学习器（强于随机猜想）提升为强学习器。 数据集有$n$个点，Boosting框架给数据集赋予了权重分布结构${w_i}_{i=1}^n$表示这个点重要性（这些权重值会在代价函数中体现）。对于前项学习器分类错误的点，提高权重（即提高下一项学习器代价函数对于“错”点关注）。最后得到一个学习器序列${y_i}_{i=1}^M$,最后通过加权得到最终的模型。 上面的例子只是简单介绍。Boosting最出名的算法框架为Adaboost（adaptive boosting）。 Adaboost 算法流程以二元分类为例： 算法输入：$TrainData \ D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$ ​ 基学习器： $L$ ​ 训练迭代的次数： $T$ 算法过程： 1、初始化数据集权值分布：$D_1(x)=1/m$ 2、For t=1,2,3,…,T DO 3、$h_t=L(D,D_t)$ 4、$\epsilon_t=P_{x\sim D_t}(h_t(x)\ne f(x))$ 5、if $\epsilon_t &gt;0.5$ then break 6、$\alpha_t=\dfrac{1}{2}ln(\dfrac{1-\epsilon_t}{\epsilon_t} )$ 7、$D_{t+1}(x)=\dfrac{D_t(x)}{Z_t}exp(-\alpha_tf(x)h_t(x))$ 8、end for 算法输出：$H(x)=sign{\sum_{t=1}^{T}}\alpha_t h_t(x)$ 注记：Boosting只能针对二分类的问题。对于多分类问题已经有相关的推广工作。 Multi-class AdaBoost ：https://web.stanford.edu/~hastie/Papers/samme.pdf 该论文的算法已经在sklearn中进行封装实现。第二部分我们将讲解。 Stacking第二部分 实践sklearn中有集成学习丰富的封装包： The sklearn.ensemble module includes ensemble-based methods for classification, regression and anomaly detection. User guide: See the Ensemble methods section for further details. ensemble.AdaBoostClassifier([…]) An AdaBoost classifier. ensemble.AdaBoostRegressor([base_estimator, …]) An AdaBoost regressor. ensemble.BaggingClassifier([base_estimator, …]) A Bagging classifier. ensemble.BaggingRegressor([base_estimator, …]) A Bagging regressor. ensemble.ExtraTreesClassifier([…]) An extra-trees classifier. ensemble.ExtraTreesRegressor([n_estimators, …]) An extra-trees regressor. ensemble.GradientBoostingClassifier([loss, …]) Gradient Boosting for classification. ensemble.GradientBoostingRegressor([loss, …]) Gradient Boosting for regression. ensemble.IsolationForest([n_estimators, …]) Isolation Forest Algorithm ensemble.RandomForestClassifier([…]) A random forest classifier. ensemble.RandomForestRegressor([…]) A random forest regressor. ensemble.RandomTreesEmbedding([…]) An ensemble of totally random trees. ensemble.VotingClassifier(estimators[, …]) Soft Voting/Majority Rule classifier for unfitted estimators. AdaBoostClassifier参数介绍源码路径：sklearn\ensemble\weight_boosting.py 主要参数： base_estimator 12345&gt; base_estimator : object, optional (default=DecisionTreeClassifier)&gt; The base estimator from which the boosted ensemble is built.&gt; Support for sample weighting is required, as well as proper `classes_`&gt; and `n_classes_` attributes.&gt; 基分类器。默认为决策树（DecisionTreeClassifier）。基分类器的选择有个trick在algorithm参数中一并介绍。 n_estimators n_estimators : integer, optional (default=50) The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. 最大训练迭代次数。默认值为50。另外从boosting原理看，也可以理解为最大生成基学习器的序列长度。 learning_rate 12345&gt; learning_rate : float, optional (default=1.)&gt; Learning rate shrinks the contribution of each classifier by&gt; ``learning_rate``. There is a trade-off between ``learning_rate`` and&gt; ``n_estimators``.&gt; 学习率（步长）。源码中的介绍也提到参数的选择和参数n_estimators是一个trade-off。 algorithm 1234567&gt; algorithm : &#123;'SAMME', 'SAMME.R'&#125;, optional (default='SAMME.R')&gt; If 'SAMME.R' then use the SAMME.R real boosting algorithm.&gt; ``base_estimator`` must support calculation of class probabilities.&gt; If 'SAMME' then use the SAMME discrete boosting algorithm.&gt; The SAMME.R algorithm typically converges faster than SAMME,&gt; achieving a lower test error with fewer boosting iterations.&gt; 算法，其实应该是权重更新算法。参数有两个选择：’SAMME’, ‘SAMME.R’，默认是“’SAMME.R”。理解两个算法的区别就需要我们回溯到原论文：《Multi-class AdaBoost》： SAMME 算法 SAMME.R 算法 简单的理解：如果使用SAMME.R算法，需要基分类器的输出是概率值。即在sklearn框架中需要class中有predict_proba函数。 可以通过下面的python脚本查看哪些分类器是满足的。 12345import inspectfrom sklearn.utils.testing import all_estimatorsfor name, clf in all_estimators(type_filter='classifier'): if 'sample_weight' in inspect.getargspec(clf().fit)[0]: print(name) random_state 123456&gt; random_state : int, RandomState instance or None, optional (default=None)&gt; If int, random_state is the seed used by the random number generator;&gt; If RandomState instance, random_state is the random number generator;&gt; If None, the random number generator is the RandomState instance used&gt; by `np.random`.&gt; 随机种子。参数随意，如果要结果能复现，需要相同的随机种子。 AdaBoostRegressor参数简介源码路径：sklearn\ensemble\weight_boosting.py 主要参数： base_estimator 1234&gt; base_estimator : object, optional (default=DecisionTreeRegressor)&gt; The base estimator from which the boosted ensemble is built.&gt; Support for sample weighting is required.&gt; 基回归器。默认为回归树（DecisionTreeRegressor）。 n_estimators 1234&gt; n_estimators : integer, optional (default=50)&gt; The maximum number of estimators at which boosting is terminated.&gt; In case of perfect fit, the learning procedure is stopped early.&gt; 最大训练迭代次数。默认值为50。同AdaBoostClassifier。 learning_rate 12345&gt; learning_rate : float, optional (default=1.)&gt; Learning rate shrinks the contribution of each regressor by&gt; ``learning_rate``. There is a trade-off between ``learning_rate`` and&gt; ``n_estimators``.&gt; 学习率（步长）。 loss 1234&gt; loss : &#123;'linear', 'square', 'exponential'&#125;, optional (default='linear')&gt; The loss function to use when updating the weights after each&gt; boosting iteration.&gt; 误差。 这个参数只有AdaBoostRegressor有，Adaboost.R2算法需要用到。有线性‘linear’, 平方‘square’和指数 ‘exponential’三种选择, 默认是线性，一般使用线性就足够了，除非你怀疑这个参数导致拟合程度不好。这个值的意义在原理篇我们也讲到了，它对应了我们对第k个弱分类器的中第i个样本的误差的处理，即：如果是线性误差，则eki=|yi−Gk(xi)|Ekeki=|yi−Gk(xi)|Ek；如果是平方误差，则eki=(yi−Gk(xi))2E2keki=(yi−Gk(xi))2Ek2，如果是指数误差，则eki=1−exp（−yi+Gk(xi))Ek）eki=1−exp（−yi+Gk(xi))Ek），EkEk为训练集上的最大误差Ek=max|yi−Gk(xi)|i=1,2…m random_state 123456&gt; random_state : int, RandomState instance or None, optional (default=None)&gt; If int, random_state is the seed used by the random number generator;&gt; If RandomState instance, random_state is the random number generator;&gt; If None, the random number generator is the RandomState instance used&gt; by `np.random`. &gt; 随机数种子。 调参心得传授集成学习的参数按照层次可以分为两层： 1、集成框架的参数。 2、基分类器的参数。 例子参考文献1、ROC和AUC介绍以及如何计算AUC（http://alexkong.net/2013/06/introduction-to-auc-and-roc/） 2、sklearn（http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics） 3、wiki百科 （https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF） 4、统计学习方法 5、深度学习]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[5分钟介绍深度学习（科普）]]></title>
    <url>%2F2018%2F04%2F16%2F2018-04-15-5%E5%88%86%E9%92%9F%E4%BB%8B%E7%BB%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88%E7%A7%91%E6%99%AE%EF%BC%89%2F</url>
    <content type="text"><![CDATA[历史背景最近几年Deep Learning、AI人工智能、机器学习等名词称为新闻热点，特别是Google Deep mind的Alpha Go战胜韩国棋手李世石，让深度学习妇孺皆知。 首先从概念范畴上讲，deep learning属于机器学习的一个分支，追根溯源其实是人工神经网络。顾名思义，人工神经网络是借鉴人类神经网路的结构原型，算生物仿生学（虽然人类到现在也没弄明白大脑原理）。 例如下面就是一个3层结构的人工神经网络（1个输入层、1个隐藏层、1个输出层）。 1989年Yann LeCun使用反向传播算法（Back Propagation）应用于多层神经网络训练。但一旦层数较大，网络的参数和训练计算量成倍增加，通常需要几周时间才能完成参数训练，另外反向传播算法容易梯度爆炸。研究人员获得好的结果，时间成本太大。 所以当时机器学习研究方向中，支持向量机（SVM）算法比多层神经网络更为热门，神经网络研究则相当冷门。 直到2012 年的ImageNet 图像分类竞赛中，Alex Krizhevsky使用CNN（卷积）多层网络（共8层、6千万个参数）赢得当年的比赛，领先第二名10.8个百分点。并且模型使用GPU芯片训练、引入正则技术（Dropout）。 从此多层神经网络成为机器学习中的研究热点。而为了“洗白”以前暗淡历史，被赋予了新的名称：Deep Learning。 深度学习背后的数学目前的人工智能均属于弱人工智能（不具备心智和意识）。事实上，深度学习目前主要在图像识别和声音识别场景中获得较好的效果。深度学习的成为热点，依赖于两方面条件的成熟: 算力的提升，训练中大量使用GPU。 大量数据的获得和沉淀。 弱人工智能背后的理论基础依赖于数学和统计理论，其实更应该算数据科学的范畴。 比如输入数据具有$m$维特征，而输出特征为$k$维（例如如果是个二分类问题，$k=2$）。我们使用3层神经网络（输入层为$m$维，即含有$m$个神经元；隐藏层为$n$维，输出层为$k$维）用来训练。 通常我们将输入数据看成$R^m$（$m$维欧几里得空间），输出数据看成$R^k$（$k$维欧式空间）,如下图： 从数学上看，神经网络的结构定义了一个函数空间：${R^{n}} \xrightarrow{\text{f}} {R^{h}} \xrightarrow{\text{g}} {R^{m}}$ 。这个函数空间中元素是非线性的（隐藏层和输出层有非线性的激活函数）。空间中每个函数由网络中的参数（w，b）唯一决定。 神经网络训练的过程可以形象的理解为寻找最佳函数的过程：输入层“吃进”大量训练数据，通过非线性函数的作用，观察输出层输出结果和实际值的差异。这是一个监督学习的过程。 如果差异（误差）在容忍范围内，停止训练，认为该函数是目标函数。 如果差异较大，反向传播算法根据梯度下降的反向更新网络中的参数（w，b），即挑选新的函数。 重新喂进数据，计算新挑选函数的误差。如此循环，直到找到目标函数（也可设置提前结束训练）。 为什么神经网络的发展最后偏向的是“深度”呢？即增加层数来提高网络的认知能力。为什么没有“宽度学习”？即增加网络隐藏层的维度（宽度）。 其实从数学上可以证明深度网络和“宽度网络”的等价性。证明提示：考虑网络定义的函数空间出发。 写在最后（畅想未来）深度学习的局限性思考目前深度学习被各行各业应用于各种场景，而且有些特定场景取得了良好结果。但是传统的深度学习仍属于监督学习，更像一个被动的执行者，按照人类既定的规则，吃进海量数据，然后训练。 那么深度网络是否真的理解和学到了模式？还是只学会对有限数据的模式识别？甚至就是一个庞大的记忆网络？这都是值得我们深度思考的。 GAN对抗网络那么怎么能说明模型真的学习并理解了。我们提出了一个原则：如果你理解了一个事物，那么你就可以创造它。这样就发明了GAN对抗网络。让网络自己去创造事物，然后用现实数据去监督，当网络的创造能力和现实接近时，我们认为网络学会了。 其实思想类似传统的遗传算法。 强化学习另外传统的深度学习，输入的环境（数据）是固定的。然而现实中我们学习过程其实是：环境（数据）与学习个体互相作用的交互过程。这个学习过程人类由于时间有限，是个漫长的过程。但是计算机有个天然优势，可以同时启用成千上万的学习个体完成与环境数据的交互学习过程。例如Alpha Go启用上万个体，两两互搏，配上强大算力，短时间完成学习，这是人类不可企及的。 迁移学习人类学习中还有个方法叫：触类旁通。其实就是不同场景训练模型的借鉴。例如A场景得到训练好的模型（网络参数），对于新的场景B，可以尝试直接用A场景的网络（或部分使用，拼接），以此来减少训练成本。 那么新的问题来了：是否具有统一的迁移标准，即什么模型是适合迁移的？如果这些问题没有理论基础支持，迁移学习也摆脱不了“炼丹术”的非议。 深度学习从过去的暗淡无色到现在的光耀夺目。 然而任何方法都是有边际效应的。 人工智能的终点还很遥远，谁是下一颗耀眼的明星，需要学界和工业界共同探索。 ​ ​ 2018年4月15日 夜]]></content>
      <categories>
        <category>network</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[计算机语言中编译和解释的总结]]></title>
    <url>%2F2018%2F04%2F14%2F2018-04-14-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%AD%E8%A8%80%E4%B8%AD%E7%BC%96%E8%AF%91%E5%92%8C%E8%A7%A3%E9%87%8A%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 几个概念 第二部分 编译器和解释器 第三部分 解释型语言和编译型语言 参考文献及资料 背景 非计算机科班，主要是总结给自己看的，如果有表达错误，请大家指正。 第一部分 几个概念高级语言与低级语言高级语言（High Level Programming Language）和低级语言（Low Level Programming Language）是一对相对的共生概念（没有一个严格的量化区分标准）。 低级语言更接近计算机底层资源（直接与硬件资源进行交互）。例如汇编语言。 高级语言进行了封装和抽象，语言设计更容易被人类思维逻辑所理解（和低级语言比较，学习曲线较缓）。例如C、C++、java、python等。 随着计算机语言的蓬勃发展（计算机语言的文艺复兴），过去一些高级语言，也有人重新定位成低级语言，例如C语言。 字节码与机器码字节码（Byte Code）不是一种计算机语言。属于高级语言预编译生成的中间码。高级语言源码在预编译的过程中，就完成这部分工作，生成字节码。 机器码（Machine Code）是一组可以直接被CPU执行的指令集。所有语言（低级和高级）最后都需要编译或解释成机器码（CPU指令集），才能执行。 第二部分 编译器和解释器编译器（Interpreter） A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. The most common reason for converting source code is to create an executable program. 编译器是一种计算机程序。 编译器是一个计算机语言的翻译工具，直接将源代码文件预编译（形象的说：翻译）成更低级的代码语言（字节码码、机器码）。 编译器不会去执行编译的结果，只生成编译的结果文件。 解释器（Compiler） In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without previously compiling them into a machine language program. An interpreter generally uses one of the following strategies for program execution: 1、parse the source code and perform its behavior directly. 2、translate source code into some efficient intermediate representation and immediately execute this. 3、explicitly execute stored precompiled code made by a compiler which is part of the interpreter system. 解释器是一种计算机程序。 解释器读取源代码或者中间码文件，转换成机器码并与计算机硬件交互。即逐行执行源码。 解释器会将源代码转换成一种中间代码不会输出更低级的编译结果文件。输出执行结果。 第三部分 解释型语言和编译型语言两者的区别主要是源码编译时间的差异。相同点都要翻译成机器码后由计算机执行。 编译型语言 编译语言的源码文件需要提前通过编译器编译成机器码文件（比如win中的exe可执行文件）。 执行时，只需执行编译结果文件。不需要重复翻译。 这类语言有：C、C++、Fortran、Pascal等。 解释型语言 解释型语言在运行时进行翻译。比如VB语言，在执行的时候，解释器将语言翻译成机器码，然后执行。 这类语言有：Ruby、Perl、JavaScript、PHP等。 混合型语言但是随着计算机语言的发展，有些语言兼具两者的特点。 JAVA语言 JAVA编译过程只是将.java文件翻译成字节码（Byte Code）（.class文件）。字节码文件交由java虚拟机（JVM）解释运行。也就是说Java源码文件既要编译也要JVM虚拟机进行解释后运行。所以有种说法认为Java是半解释型语言（semi-interpreted” language）。 Python语言 python其实类似Java。例如一个python文件test.py ，解释器首先尝试读取该文件历史编译结果（pyc文件）即test.pyc文件或者test.pyo 。如果没有历史文件或者编译文件的日期较旧（即py文件可能有更新），解释器会重新编译生成字节码文件（pyc文件），然后Python虚拟机对字节码解释执行。 参考文献及资料【1】 你知道「编译」与「解释」的区别吗？]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Python语言中import机制]]></title>
    <url>%2F2018%2F04%2F14%2F2018-04-14-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Python%E8%AF%AD%E8%A8%80%E4%B8%ADimport%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 包和模块 第二部分 Import 方法 第三部分 命名空间（namespace） 第四部分 Import的过程 第五部分 将模块、包的路径加入检索路径 参考文献及资料 背景在Python语言使用过程中，经常会import第三方包，使用过程中或多或少遇到一些问题。本文将从原理层面简介Python中import机制，明白原理后，在遇到问题时候就会自己排错了。 第一部分 Python中的包和模块首先介绍Python中两个概念：包和模块。简单的理解（从文件系统角度），包（package）是一个文件夹，而模块（module）是一个Python源码文件（扩展名为.py）。 包（package）：文件夹（文件夹中含有文件__init__.py），包里面含有很多模块组成。 __init__.py文件，在里面自定义初始化操作，或为空。 模块（module）：即python文件，文件中定义了函数、变量、常量、类等。 第二部分 Import 方法2.1 Import 模块方法先看一个例子。我们经常使用的模块math ，背后对应其实是一个python文件：math.py 。该文件在C:\Anaconda3\Lib\site-packages\pymc3目录里面（具体环境会有差异）。 123import mathmath.sqrt(2)#1.4142135623730951 如果只要import math.py中具体的函数： 123from math import sqrt,sinsqrt(2)sin(1) 另外可以将模块中所有内容导入： 12from math import *sqrt(2) 2.2 Import 包方法包（package）可以简单理解为文件夹。该文件夹下须存在 __init__.py 文件, 内容可以为空。另外该主文件夹下面可以有子文件夹，如果也有 __init__.py 文件，这是子包。类似依次嵌套（套娃）。 例如Tensorflow的包（文件树）： 123456789101112root@vultr:~/anaconda3/lib/python3.6/site-packages/tensorflow# tree -L 1.├── aux-bin├── contrib├── core├── examples├── include├── __init__.py├── libtensorflow_framework.so├── __pycache__├── python└── tools __init__.py 文件在import包时，优先导入，作为import包的初始化。 我们以Tensorflow为例： 12345678#导入包import tensorflow as tf#导入子包：contribimport tensorflow.contrib as contribfrom tensorflow import contrib#导入具体的模块：mnistfrom tensorflow.examples.tutorials import mnistimport tensorflow.examples.tutorials.mnist 第三部分 命名空间（namespace）Namespace是字典数据，供编译器、解释器对源代码中函数名、变量名、模块名等信息进行关联检索（这是一个名称资源登记簿）。 3.1 定义Python语言使用namespace（命名空间）来存储变量，namespace是一个mapping（映射）。namespace可以理解是一个字典（dict）数据类型，其中键名（key）为变量名，而键值（value）为变量的值。 A namespace is a mapping from names to objects. Most namespaces are currently implemented as Python dictionaries。 每一个函数拥有自己的namespace。称为local namespace（局部命名空间），记录函数的变量。 每一个模块（module）拥有自己的namespace。称为global namespace（全局命名空间），记录模块的变量，包括包括模块中的函数、类，其他import（导入）的模块，还有模块级别的变量和常量。 每一个包（package）拥有自己的namespace。 也是global namespace ，记录包中所有子包、模块的变量信息。 Python的built-in names（内置函数、内置常量、内置类型）。 即内置命名空间。在Python解释器启动时创建，任何模块都可以访问。当退出解释器后删除。 3.2 命名空间的检索顺序当代码中需要访问或获取变量时（还有模块名、函数名），Python解释器会对命名空间进行顺序检索，直到根据键名（变量名）找到键值（变量值）。查找的顺序为（LEGB）： local namespace，即当前函数或者当前类。如找到，停止检索。 enclosing function namespace，嵌套函数中外部函数的namespace。 global namespace，即当前模块。如找到，停止检索。 build-in namespace，即内置命名空间。如果前面两次检索均为找到，解释器才会最后检索内置命名空间。如果仍然未找到就会报NameRrror（类似：NameError: name &#39;a&#39; is not defined）。 3.3 举栗子讲完了理论介绍，我们来举栗子，直观感受一下。 12345678910111213#进入python环境Python 3.5.3 |Anaconda custom (64-bit)| (default, May 11 2017, 13:52:01) [MSC v.1900 64 bit (AMD64)] on win32Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; print(globals())&#123;'__name__': '__main__', '__doc__': None, '__spec__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;&#125;&gt;&gt;&gt; x=1&gt;&gt;&gt; print(globals())&#123;'__name__': '__main__', '__doc__': None, '__spec__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__builtins__': &lt;module 'builtins' (built-in)&gt;, 'x': 1&#125; 上面的例子我们查看了global namespace的字典（dict），其中&#39;__builtins__&#39;就是内置命名空间。新建变量x=1后，全局命名空间会新增这个K-V对（&#39;x&#39;: 1）。 还可以通过下面的方法查看import模块、包的namespace。 当我们import一个module（模块）或者package（包）时，伴随着新建一个global namespace（全局命名空间）。 1234567891011import mathmath.__dict__&#123;'__name__': 'math', 'tanh': &lt;built-in function tanh&gt;, 'nan': nan, 'atanh': &lt;built-in function atanh&gt;,'acosh': &lt;built-in function acosh&gt;, #中间略'trunc': &lt;built-in function trunc&gt;, 'acos': &lt;built-in function acos&gt;, 'sqrt': &lt;built-in function sqrt&gt;, 'floor': &lt;built-in function floor&gt;, 'gamma': &lt;built-in function gamma&gt;, 'cosh': &lt;built-in function cosh&gt;&#125;import tensorflowtensorflow.__dict__#包的所有模块、函数等命名空间信息。大家可以试一下。 大家可以动手试试其他的场景，比如函数内部查看locals() 。函数内部的变量global声明后，查看globals()字典会有怎样变化。这里就不再一一验证举栗了。 对于包，我们以tensorflow为例： 123456import tensorflowtensorflow.__dict__##中间略，只摘取部分信息。命名空间中包含module和function的信息。'angle': &lt;function tensorflow.python.ops.math_ops.angle&gt;, 'app': &lt;module 'tensorflow.python.platform.app' from '/root/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py'&gt;, 'arg_max': &lt;function tensorflow.python.ops.gen_math_ops.arg_max&gt;, 第四部分 Import的过程当我们执行import 模块、包时，主要有三个过程：检索、加载、名字绑定。 第一步：检索（Finder）Python解释器会对模块所属位置进行搜索： （1）检索：内置模块（已经加载到缓存中的模块）内置模块（已经加载到缓存中的模块），即在 sys.modules 中检索。Python已经加载到内存中的模块均会在这个字典中进行登记。如果已经登记，不再重复加载。直接将模块的名字加入正在import的模块的namespace。可以通过下面方法查看： 123456789&gt;&gt;&gt; import sys&gt;&gt;&gt; print(sys.modules)&#123;'_signal': &lt;module '_signal' (built-in)&gt;, 'os.path': &lt;module 'ntpath' from 'C:\Anaconda3\\lib\\ntpath.py'&gt;,pickle': &lt;module 'pickle' from 'C:\\Anaconda3\\lib\\pickle.py'&gt;, #中间略'subprocess':module 'subprocess' from 'C:\\Anaconda3\\lib\\subprocess.py'&gt;, 'sys': &lt;module 'ys' (built-in)&gt;, 'ctypes.util': &lt;module 'ctypes.util' from 'C:\\Anaconda3\\lib\ctypes\\util.py'&gt;, '_weakref': &lt;module '_weakref' (built-in)&gt;, '_imp': &lt;module_imp' (built-in)&gt;&#125; 如果不是built-in，value中会有模块的绝对路径信息。 通过key查找模块位置，如果value为None，就会抛出错误信息：ModuleNotFoundError。 如果key不存在，就会进入下一步检索。 如果我们导入过包，例如tensorflow。当我们要使用其中模块，需要该模块的全名（即全路径信息），例如：tensorflow.examples.tutorials.mnist.input_data ，因为sys.modules中只有全路径的key。 1234import tensorflowprint(sys.modules)##这个字典中会有tensorflow所有子包、模块的信息和具体的路径。#'tensorflow.examples.tutorials.mnist.input_data': &lt;module 'tensorflow.examples.tutorials.mnist.input_data' from '/root/anaconda3/lib/python3.6/site-packages/tensorflow/examples/tutorials/mnist/input_data.py'&gt; （2）检索 sys.meta_path逐个遍历其中的 finder 来查找模块。否则进入下一步检索。 （3）检索模块所属包目录如果模块Module在包（Package）中（如import Package.Module），则以Package.__path__为搜索路径进行查找。 （4）检索环境变量如果模块不在一个包中（如import Module），则以 sys.path 为搜索路径进行查找。 如果上面检索均为找到，抛出错误信息：ModuleNotFoundError。 第二步：加载（Loader）加载完成对模块的初始化处理： 设置属性。包括__name__、__file__、__package__和__loader__ 。 编译源码。编译生成字节码文件（.pyc文件），如果是包，则是其对应的__init__.py文件编译为字节码（*.pyc）。如果字节码文件已存在且仍然是最新的（时间戳和py文件一致），则不会重新编译。 加载到内存。模块在第一次被加载时被编译，载入内存，并将信息加入到sys.modules中。 也可以强制用reload()函数重新加载模块（包）。 第三步：名字绑定将模块和包的命名空间信息导入到当前执行Python文件的namespace（命名空间）。 第五部分 将模块、包的路径加入检索路径讲完了枯燥的理论背景，下面我们来介绍实际应用。当你写好一个模块文件，如何正确完成import模块？主要有下面两类方法： 5.1 动态方法（sys.path中添加）我们知道检索路径中sys.path，所以可以在import模块之前将模块的绝对路径添加到sys.path中。同样导入包需要加入包的文件夹绝对路径。具体方法如下： 12345import sys##sys.path.append(dir)sys.path.append('your\module（package）\file\path')##sys.path.insert(pos,dir)sys.path.insert(0,'your\module（package）\file\path') 注意：` 1、这里pos参数是插入sys.path这个list数据的位置，pos=0，即list第一位，优先级高。 2、需要注意有效范围，python程序向sys.path添加的目录只在此程序的生命周期之内有效。程序结束，失效。所以这是一种动态方法。 123456789#win7import sysprint(sys.path)#输出['', 'C:\\Python27\\lib\\site-packages\\pip-8.1.1-py2.7.egg', 'C:\\windows\\system32\\python27.zip', 'C:\\Python27\\DLLs', 'C:\\Python27\\lib', 'C:\\Python27\\lib\\plat-win', 'C:\\Python27\\lib\\lib-tk', 'C:\\Python27', 'C:\\Users\\rongxiang\\AppData\\Roaming\\Python\\Python27\\site-packages', 'C:\\Python27\\lib\\site-packages'] 5.2 静态方法（1）另外检索路径还有系统环境变量，所以可以将模块（包）路径添加在系统环境变量中。 （2）粗暴一点直接将模块（包）拷贝到sys.path的其中一个路径下面。但是这种管理比较乱。 （3）Python在遍历sys.path的目录过程中，会解析 .pth 文件，将文件中所记录的路径加入到sys.path ，这样.pth 文件中的路径也可以找到了。例如我们在C:\Python27\lib\site-packages 中新建一个.pth文件。例如： 12# .pth file for the your module or package'your\module（package）\file\path' 这样在模块（包）上线时，我们只需要将模块（包）的目录或者文件绝对路径放在新建的.path文件中即可。 参考文章1、http://www.cnblogs.com/russellluo/p/3328683.html# 2、https://github.com/Liuchang0812/slides/tree/master/pycon2015cn]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据科学实践中常用开放数据集介绍]]></title>
    <url>%2F2018%2F04%2F05%2F2018-04-01-%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AE%9E%E8%B7%B5%E4%B8%AD%E5%B8%B8%E7%94%A8%E5%BC%80%E6%94%BE%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 开放数据集 参考文献及资料 背景数据科学研究的对象是数据，学习过程中需要相关数据集辅助大家练习、做实验。从而体会数据科学中算法方法论。中国古语云：巧妇难有无米之炊，说的就是数据对于数据科学学习的重要性。 这篇文章收集介绍了各种常用的开放数据集，供大家学习参考。会持续更新。 第二部分 开放数据集这里主要将开放数据分为三类：图像类、自然语言（NLP）类、音频类。 1.1 图像类MNIST手写数据集 介绍： MNIST（全称：Modified National Institute of Standards and Technology database）数据集是常见的深度学习开放数据集（基本属于深度学习的hello world数据集）。这是一个手写阿拉伯数据集（0-9数字），数据主要采集于美国高中学生。数据集总量为7W个手写数字图像（训练集6w个、测试机1w个）。 文件 内容 train-images-idx3-ubyte.gz 训练集图片 - 60000张训练图片 train-labels-idx1-ubyte.gz 训练集图片对应的数字标签（0-9） t10k-images-idx3-ubyte.gz 测试集图片 - 10000 张 图片 t10k-labels-idx1-ubyte.gz 测试集图片对应的数字标签 数据存储大小：二进制文件，50M，压缩形式约10M。每张图像被归一化成28*28的像素矩阵。 图像数据格式：像素值为0到255. 0表示背景（白色），255表示前景（黑色）。例如下面手写数字1的数据矩阵表示： 官方网页连接：http://yann.lecun.com/exdb/mnist/ 读取数据案例（Python）： Tensorflow中已经有对MNIST数据集解析的脚本，我们可以直接调用： 文件 目的 input_data.py、mnist.py 用于读取MNIST数据集 12345678910111213141516import tensorflow as tf#tf为1.7版本import numpy as npfrom tensorflow.examples.tutorials.mnist import input_datadata_dir = '/root/tftest/mnistdata/'#data_dir为数据集文件存放目录mnist = input_data.read_data_sets(data_dir, one_hot=True，validation_size=5000)#mnist = input_data.read_data_sets(data_dir, one_hot=False)#one_hot参数True，表示标签进行one-hot编码处理。#validation_size参数可以从训练集中划出一部分数据作为验证集。默认是5w个，可以自己调节。x_train,y_train,x_test,y_test,x_vali,y_vali = \mnist.train.images,mnist.train.labels,mnist.test.images,mnist.test.labels,\mnist.validation.images,mnist.validation.labels#x_train的数据类型为：&lt;class 'numpy.ndarray'&gt; ​ 上面的例子划分好数据就可以喂给各种算法模型进行训练。 扩展：EMNIST数据集：https://arxiv.org/abs/1702.05373。 按照MNIST规范，数据集更大：包含240,000个训练图像和40,000个手写数字测试图像。 MS-COCO图像分割数据集 介绍： MS-COCO（全称是Common Objects in Context）是微软团队提供的一个可以用来进行图像识别的数据集。数据集中的图像分为训练、验证和测试集。COCO数据集现在有3种标注类型：object instances（目标实例）, object keypoints（目标上的关键点）, 和image captions（看图说话），使用JSON文件存储。 一共有33w张图像，80个对象类别，每幅图5个字母、25w个关键点。 数据存储大小：约25G（压缩形式） 数据格式：中文介绍可以参考知乎这篇文章：COCO数据集的标注格式 。 官方网站：http://mscoco.org/ ImageNet图像数据集 介绍： Imagenet是深度学习中大名鼎鼎的数据集。数据集有1400多万幅图片，涵盖2万多个类别；其中有超过百万的图片有明确的类别标注和图像中物体位置的标注。深度学习中关于图像分类、定位、检测等研究工作大多基于此数据集展开。Imagenet数据集文档详细，有专门的团队维护，使用非常方便，在计算机视觉领域研究论文中应用非常广，几乎成为了目前深度学习图像领域算法性能检验的“标准”数据集。 数据存储大小：约150G 官方网站：http://www.image-net.org/ Open Image图像数据集 介绍： Open Image为Google提供。数据集包含近900万个图像URL。这些图像已经用数千个类的图像级标签边框进行了注释。该数据集包含9,011,219张图像的训练集，41,260张图像的验证集以及125,436张图像的测试集。 数据大小：500G 官方网站：https://github.com/openimages/dataset VisualQA图像数据库 介绍： VQA是一个包含有关图像的开放式问题的数据集。这些问题需要理解视野和语言。数据集有265,016张图片。 数据大小：25G 官方网站：http://www.visualqa.org/ The Street View House Numbers (SVHN) Dataset街边号码牌数据集 介绍： SVHN图像数据集用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它可以被看作与MNIST相似，但是将更多标记数据（超过600,000个数字图像）并入一个数量级并且来自显着更难以解决的真实世界问题（识别自然场景图像中的数字和数字）。SVHN数据从谷歌街景图片中的房屋号码中获得的。书记含有用于训练的73257个数字，用于测试的26032个数字以及用作额外训练数据的531131个附加数字。 数据集大小： [train.tar.gz]， [test.tar.gz]， [extra.tar.gz ] 共三个文件。 官方网站：http://ufldl.stanford.edu/housenumbers/ CIFAR-10图像数据集 介绍： CIFAR-10数据集由10个类的60,000个图像组成（每个类在上图中表示为一行）。总共有50,000个训练图像和10,000个测试图像。数据集分为6个部分 - 5个培训批次和1个测试批次。每批有10,000个图像。 数据大小：170M 官方网站：http://www.cs.toronto.edu/~kriz/cifar.html Fashion-MNIST 介绍 Fashion-MNIST包含60,000个训练图像和10,000个测试图像。它是一个类似MNIST的时尚产品数据库。开发人员认为MNIST已被过度使用，因此他们将其作为该数据集的直接替代品。每张图片都以灰度显示，并与10个类别的标签相关联。 数据集大小：30M 官方网站：https://github.com/zalandoresearch/fashion-mnist 1.2 自然语言类数据库IMDB电影评论数据集 介绍： 这是电影爱好者的梦幻数据集。它具有比此领域以前的任何数据集更多的数据。除了训练和测试评估示例之外，还有更多未标记的数据供您使用。原始文本和预处理的单词格式包也包括在内。 数据集大小：80 M 官方网站：http://ai.stanford.edu/~amaas/data/sentiment/ 模型案例：https://arxiv.org/abs/1705.09207 Twenty Newsgroups Data Set 介绍： 该数据集包含有关新闻组的信息。为了管理这个数据集，从20个不同的新闻组中获取了1000篇Usenet文章。这些文章具有典型特征，如主题行，签名和引号。 数据集大小：20 M 官方网站：https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups 模型案例：https://arxiv.org/abs/1606.01781 Sentiment140情感分析数据集 介绍： Sentiment140是一个可用于情感分析的数据集。 数据集大小：80 M 官方网站：http://help.sentiment140.com/for-students/ 模型案例：http://www.aclweb.org/anthology/W17-5202 WordNet 介绍： WordNet是英语synsets的大型数据库。Synsets是同义词组，每个描述不同的概念。WordNet的结构使其成为NLP非常有用的工具。 数据集大小：10 M 官方网站：https://wordnet.princeton.edu/ 模型案例：https://aclanthology.info/pdf/R/R11/R11-1097.pdf Yelp评论 介绍： 这是Yelp为了学习目的而发布的一个开放数据集。它由数百万用户评论，商业属性和来自多个大都市地区的超过20万张照片组成。这是一个非常常用的全球NLP挑战数据集。 数据集大小：2.66 GB JSON，2.9 GB SQL和7.5 GB照片（全部压缩） 官方网站：https://www.yelp.com/dataset 模型案例：https://arxiv.org/pdf/1710.00519.pdf 维基百科语料库 介绍： 该数据集是维基百科全文的集合。它包含来自400多万篇文章的将近19亿字。什么使得这个强大的NLP数据集是你可以通过单词，短语或段落本身的一部分进行搜索。 数据集大小： 20 MB 官方网站：https://corpus.byu.edu/wiki/ 模型案例：https://arxiv.org/pdf/1711.03953.pdf 博客作者身份语料库 介绍： 此数据集包含从数千名博主收集的博客帖子，从blogger.com收集。每个博客都作为一个单独的文件提供。每个博客至少包含200次常用英语单词。 数据集大小： 300 MB 官方网站：http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm 模型案例：https://arxiv.org/pdf/1609.06686.pdf 欧洲语言的机器翻译 介绍： 数据集包含四种欧洲语言。 数据集大小： 约15 G 官方网站：http://statmt.org/wmt11/translation-task.html 模型案例：https://arxiv.org/abs/1706.03762 1.3 音频/语音数据集口语数字数据集 介绍： 为了解决识别音频样本中的口头数字的任务而创建。这是一个开放的数据集，所以希望随着人们继续贡献更多样本，它会不断增长。 数据集大小： 约10 G=M 记录数量：1500个音频样本 官方网站：https://github.com/Jakobovski/free-spoken-digit-dataset 模型案例：https://arxiv.org/pdf/1712.00866 免费音乐档案（FMA） 介绍： FMA是音乐分析的数据集。数据集由全长和HQ音频，预先计算的特征以及音轨和用户级元数据组成。它是一个开放数据集，用于评估MIR中的几个任务。以下是数据集连同其包含的csv文件列表： tracks.csv：所有106,574首曲目的每首曲目元数据，如ID，标题，艺术家，流派，标签和播放次数。 genres.csv：所有163种风格的ID与他们的名字和父母（用于推断流派层次和顶级流派）。 features.csv：用librosa提取的共同特征 。 echonest.csv：由Echonest （现在的 Spotify）为13,129首音轨的子集提供的音频功能 。 数据集大小： 约1T 记录数量：1500个音频样本 官方网站：https://github.com/mdeff/fma 模型案例：https://arxiv.org/pdf/1803.05337.pdf 舞厅 介绍： 该数据集包含舞厅跳舞音频文件。以真实音频格式提供了许多舞蹈风格的一些特征摘录。 以下是数据集的一些特征： 数据集大小： 约14 G 记录数量：约700个音频样本 官方网站：http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html 模型案例：https://pdfs.semanticscholar.org/0cc2/952bf70c84e0199fcf8e58a8680a7903521e.pdf 百万歌曲数据集 介绍： 百万歌曲数据集是音频功能和元数据的一百万当代流行音乐曲目可自由可用的集合。 其目的是： 鼓励对扩大到商业规模的算法进行研究 为评估研究提供参考数据集 作为使用API创建大型数据集的捷径（例如Echo Nest的） 帮助新研究人员在MIR领域开始工作 数据集的核心是一百万首歌曲的特征分析和元数据。该数据集不包含任何音频，只包含派生的功能。示例音频可以通过使用哥伦比亚大学提供的代码从7digital等服务中获取。 数据集大小： 约280 G 记录数量：它的一百万首歌曲！ 官方网站：https://labrosa.ee.columbia.edu/millionsong/ 模型案例：http://www.ke.tu-darmstadt.de/events/PL-12/papers/08-aiolli.pdf LibriSpeech 介绍： 该数据集是大约1000小时的英语语音的大型语料库。这些数据来自LibriVox项目的有声读物。它已被分割并正确对齐。如果您正在寻找一个起点，请查看已准备好的声学模型，这些模型在kaldi-asr.org和语言模型上进行了训练，适合评估，网址为http://www.openslr.org/11/。 数据集大小： 约60 G 记录数量：1000小时的演讲 官方网站：http://www.openslr.org/12/ 模型案例：https://arxiv.org/abs/1712.09444 VoxCeleb 介绍： VoxCeleb是一个大型的说话人识别数据集。它包含约1,200名来自YouTube视频的约10万个话语。数据大部分是性别平衡的（男性占55％）。名人跨越不同的口音，职业和年龄。开发和测试集之间没有重叠。对于隔离和识别哪个超级巨星来说，这是一个有趣的用例。 数据集大小： 约150 M 记录数量： 1,251位名人的100,000条话语 官方网站：http://www.robots.ox.ac.uk/~vgg/data/voxceleb/ 模型案例：https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf 1.4 比赛数据Twitter情绪分析数据 介绍： 仇恨以种族主义和性别歧视为形式的言论已成为叽叽喳喳的麻烦，重要的是将这类推文与其他人分开。在这个实践问题中，我们提供既有正常又有仇恨推文的Twitter数据。您作为数据科学家的任务是确定推文是仇恨推文，哪些不是。 数据集大小： 约3 M 记录数量： 31,962条推文 官方网站：https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/ 印度演员的年龄检测 介绍： 对于任何深度学习爱好者来说，这是一个令人着迷的挑战。该数据集包含数千个印度演员的图像，你的任务是确定他们的年龄。所有图像都是手动选择的，并从视频帧中剪切，导致尺度，姿势，表情，照度，年龄，分辨率，遮挡和化妆的高度可变性。 数据集大小： 约48 M 记录数量： 训练集中的19,906幅图像和测试集中的6636幅图像 官方网站：https://datahack.analyticsvidhya.com/contest/practice-problem-age-detection/ 城市声音分类 介绍： 这个数据集包含超过8000个来自10个班级的城市声音摘录。这个实践问题旨在向您介绍常见分类方案中的音频处理。 数据集大小： 训练集 - 3 GB（压缩），测试集 - 2 GB（压缩） 记录数量： 来自10个班级的8732个城市声音标注的声音片段（&lt;= 4s） 官方网站：https://datahack.analyticsvidhya.com/contest/practice-problem-urban-sound-classification/ 参考文献及资料【1】 https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners 【2】 https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/ 【3】 https://deeplearning4j.org/cn/opendata]]></content>
      <categories>
        <category>Bigdata</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo:解决Typora编辑table无法被解析问题]]></title>
    <url>%2F2018%2F04%2F01%2F2018-04-05-Hexo%E8%A7%A3%E5%86%B3Typora%E7%BC%96%E8%BE%91table%E6%97%A0%E6%B3%95%E8%A2%AB%E8%A7%A3%E6%9E%90%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 掉坑背景 第二部分 爬坑过程和解决办法 第三部分 提示 参考文献及资料 第一部分 掉坑背景使用Typora编辑Makedown文件，添加表格，但是提交给Hexo渲染网页，无法正常解析显示，而是显示源码。例如：| Table Header 1 | Table Header 2 || ————– | ————– || Division 1 | Division 2 || Division 1 | Division 2 | 第二部分 爬坑过程和解决办法一开始认为是Hexo的bug，Google也没人遇到类似情况，都准备在github上建问题单了。最后本着严谨的态度，以文本的格式打开文档，发现表格源码和正文之间没有空行！！！！！ 这尼玛坑爹呀，所以Hexo无法解析，但是Typora能正常解析。空出一行后正常解析： 123456&lt;正文&gt;(空一行)| Table Header 1 | Table Header 2 || - | - | | Division 1 | Division 2 | | Division 1 | Division 2 | Table Header 1 Table Header 2 Division 1 Division 2 Division 1 Division 2 这一点Typora做的不够兼容（只怪他太过于强大的解析能力。。。。）。Tyopra不服了，我强大也有错？？哈哈哈 记录该坑供掉坑小伙伴参考。 第三部分 提示如果掉坑小伙伴，上面办法没解决。用文本方式打开文件，逐个排查原因。 参考文献及资料暂无]]></content>
      <categories>
        <category>hexo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu挂载新的硬盘（2T以上）]]></title>
    <url>%2F2018%2F04%2F01%2F2018-04-01-Ubuntu%E6%8C%82%E8%BD%BD%E6%96%B0%E7%9A%84%E7%A1%AC%E7%9B%98%EF%BC%882T%E4%BB%A5%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 查看硬盘信息 第二部分 新挂载硬盘分区 第三部分 使用parted分区 第四部分 格式化新建分区 第五部分 挂载分区 第六部分 配置开机自动挂载分区 第七部分 附录 参考文献及资料 背景系统环境： Linux version 4.13.0-37-generic (Ubuntu 5.4.0-6ubuntu1~16.04.9)。root用户登入操作。 第一部分 查看硬盘信息机器断电时，接入硬盘。开机后用下面的命令查看硬盘状况（非root用户需sudo）。 123456789101112131415161718root@deeplearning:~# fdisk -lDisk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: CC8004FC-D422-48FA-8ACF-54C3F48E860BDevice Start End Sectors Size Type/dev/sda1 2048 1050623 1048576 512M EFI System/dev/sda2 1050624 909946879 908896256 433.4G Linux filesystem/dev/sda3 909946880 976771071 66824192 31.9G Linux swapDisk /dev/sdb: 3.7 TiB, 4000787030016 bytes, 7814037168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytes 查看到系统由两块硬盘：/dev/sda和/dev/sdb，如果还有其他硬盘会继续sdc、sdd编号。 正在使用的系统盘sda已经有三个分区（sda1、sda2、sda3），新挂载的硬盘sdb位分区。 第二部分 新挂载硬盘分区新硬盘存储空间一共4T，我们对硬盘进行分区。划分为两个分区： 1234567891011121314root@deeplearning:~# fdisk /dev/sdbWelcome to fdisk (util-linux 2.27.1).Changes will remain in memory only, until you decide to write them.Be careful before using the write command./dev/sdb: device contains a valid 'ext4' signature; it is strongly recommended to wipe the device with wipefs(8) if this is sible collisionsDevice does not contain a recognized partition table.The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format can not be used on drives for volumes lar512-byte sectors. Use GUID partition table format (GPT).Created a new DOS disklabel with disk identifier 0x6b028a17.Command (m for help): 注意这里已经有警告：The size of this disk is 3.7 TiB (4000787030016 bytes). DOS partition table format can not be used on drives for volumes lar512-byte sectors. Use GUID partition table format (GPT) 这里情况特殊，新加入的磁盘为4T。fdisk命令对于大于2T的分区无法划分。如果继续使用fdisk工具，最多只能分出2T的分区，剩下的空间无法利用。这不坑爹嘛。提示我们使用parted命令。 第三部分 使用parted分区parted命令可以划分单个分区大于2T的GPT格式的分区。 更改分区表类型： 1root@deeplearning:~# parted -s /dev/sdb mklabel gpt 使用parted进行分区： 123456789101112131415161718192021222324252627282930313233root@deeplearning:~# parted /dev/sdbGNU Parted 3.2Using /dev/sdbWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) print Model: ATA WDC WD40EFRX-68N (scsi)Disk /dev/sdb: 4001GBSector size (logical/physical): 512B/4096BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags(parted) mklabel gpt Warning: The existing disk label on /dev/sdb will be destroyed and all data on this disk will be lost. Do you want to continue?Yes/No? yes (parted) mkpart Partition name? []? File system type? [ext2]? ext4 Start? 0% End? 100% (parted) print Model: ATA WDC WD40EFRX-68N (scsi)Disk /dev/sdb: 4001GBSector size (logical/physical): 512B/4096BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 1049kB 4001GB 4001GB ext4(parted) quit Information: You may need to update /etc/fstab. 最后我们验证一下，sdb1分区成功，提示我们要更新系统文件：/etc/fstab。 12345678910111213141516171819202122232425root@deeplearning:~# ls /dev/sd* /dev/sda /dev/sda1 /dev/sda2 /dev/sda3 /dev/sdb /dev/sdb1root@deeplearning:~# fdisk -lDisk /dev/sda: 465.8 GiB, 500107862016 bytes, 976773168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: gptDisk identifier: CC8004FC-D422-48FA-8ACF-54C3F48E860BDevice Start End Sectors Size Type/dev/sda1 2048 1050623 1048576 512M EFI System/dev/sda2 1050624 909946879 908896256 433.4G Linux filesystem/dev/sda3 909946880 976771071 66824192 31.9G Linux swapDisk /dev/sdb: 3.7 TiB, 4000787030016 bytes, 7814037168 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytesDisklabel type: gptDisk identifier: 0D8B0FBC-83F6-4D77-ABDB-98875EC511E4Device Start End Sectors Size Type/dev/sdb1 2048 7814035455 7814033408 3.7T Linux filesystem 第四部分 格式化新建分区将分区格式化为ext4格式的文件系统。 1234567891011121314root@deeplearning:~# mkfs.ext4 /dev/sdb1mke2fs 1.42.13 (17-May-2015)Creating filesystem with 976754176 4k blocks and 244195328 inodesFilesystem UUID: dfcd419f-38a5-4a5c-9b93-9f236d2c2444Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544Allocating group tables: done Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done 如果有多个分区需要依次执行格式化。 第五部分 挂载分区新建硬盘即将挂载的目录，然后将硬盘挂载到该目录下。并验证挂载成功，检查硬盘空间。 12345678910111213root@deeplearning:/# mkdir /dataroot@deeplearning:/# mount /dev/sdb1 /dataroot@deeplearning:/# df -hFilesystem Size Used Avail Use% Mounted onudev 16G 0 16G 0% /devtmpfs 3.2G 9.3M 3.2G 1% /run/dev/sda2 427G 21G 385G 5% /tmpfs 16G 0 16G 0% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 16G 0 16G 0% /sys/fs/cgroup/dev/sda1 511M 3.5M 508M 1% /boot/efitmpfs 3.2G 12K 3.2G 1% /run/user/1000/dev/sdb1 3.6T 68M 3.4T 1% /data 上面我们把新的硬盘挂载到了/data目录，硬盘空间大小正常。 第六部分 配置开机自动挂载分区6.1 查看分区的UUID123root@deeplearning:/# blkid#（略）.../dev/sdb1: UUID="dfcd419f-38a5-4a5c-9b93-9f236d2c2444" TYPE="ext4" PARTUUID="fe373bd5-5b19-4ed0-8713-716455a8ebb4" 6.2 配置/etc/fstab将分区信息写到/etc/fstab文件中让它永久挂载: 将下面的配置信息加入配置文件尾部： 1UUID=dfcd419f-38a5-4a5c-9b93-9f236d2c2444 /data ext4 defaults 0 1 第七部分 附录7.1 /etc/fstab配置说明12345678910111213# Use 'blkid' to print the universally unique identifier for a# device; this may be used with UUID= as a more robust way to name devices# that works even if disks are added and removed. See fstab(5).&lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt; 1 2 3 4 5 6对应参数说明：1、指代文件系统的设备名。最初，该字段只包含待挂载分区的设备名（如/dev/sda1）。现在，除设备名外，还可以包含LABEL或UUID2、文件系统挂载点。文件系统包含挂载点下整个目录树结构里的所有数据，除非其中某个目录又挂载了另一个文件系统3、文件系统类型。下面是多数常见文件系统类型（ext3,tmpfs,devpts,sysfs,proc,swap,vfat）4、mount命令选项。mount选项包括noauto（启动时不挂载该文件系统）和ro（只读方式挂载文件系统）等。在该字段里添加用户或属主选项，即可允许该用户挂载文件系统。多个选项之间必须用逗号隔开。其他选项的相关信息可参看mount命令手册页（-o选项处）5、转储文件系统？该字段只在用dump备份时才有意义。数字1表示该文件系统需要转储，0表示不需要转储6、文件系统检查？该字段里的数字表示文件系统是否需要用fsck检查。0表示不必检查该文件系统，数字1示意该文件系统需要先行检查（用于根文件系统）。数字2则表示完成根文件系统检查后，再检查该文件系统。 7.2 Parted命令说明（本文使用交互模式完成配置）Parted 命令分为两种模式：命令行模式和交互模式。 命令行模式： parted [option] device [command] ,该模式可以直接在命令行下对磁盘进行分区操作，比较适合编程应用。 交互模式：parted [option] device 类似于使用fdisk /dev/xxx MBR：MBR分区表(即主引导记录)大家都很熟悉。所支持的最大卷：2T，而且对分区有限制：最多4个主分区或3个主分区加一个扩展分区 GPT： GPT（即GUID分区表）。是源自EFI标准的一种较新的磁盘分区表结构的标准，是未来磁盘分区的主要形式。与MBR分区方式相比，具有如下优点。突破MBR 4个主分区限制，每个磁盘最多支持128个分区。支持大于2T的分区，最大卷可达18EB。 parted是一个可以分区并进行分区调整的工具，他可以创建，破坏，移动，复制，调整ext2 linux-swap fat fat32 reiserfs类型的分区，可以创建，调整，移动Macintosh的HFS分区，检测jfs，ntfs，ufs，xfs分区。 使用方法：parted [options] [device [command [options...]...]] 12345678910111213141516171819202122232425262728options-h 显示帮助信息-l 显示所有块设备上的分区device 对哪个块设备进行操作，如果没有指定则使用第一个块设备command [options...]check partition 对分区做一个简单的检测cp [source-device] source dest 复制source-device设备上的source分区到当前设备的dest分区mklabel label-type 创建新分区表类型，label-type可以是："bsd", "dvh", "gpt", "loop","mac", "msdos", "pc98", or "sun" 一般的pc机都是msdos格式，如果分区大于2T则需要选用gpt格式的分区表。mkfs partition fs-type 在partition分区上创建一个fs-type文件系统，fs-type可以是："fat16", "fat32", "ext2", "linux-swap","reiserfs" 注意不支持ext3格式的文件系统，只能先分区然后用专有命令进行格式化。mkpart part-type [fs-type] start end 创建一个part-type类型的分区，part-type可以是："primary", "logical", or "extended" 如果指定fs-type则在创建分区的同时进行格式化。start和end指的是分区的起始位置，单位默认是M。eg：mkpart primary 0 -1 0表示分区的开始 -1表示分区的结尾 意思是划分整个硬盘空间为主分区mkpartfs part-type fs-type start end 创建一个fs-type类型的part-type分区，不推荐使用，最好是使用mkpart分区完成后使用mke2fs进行格式化。name partition name 给分区设置一个名字，这种设置只能用在Mac, PC98, and GPT类型的分区表，设置时名字用引号括起来select device 在机器上有多个硬盘时，选择操作那个硬盘resize partition start end 调整分区大小rm partition 删除一个分区rescue start end 拯救一个位于stat和end之间的分区unit unit 在前面分区时，默认分区时数值的单位是M，这个参数卡伊改变默认单位，"kB", "MB", "GB", "TB"move partition start end 移动partition分区print 显示分区表信息 quit 退出parted 参考文献【1】 Setting up a large (2TB+) hard disk drive on Linux]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-深入理解PCA算法]]></title>
    <url>%2F2018%2F03%2F28%2F2018-03-06-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PCA%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[背景数据科学中有个经典数据处理技术：Principal components analysis（简称PCA）。英文直译为：主成分分析。 数据的抽象数据科学研究中，一个数据样本通常抽象成特征向量（feature vector），例如：$$x=(x^{(1)},x^{(2)},…,x^{(i)},…,x^{(n)})^T$$这里$x^{(i)}$表示第$i$个特征（数值），这是一个$n$维特征的样本数据。例如人脸图像（灰色），每张图片的像素值可以转换成一个列向量，像素值即特征。 进一步抽象，如果给这些特征向量赋予欧式度量（拓扑结构），这些特征向量取自$R^n$空间（n维欧式空间）。这样我们就可以在欧式空间框架下研究数据集了。 当然你可以赋予别的度量，甚至赋予一个拓扑结构（这是一件有趣的事情，读者可以思考探索）。 数据的降维降维的目的当我们将数据抽象为欧式空间的向量，欧式空间 剔除数据集的特征存在冗余信息。 数据的采集过程中，采集特征之间本身存在相关性（线性或非线性）。去除冗余信息后，减少计算量。 减少数据集中噪声信息。 高维数据的降维可视化。 将高维空间的数据映射到3维或者2维欧式空间中，降维映射会保持高维数据的结构信息。比如高维空间相似的点，在低维可视化空间中体现为距离较近，我们说降维映射是保持距离的。 降维和特征选择在数据特征工程中，我们会对原始数据进行特征选择（feature selection），提取主要的特征因素，直接删除冗余或者认为次要的数据特征。广义上，这也属于数据降维技术，并且具有较强的可解释性。 另外数据降维还包含： 3.1 主成分分析PCA 3.2 多维缩放(MDS) 3.3 线性判别分析(LDA) 3.4 等度量映射(Isomap) 3.5 局部线性嵌入(LLE) 3.6 t-SNE 3.7 Deep Autoencoder Networks 主成分分析 最大方差形式假设${x_i}_{i=1}^{S}$ 是取自$R^n$欧式空间的数据点（列向量），即$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(j)},…,x_i^{(n)})^T$，我们将$R^n$中的数据点投影到$R^m$（$m&lt;n$）空间中，同时最大化投影（正交子空间投影）数据在$R^m$空间中的方差。 约定向量均为列向量 首先我们考虑$m=1$的情况，即$R^1$空间。这时我们可以挑选单位向量$u\in R^n$作为$R^1$的方向。因为$u^Tu=1$（欧式距离），所以数据点在$u$上的投影向量为 $(x_i^Tu)u$，坐标值为标量$x_i^Tu$。数据点在新的空间中的均值为：$$A=\frac{1}{S}\sum_{i=1}^S{x_i^Tu}=\frac{1}{S}\sum_{i=1}^S\sum_{j=1}^nx_{i}^{j}u_{i}^{j}=\sum_{j=1}^n(\frac{1}{S}\sum_{i=1}^Sx_{i}^{j})u_{i}^{j}=\overline{x}^Tu$$其中$\overline{x}=(\frac{1}{S}\sum_{i=1}^Sx_{i}^{j})_{j=1,…,n}^T$ 数据点在新的空间中方差为：$$V=\frac{1}{S}\sum_{i=1}^{S}(x_i^Tu-\overline{x}^Tu)^2=u^TCu$$其中C是数据的协方差矩阵（对称矩阵），定义为：$$C=\frac{1}{S}\sum_{i=1}^{S}(x_i-\overline{x})^T(x_{i}-\overline{x})$$关于$u$最大化方差$V=u^TCu$ ，引入拉格朗日乘子，即求解最大化函数：$$f(u,\lambda)=u^TCu+\lambda(1-u^Tu)$$关于$u$求偏导数：$$\frac{\partial f}{\partial u}=Cu-\lambda u=0$$显然$u$是矩阵$C$的特征值$\lambda$的特征向量。这时候方差 $V=u^TCu=u^T\lambda u=\lambda$ 所以我们选择协方差矩阵$C$的最大特征值对应的特征向量即可。 对于$m&gt;1$的情况，我们的最优目标是使得原始数据点在$R^m$的每个方向上$u_i$（$u_i \in U={u_1,u_2,…u_m}， \Vert u_i\Vert=1 $）的投影方差均最大化。即我们要挑选合适U集合，使得数据点在每个$u_i \in U$上的投影方差最大。 协方差矩阵是实对称矩阵，可以正交对角化，对角线上的元素为矩阵特征值集合。 有上面的推导容易知道U为特征值大小前$m$名所对应的特征向量的集合。 最小误差形式假设${u_j}_{j=1}^n$是$R^n$空间的单位正交基（$u_i u_j=\delta_{ij}$），那么对于数据点${x_i}_{i=1}^S$可以由基向量线性表出：$$x_i=\sum_{j=1}^na_{ij}u_j=\sum_{j=1}^n(x_iu_j)u_j$$我们的目标是对空间降维重建，使得数据点在新的坐标系（维度为$m&lt;n$）下得到近似表达，不失一般性，我们假设挑选基向量为${u_i}_{i=1}^m$,数据点在新的坐标系上的表示为：$$\hat{x_i}=\sum_{j=1}^ma_{ij}u_j$$我们的目标是最小化重建误差，定义为：$$J=\frac{1}{S}\sum_{i=1}^S\Vert x_i-\hat{x_i}\Vert^2=\frac{1}{S}\sum_{i=1}^S\Vert \sum_{j=1}^na_{ij}u_j-\sum_{j=1}^ma_{ij}u_j\Vert^2=\frac{1}{n}\sum_{i=1}^S\Vert\sum_{j=m+1}^na_{ij}u_j\Vert^2$$由单位正交性，得到：$$J=\frac{1}{S}\sum_{i=1}^S\sum_{j=m+1}^na_{ij}^2=\frac{1}{S}\sum_{i=1}^S\sum_{j=m+1}^nu_j^Tx_ix_i^Tu_j=\sum_{j=m+1}^nu_j^TCu_j$$ 降维和特征选择数学原理矩阵和线性变换算法实现过程特征分解奇异值分解（SVD）算法的深度理解知乎文章：https://www.zhihu.com/question/36348219 第一层理解（最大方差投影）第二层理解（最小重建误差）第三层理解（高斯先验误差）第四层理解（线性流行对齐）矩阵知识准备 KLT（Karhunen Loeve Transform）变换 KLT变换最早用于信号学中用于压缩信息量，用于除去输入数据的相关性。首先KLT是一个线性变换，所以除去的是线性相关性。]]></content>
      <categories>
        <category>PCA</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jupyter notebook:主题和字体的美化]]></title>
    <url>%2F2018%2F03%2F19%2F2018-03-26-JupyterNotebook%E4%B8%BB%E9%A2%98%E5%92%8C%E5%AD%97%E4%BD%93%E7%9A%84%E7%BE%8E%E5%8C%96%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 安装 第二部分 命令格式 第三部分 案例 参考文献及资料 背景Jupyter notebook是数据科学常用的代码交互式工具。通常在server端启jupyter进程（web服务），client端打开浏览器，jupyter提供代码编写和调试交互环境。非常方便。 但是jupyter提供的默认界面不够美观，特别是windows操作系统默认字体为浏览器默认字体–宋体（下图），另外默认主题太难看了，没有通常IDE提供的主题美观。 发现一个Jupyter的美化工具：jupyterthemes ，和大家分享一下。简单介绍一下安装和配置。细节介绍参考项目的介绍文档。 第一部分 安装使用pip安装： 1root@vultr:~# pip install jupyterthemes 或者使用Anaconda的conda安装： 1root@vultr:~# conda install -c conda-forge jupyterthemes 第二部分 命令格式使用jt -h显示命令帮助说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354root@vultr:~# jt -husage: jt [-h] [-l] [-t THEME] [-f MONOFONT] [-fs MONOSIZE] [-nf NBFONT] [-nfs NBFONTSIZE] [-tf TCFONT] [-tfs TCFONTSIZE] [-dfs DFFONTSIZE] [-ofs OUTFONTSIZE] [-mathfs MATHFONTSIZE] [-m MARGINS] [-cursw CURSORWIDTH] [-cursc CURSORCOLOR] [-cellw CELLWIDTH] [-lineh LINEHEIGHT] [-altp] [-altmd] [-altout] [-P] [-T] [-N] [-vim] [-r] [-dfonts]optional arguments: -h, --help show this help message and exit #-h，--help显示此帮助信息并退出 -l, --list list available themes #-l， 列出可用主题 -t THEME, --theme THEME theme name to install（配置需要安装的主题） -f MONOFONT, --monofont MONOFONT monospace code font（代码的字体） -fs MONOSIZE, --monosize MONOSIZE code font-size（代码字体大小） -nf NBFONT, --nbfont NBFONT notebook font（notebook 字体） -nfs NBFONTSIZE, --nbfontsize NBFONTSIZE notebook fontsize（notebook 字体大小） -tf TCFONT, --tcfont TCFONT txtcell font（文本的字体） -tfs TCFONTSIZE, --tcfontsize TCFONTSIZE txtcell fontsize（文本的字体大小） -dfs DFFONTSIZE, --dffontsize DFFONTSIZE pandas dataframe fontsize（pandas类型的字体大小） -ofs OUTFONTSIZE, --outfontsize OUTFONTSIZE output area fontsize（输出区域字体大小） -mathfs MATHFONTSIZE, --mathfontsize MATHFONTSIZE mathjax fontsize (in %)（数学公式字体大小） -m MARGINS, --margins MARGINS fix margins of main intro page -cursw CURSORWIDTH, --cursorwidth CURSORWIDTH set cursorwidth (px)（设置光标宽度） -cursc CURSORCOLOR, --cursorcolor CURSORCOLOR cursor color (r, b, g, p)（设置光标颜色） -cellw CELLWIDTH, --cellwidth CELLWIDTH set cell width (px or %)（单元的宽度） -lineh LINEHEIGHT, --lineheight LINEHEIGHT code/text line-height (%)（行高） -altp, --altprompt alt input prompt style -altmd, --altmarkdown alt markdown cell style -altout, --altoutput set output bg color to notebook bg -P, --hideprompt hide cell input prompt -T, --toolbar make toolbar visible（工具栏可见） -N, --nbname nb name/logo visible -vim, --vimext toggle styles for vim -r, --reset reset to default theme（设置成默认主题） -dfonts, --defaultfonts force fonts to browser default（设置成浏览器默认字体） 第三部分 案例例如下面的命令完成效果： 使用的主题是：monokai，工具栏可见，命名笔记本的选项，代码的字体为13，代码的字体为consolamono。 1root@vultr:~# jt -t monokai -T -N -fs 13 -f consolamono 如果jupyter进程已启，需要重新启进程后生效。 实现的效果截图： 其他主题效果大家可以自己尝试。 参考文献及资料1、jupyter官网，链接：https://jupyter.org/]]></content>
      <categories>
        <category>jupyter</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派使用USB摄像头自制家庭监控]]></title>
    <url>%2F2018%2F03%2F18%2F2018-03-18-%E6%A0%91%E8%8E%93%E6%B4%BE%E4%BD%BF%E7%94%A8USB%E6%91%84%E5%83%8F%E5%A4%B4%E8%87%AA%E5%88%B6%E5%AE%B6%E5%BA%AD%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 部署步骤 参考文献及资料 背景树莓派摄像头和USB摄像头 树莓派有配套的摄像头模块（Raspberry Pi camera board），如下图。 另外树莓派也支持USB摄像头。关于树莓派支持的USB摄像头有个清单参考（需要梯子）。大家购买前最好确认一下是否在兼容清单中。 第一部分 部署步骤第一步：检查USB摄像头和树莓派的兼容性将USB摄像头和树莓派连接，查看USB接口连接情况。 12345root@raspberrypi:/# lsusbBus 001 Device 004: ID 046d:0825 Logitech, Inc. Webcam C270Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet AdapterBus 001 Device 002: ID 0424:9514 Standard Microsystems Corp.Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub 发现004口上面连接并识别了摄像头（我的是Logitech 270摄像头）。 如果没有识别出来，需要查看USB兼容清单是否有该型号。另外由于树莓派供电功率较小，也有可能是USB供电功率不足，需要有外置电源的USB摄像头。 另外还可以查看设备驱动情况： 12root@raspberrypi:/# ls /dev/vid*/dev/video0 发现video0设备，说明识别了USB摄像头（罗技的c270i） 第二步：MOTION软件实现对于USB摄像头，有多种软件包可以实现拍照和摄像等功能，这里使用motion。 安装motion 1root@raspberrypi:/# sudo apt-get install motion 编辑配置配置文件 1root@raspberrypi:~# vi /etc/motion/motion.conf 调整相关参数 123456789101112# The mini-http server listens to this port for requests (default: 0 = disabled)stream_port 8082# web界面访问端口# TCP/IP port for the http server to listen on (default: 0 = disabled)webcontrol_port 8080#控制端口# Restrict control connections to localhost only (default: on)webcontrol_localhost off# Target base directory for pictures and films# Recommended to use absolute path. (Default: current working directory)target_dir /var/lib/motion#照片及视频存放路径 其他参数调整如下： ​ ffmpeg_output_movies=off ​ stream_localhost=off ​ webcontrol_localhost=off ​ locate_motion_mode=peview ​ locate_motion_style=redbox ​ text_changes=on 如果需要对网页进行加密，可以调整下面的配置实现： 123456789# Set the authentication method (default: 0)# 0 = disabled# 1 = Basic authentication# 2 = MD5 digest (the safer authentication)stream_auth_method 1# Authentication for the stream. Syntax username:password# Default: not defined (Disabled)stream_authentication admin:admin1234 开启motion进程修改motion文件，设置为守护进程运行（即参数配置为：yes）： 123# vi /etc/default/motion# set to 'yes' to enable the motion daemonstart_motion_daemon=yes 启进程： 1234567root@raspberrypi:/etc/init.d# motion start[0] [NTC] [ALL] conf_load: Processing thread 0 - config file /etc/motion/motion.conf[0] [ALR] [ALL] conf_cmdparse: Unknown config option "sdl_threadnr"[0] [NTC] [ALL] motion_startup: Motion 3.2.12+git20140228 Started[0] [NTC] [ALL] motion_startup: Logging to syslog[0] [NTC] [ALL] motion_startup: Using log type (ALL) log level (NTC)[0] [NTC] [ALL] become_daemon: Motion going to daemon mode 查看监控画面地址栏中输入地址和端口号（IP：8082），上面配置的web界面访问端口为8082： 查看监控数据存放目录另外目录/var/lib/motion中存放历史数据。 第三步：内网穿透（外网访问web监控界面）实现上面的步骤，你只能在家里本地局域网访问监控界面，意义不大。 由于目前中国宽带服务公司都不会给家庭网络外网地址。所以需要内网穿透，实现外网访问家庭内网。 具体可以使用frp软件实现内网穿透。具体做法参考的博客中另一篇介绍frp的分享文章。 参考文献及资料1、Building a Motion Activated Security Camera with the Raspberry Pi Zero，链接： https://www.bouvet.no/bouvet-deler/utbrudd/building-a-motion-activated-security-camera-with-the-raspberry-pi-zero 2、How to make a DIY home alarm system with a raspberry pi and a webcam，链接：https://medium.com/@Cvrsor/how-to-make-a-diy-home-alarm-system-with-a-raspberry-pi-and-a-webcam-2d5a2d61da3d]]></content>
      <categories>
        <category>raspberry</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu添加国内apt更新源]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-15-Ubuntu%E6%B7%BB%E5%8A%A0%E5%9B%BD%E5%86%85apt%E6%9B%B4%E6%96%B0%E6%BA%90%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 更换步骤 参考文献及资料 背景本篇博客主要介绍如何更改Ubuntu系统的apt源。 关于源我们使用apt安装软件时，会到国外源下载软件包。但是由于各种原因（你懂的）国外站点到国内的下载速度非常缓慢，甚至1k/s。对于大的包，这是无法忍受的等待，经常会超时中断。所以我们换成国内的源站点。其中口碑比较好的源站点有：阿里源、清华源、中科大源等。 Ubuntu系统的源地址文件位置：/etc/apt/sources.list 第一部分 更换步骤第一步：备份 对于系统文件的修改建议实施备份。养成良好的变更习惯。关键时候能救命。 关于备份文件命名有两个建议：（1）含有backup字段提示为备份文件；（2）含有备份日期，便于区分多个备份。当然如果是多用户话应该含有用户名，便于区分。 1root@deeplearning:/# cp /etc/apt/sources.list /etc/apt/sources.list.backup.20180315 第二步：添加源地址 我们添加阿里源， 进入阿里云开源镜像站，找到ubuntu的帮助信息： 我们版本号Ubuntu 16.04.4 LTS，并且Codename: xenial。需要根据自己的版本对应相应的源。 123456789101112131415161718# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricteddeb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-propertiesdeb http://archive.canonical.com/ubuntu xenial partnerdeb-src http://archive.canonical.com/ubuntu xenial partnerdeb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricteddeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-propertiesdeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse 直接将source.list中内容用上面的源地址内容替换，保存后退出。更新源： 1root@deeplearning:/# apt-get update 附录：apt的常用操作命令清单12345678910111213141516sudo apt-get update 更新源sudo apt-get install package 安装包sudo apt-get remove package 删除包sudo apt-cache search package 搜索软件包sudo apt-cache show package 获取包的相关信息，如说明、大小、版本等sudo apt-get install package --reinstall 重新安装包sudo apt-get -f install 修复安装sudo apt-get remove package --purge 删除包，包括配置文件等sudo apt-get build-dep package 安装相关的编译环境sudo apt-get upgrade 更新已安装的包sudo apt-get dist-upgrade 升级系统sudo apt-cache depends package 了解使用该包依赖那些包sudo apt-cache rdepends package 查看该包被哪些包依赖sudo apt-get source package 下载该包的源代码sudo apt-get clean &amp;&amp; sudo apt-get autoclean 清理无用的包sudo apt-get check 检查是否有损坏的依赖 参考文献及资料1、Ubuntu更换阿里云软件源，链接:https://yq.aliyun.com/articles/704603]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动手实现Ubuntu系统WOL远程唤醒]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-14-%E5%8A%A8%E6%89%8B%E5%AE%9E%E7%8E%B0Ubuntu%E7%B3%BB%E7%BB%9FWOL%E8%BF%9C%E7%A8%8B%E5%94%A4%E9%86%92%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 检查主机板块和网卡是否支持Wol 第二部分 部署步骤 参考文献及资料 背景本篇博客主要介绍通过局域网唤醒服务器（远程启动计算机）。具体在Ubuntu操作系统上实现。具体数据流为：通过互联网远程登录长期开机的树莓派，然后通过树莓派唤醒同一个局域网的高性能服务器。 目前中国家庭宽带网络都是没有外网IP的，如果外网访问家庭网络，需要做端口映射，实现远程访问。 什么是WoL（Wake on LAN）电脑处在关机（或休眠）状态时，只要主机保持连接电源、网线连接网卡，其实网卡和主板仍然有微弱供电。这部分供电能让网卡监听和解读来自外部网络的广播信息。其中会对一种特殊的广播信息Magic Packet（魔法数据包）进行侦测。Magic Packet网络包以广播的形式发送，发送的范围可以是整个局域网或者指定的子网。另外Magic Packet中唤醒服务器IP可以是多个，侦测主机一旦发现包中的唤醒IP集中包含自己的IP，会通知主板、电源供电器，开始执行唤醒，打开机器。 第一部分 检查主机板块和网卡是否支持Wol 主板是否支持：进入BIOS，将“Power Management Setup”中的“Wake Up On LAN”或“Resume by LAN”项设置为“Enable”或“On” 网卡是否支持： 1# ethtool enp0s31f6 其中有下面的字段信息： 12Supports Wake-on: pumbgWake-on: g 第二部分 部署步骤2.1 方法1需要安装wakeonlan包： 1root@raspberrypi:~# sudo apt-get install wakeonlan 下面的命令通过树莓派发送魔术包： 12root@raspberrypi:~# wakeonlan -i 192.168.1.3 b0:6f:bf:b0:9f:2fSending magic packet to 192.168.1.3:9 with b0:6f:b0:bf:9f:2f 2.2 方法2在网关配置ARP信息（IP与物理地址进行绑定），发送网段的广播： 12root@raspberrypi:~# wakeonlan -i 192.168.1.0 b0:6f:bf:b0:9f:2fSending magic packet to 192.168.1.0:9 with b0:6f:b0:bf:9f:2f 参考文献及链接1、WakeOnLan 链接：https://help.ubuntu.com/community/WakeOnLan 2、wiki 链接：https://en.wikipedia.org/wiki/Wake-on-LAN 3、Ubuntu 與 Wake on LAN 链接：http://softsmith.blogspot.com/2014/05/ubuntu-wake-on-lan.html]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[动手实现深度学习相机]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-14-deeplearning_camera(%E6%9C%AA%E5%AE%8C%E7%A8%BF)%2F</url>
    <content type="text"><![CDATA[2017年年底，亚马逊（AWS）宣布将推出深度学习相机——DeepLens ，亚马逊官网已经开始预售，预计6月14日发货。但是售价为249刀（约1600人民币）。偏贵了。 具体介绍可以参考这篇文章：AWS深度学习摄像头，将对机器学习产业有何影响？ 看到有人利用树莓派和简易摄像头实现了一个深度学习相机，用来检测院子里面小鸟吃食。正好自己有一个树莓派，可以参考玩一下。进一步优化甚至可以放在家门口，对访客人脸识别。]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu系统常用命令汇总]]></title>
    <url>%2F2018%2F03%2F14%2F2018-03-15-Ubuntu%E7%B3%BB%E7%BB%9F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 系统信息 第二部分 资源信息 第三部分 磁盘信息 第四部分 网络信息 第五部分 进程信息 第六部分 用户信息 参考文献及资料 背景本篇博客主要汇总查询Ubuntu系统的信息的相关命令及展示案例。会持续更新。 第一部分 系统信息1.1 查看：CPU信息12root@deeplearning:/# cat /proc/versionLinux version 4.13.0-37-generic (buildd@lcy01-amd64-012) (gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)) #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 1.2 查看：内核、操作系统、CPU信息12root@deeplearning:/# uname -aLinux deeplearning 4.13.0-37-generic #42~16.04.1-Ubuntu SMP Wed Mar 7 16:03:28 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux 1.3 查看：操作系统版本信息123456root@deeplearning:/# lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 16.04.4 LTSRelease: 16.04Codename: xenial 1.4 查看：计算机名12root@vultr:~# hostnamevultr.guest 第二部分 资源信息2.1 查看：存储分区的使用信息123456789root@vultr:~# df -hFilesystem Size Used Avail Use% Mounted onudev 469M 0 469M 0% /devtmpfs 99M 11M 89M 11% /run/dev/vda1 25G 12G 12G 49% /tmpfs 495M 0 495M 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 495M 0 495M 0% /sys/fs/cgrouptmpfs 99M 0 99M 0% /run/user/0 2.2 查看：系统运行时间、用户数量12root@vultr:~# uptime 23:04:54 up 10 days, 17:21, 1 user, load average: 0.21, 0.06, 0.02 第三部分 磁盘信息3.1 查看:所有分区信息12345678910root@vultr:~# fdisk -lDisk /dev/vda: 25 GiB, 26843545600 bytes, 52428800 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0xcb855d49Device Boot Start End Sectors Size Id Type/dev/vda1 * 2048 52428257 52426210 25G 83 Linux 第四部分 网络信息4.1 查看：网络接口信息12345678910root@vultr:~# ifconfig#（略）lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 198660 bytes 27478459 (27.4 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 198660 bytes 27478459 (27.4 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 4.2 查看：防火墙信息1234root@vultr:~# iptables -LChain DOCKER-USER (1 references)target prot opt source destination RETURN all -- anywhere anywhere 4.3 查看：路由表12345root@vultr:~# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface#（略）172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 4.4 查看：监听端口、已经建立的连接1234root@vultr:~# netstat -lntpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 749/sshd 1234root@vultr:~# netstat -antpActive Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 749/sshd 4.5 查看：网络统计信息123456root@vultr:~# netstat -sIp: Forwarding: 1 5729182 total packets received 9 with invalid addresses#略信息 第五部分 进程信息5.1 查看：所有进程信息123root@vultr:~# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 Mar25 ? 00:00:44 /sbin/init 5.2 查看：实时显示进程状态12345678910root@vultr:~# toptop - 23:24:19 up 10 days, 17:41, 1 user, load average: 0.02, 0.02, 0.00Tasks: 87 total, 1 running, 86 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.7 us, 0.3 sy, 0.0 ni, 99.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1012392 total, 74384 free, 270236 used, 667772 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 556896 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 25691 root 20 0 554760 13348 3448 S 0.3 1.3 35:21.42 docker-containe #（略信息） 第六部分 用户信息6.1 查看：活动用户1234root@vultr:~# w 23:25:22 up 10 days, 17:42, 1 user, load average: 0.33, 0.08, 0.02USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 140.31.74.0 22:59 2.00s 0.06s 0.00s w 6.2 查看：用户登录日志信息12345root@vultr:~# lastroot pts/0 113.41.56.0 Wed Apr 4 22:59 still logged inroot pts/0 113.41.56.0 Wed Apr 4 08:37 - 13:38 (05:00)wtmp begins Sun Apr 1 19:59:32 2018 参考文献及资料1、The 50 Most Useful Linux Commands To Run in the Terminal，链接：https://www.ubuntupit.com/best-linux-commands-to-run-in-the-terminal/]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-不平衡数据集的采样技术]]></title>
    <url>%2F2018%2F03%2F01%2F2018-02-28-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%87%87%E6%A0%B7%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[在数据处理中，经常需要对给定随机变量（一维或多维）及其概率分布函数，需要生成随机变量的采样（sampling）数据集（随机变量的随机采样样本点）。 采样的方法有很多： 1、MCMC方法（Mento Carlo Markov Chain，即：蒙特卡洛-马尔科夫链）。主要的算法有： Metroplis-Hasting 算法 Gibbs Sampling 算法]]></content>
      <categories>
        <category>Sampling</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派更改APT为国内阿里云源]]></title>
    <url>%2F2018%2F03%2F01%2F2018-03-12-%E6%A0%91%E8%8E%93%E6%B4%BE%E6%9B%B4%E6%94%B9APT%E4%B8%BA%E5%9B%BD%E5%86%85%E9%98%BF%E9%87%8C%E4%BA%91%E6%BA%90%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 更改步骤 参考文献及资料 背景发现树莓派wget国外源异常慢。其实可以更改为国内的apt源，不用走海下光缆啦。 第一部分 更改步骤1.1 编辑sources.list文件还没安装我喜爱的vim，只能先用nano编辑文件： 1root@raspberrypi:~# nano /etc/apt/sources.list 1.2 修改源将原始的源注释掉，添加阿里云的源地址（在这里感谢阿里爸爸）。 12deb http://mirrors.aliyun.com/raspbian/raspbian/ jessie main non-free contrib rpideb-src http://mirrors.aliyun.com/raspbian/raspbian/ jessie main non-free contrib rpi 1.3 更软件索引清单最后更新一下，以后就可以快速apt-get啦。 1sudo apt-get update 参考文献及资料1、Raspberry Pi 使用阿里云OPSX镜像，链接：https://zihengcat.github.io/2018/05/14/using-alibaba-cloud-opsx-mirrors-in-raspberry-pi/]]></content>
      <categories>
        <category>raspberry</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[树莓派搭建家庭内网穿透服务器(frp实现)]]></title>
    <url>%2F2018%2F03%2F01%2F2018-03-12-%E6%A0%91%E8%8E%93%E6%B4%BE%E6%90%AD%E5%BB%BA%E5%AE%B6%E5%BA%AD%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E6%9C%8D%E5%8A%A1%E5%99%A8(frp%E5%AE%9E%E7%8E%B0)%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 环境准备 第二部分 FRP介绍 第三部分 实现步骤 参考文献及资料 背景对于目前家庭网络，有下面两个迫切需求： 需要远程（比如在单位、路途等）SSH访问家里PC服务器、后续实现远程唤醒服务器。 后续部署自制家庭监控，需要远程访问监控web界面。 鉴于上面的需求，技术上需要实现外网访问内网（即内网穿透）。下面详细介绍具体实现步骤。 第一部分 环境准备 一台树莓派主机。由于耗电较少，适合长期开机，作为中转服务器。 一个公网IP。由于电信宽带不提供公网IP，只能自己想办法。正好有一台外网VPS服务器（独有公网IP） 另外顺便打个不收钱的广告：VPS可以使用vultr的虚机，支持支付宝，非常方便。 第二部分 FRP介绍实现内网穿透有很多方法：frp软件、ngrok软件、还有花生壳。关于frp有详细的官方介绍文档 ，不再赘述。 第三部分 实现步骤第一步：配置VPS服务器VPS操作系统为Ubuntu，下载linux_amd64版本： 1root@vultr:~# wget https://github.com/fatedier/frp/releases/download/v0.16.1/frp_0.16.1_linux_amd64.tar.gz 解压缩： 123456789101112root@vultr:~# tar -zxvf frp_0.16.1_linux_amd64.tar.gzfrp_0.16.1_linux_amd64/frp_0.16.1_linux_amd64/frpc_full.inifrp_0.16.1_linux_amd64/LICENSEfrp_0.16.1_linux_amd64/frpc.inifrp_0.16.1_linux_amd64/frps.inifrp_0.16.1_linux_amd64/frpcfrp_0.16.1_linux_amd64/frps_full.inifrp_0.16.1_linux_amd64/frpsroot@vultr:~# cd frp_0.16.1_linux_amd64/root@vultr:~/frp_0.16.1_linux_amd64# lsfrpc frpc_full.ini frpc.ini frps frps_full.ini frps.ini LICENSE 对于VPS服务端只有两个文件是需要的：frps （服务）和frps.ini （配置文件）是需要的，我们拷贝到/bin目录下面（这一步主要是集中放在bin目录便于管理）。 frpc 和frpc.ini是客户端服务和配置文件，后面介绍。 12root@vultr:~/frp_0.16.1_linux_amd64# cp frps /bin/frpsroot@vultr:~/frp_0.16.1_linux_amd64# cp frps.ini /bin/frps.ini 然后对配置文件进行修改： 123456789101112131415161718192021222324252627282930313233root@vultr:/bin# vi frps.ini#修改配置文件[common]bind_addr = 10.66.2.137#VPS公网IP地址（为了保护隐私，上面地址是虚构的）bind_port = 7000#frp服务端口，用户自己定于一个空闲端口（不要和其他应用服务端口冲突）。需要注意的是必须与frpc.ini相同vhost_http_port = 80#http服务端口vhost_https_port = 443#https服务端口dashboard_port = 7500#web控制台端口，10.66.2.137：7500可以访问控制界面#下面两个参数是控制界面的用户名和密码dashboard_user = admindashboard_pwd = Password123！privilege_token = Password123！#特权模式密钥，需与frpc.ini相同log_file = /bin/frps_log/frps.log#日志文件存储路径log_level = info#日志记录级别log_max_days = 7#日志最大存储天数max_pool_count = 5#后端连接池最大连接数量#口令超时时间authentication_timeout = 900#subdomain_host = frp.com #服务端绑定域名tcp_mux = true 保存修改后的配置文件，后台启服务端进程，下面是命令格式： 12345root@vultr:/bin# nohup ./frps -c ./frps.ini &amp;#查看服务进程：root@vultr:/bin# ps -ef|grep frproot 1339 1 0 Mar25 ? 00:03:54 ./frps -c ./frps.iniroot 5320 4958 0 11:42 pts/1 00:00:00 grep --color=auto frp 以上完成服务端配置。 第二步：配置树莓派客户端 注意：树莓派的CPU处理器是ARM的，所以注意下载的版本包。 1root@raspberrypi:~# wget https://github.com/fatedier/frp/releases/download/v0.16.1/frp_0.16.1_linux_arm.tar.gz 解压缩下载的包： 123456789101112root@raspberrypi:~# tar -zxvf frp_0.16.1_linux_arm.tar.gz frp_0.16.1_linux_arm/frp_0.16.1_linux_arm/frpc_full.inifrp_0.16.1_linux_arm/LICENSEfrp_0.16.1_linux_arm/frpc.inifrp_0.16.1_linux_arm/frps.inifrp_0.16.1_linux_arm/frpcfrp_0.16.1_linux_arm/frps_full.inifrp_0.16.1_linux_arm/frpsroot@raspberrypi:~# cd frp_0.16.1_linux_armroot@raspberrypi:~/frp_0.16.1_linux_arm# lsfrpc frpc_full.ini frpc.ini frps frps_full.ini frps.ini LICENSE 类似服务端操作将frpc和frpc.ini拷贝到/bin目录下面。 12root@raspberrypi:~/frp_0.16.1_linux_arm# cp frpc /bin/frpcroot@raspberrypi:~/frp_0.16.1_linux_arm# cp frpc.ini /bin/frpc.ini 修改配置文件： 123456789101112131415161718192021222324252627282930313233343536373839root@raspberrypi:/bin# vi frpc.ini#修改客户端配置文件[common]server_addr = 10.66.2.137#VPS公网IP地址（为了保护隐私，上面地址是虚构的）server_port = 7000privilege_token = Password123！log_file = /bin/frpc.log#日志目录log_level = infolog_max_days = 7pool_count = 5tcp_mux = true#配置SSH端口映射[ssh]type = tcplocal_ip = 127.0.0.1#本地端口local_port = 22#映射端口remote_port = 6000[web]type = httplocal_ip = 127.0.0.1local_port = 80use_encryption = falseuse_compression = truesubdomain = web#所绑定的公网服务器域名，一级、二级域名都可以。这里没有就不用配置了custom_domains = web.frp.com#远程监控端口映射[motion]type = tcplocal_ip = 127.0.0.1local_port = 8082remote_port = 8000 保存修改，后台启客户端进程： 1234root@raspberrypi:/bin# nohup ./frpc -c ./frpc.ini &amp;root@raspberrypi:/bin# ps -ef|grep frproot 4627 1 0 4月02 ? 00:05:19 ./frpc -c ./frpc.iniroot 13731 13669 0 19:56 pts/0 00:00:00 grep frp 第三步：验证 FRP管理界面（http://公网IP:7500） 显示两个端口映射都是online可用的： 下面是整体视图： SSH服务 我们已经将本地访问ssh的服务端口（192.168.1.2：22）映射到外网端口（10.66.2.137：6000）。 例如使用putty工具，IP地址填写：10.66.2.137，端口：6000。 连接后使用树莓派本地ssh用户和密码即可登录。 参考文献及资料[1] FRP官方网站 https://github.com/fatedier/frp]]></content>
      <categories>
        <category>raspberry</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu操作系统设置静态IP地址]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-25-Ubuntu%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE%E9%9D%99%E6%80%81IP%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 编辑interfaces文件 第二部分 添加静态IP信息及DNS信息 第三部分 重启服务生效 参考文献及资料 背景有一台Linux台式机（Ubuntu 16.04.3 LTS (GNU/Linux 4.13.0-32-generic x86_64)）。之前IP是DHCP服务分配的。准备给服务器分配静态IP，方便使用。 ubuntu的网络参数保存在文件/etc/network/interfaces中。 第一部分 编辑interfaces文件12345vi /etc/network/interfaces# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopback 第二部分 添加静态IP信息及DNS信息12345678910# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto enp0s31f6iface enp0s31f6 inet staticaddress 192.168.31.62gateway 192.168.31.254netmask 255.255.255.0dns-nameservers 192.168.31.1 8.8.8.8 第三部分 重启服务生效1234567891011root@vultr:~# service systemd-networkd status● systemd-networkd.service - Network Service Loaded: loaded (/lib/systemd/system/systemd-networkd.service; enabled-runtime; vendor preset: enabled) Active: active (running) since Wed 2020-03-18 09:18:58 UTC; 3 days ago Docs: man:systemd-networkd.service(8) Main PID: 843 (systemd-network) Status: "Processing requests..." Tasks: 1 (limit: 1108) CGroup: /system.slice/systemd-networkd.service └─843 /lib/systemd/systemd-networkd.... 如果如法生效，尝试reboot服务器。 参考文献及资料1、Configure static IP address on Ubuntu 16.04 LTS Server，链接：https://michael.mckinnon.id.au/2016/05/05/configuring-ubuntu-16-04-static-ip-address/]]></content>
      <categories>
        <category>Linux IP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-生成式模型和判别式模型]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[看机器学习的论文或材料，经常提到模型属于生成式模型（Generative Modeling）或者判别式模型（Discriminative Modeling）。这次对两个概念进行总结，如有理解偏差，希望大家指正。 方法（算法、学习方式、模型）有很多分类形式。比如常见的分为监督学习、无监督学习、强化学习。这篇博客介绍（学习总结）一种分类形式：生成式模型（Generative Modeling）和判别式模型（Discriminative Modeling）。]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-Autoencoder学习总结]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-09-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Autoencoder%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本篇博客。 AutoEncoder主要用于数据的降维和特征的提取。而现在也被扩展用于生成模型。 与其他神经网路（通常关注于输出层）不同的是：AutoEncoder主要关注于中间的隐藏层（Hidden Layer）。 1、AutoEncoder（自编码器） ​ 原始数据映射到原数据，通过压缩来提取数据的特征。如果将激活函数换成线性函数，这是一个PCA模型。 2、Sparse AutoEncoder（稀疏自编码器） 3、Denoising AutoEncoder（降噪自编码器） 4、Stacked AutoEncoder（堆叠自编码器，SAE） 4、Variational AutoEncoder（VAE）]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SAS 9.4 部署及SID文件调整]]></title>
    <url>%2F2018%2F02%2F06%2F2018-02-09-SAS%209.4%20%E9%83%A8%E7%BD%B2%E5%8F%8ASID%E6%96%87%E4%BB%B6%E8%B0%83%E6%95%B4%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 下载SAS 9.4介质及安装 第二部分 SID文件的导入 第三部分 分享一个坑 第四部分 一点提示 参考文献及资料 背景本篇博客主要介绍个人部署SAS 9.4软件过程及安装过程中调整SID文件技巧。由于工作中经常使用SAS软件连接数据库查数，所以个人电脑准备也装备一下，部署过程分享给大家。 第一部分 下载SAS 9.4介质及安装相信大家的资源检索能力，肯定能找到可用的安装介质。 介质可以参考一个网盘地址：http://pan.baidu.com/s/1qYz7ZNA 密码：ulig 解压缩后windows 直接执行setup.exe文件，linux 执行setup.sh文件执行命令：sh setup.sh。 然后选择安装语言、需要安装的组件、选择（32bit或64bit）等，不再赘述。 第二部分 SID文件的导入 如果你有未过期的SID文件，直接导入后安装即可。下面主要介绍没有时咋办（本文介绍方法主要是个人学习使用哈，商业使用建议购买SID哈，毕竟不差钱） 1、闹心的方法首先找到旧的SID文件（授权时间已经过期），导入SID文件前，将操作系统修改为历史时间，然后顺利安装。但是每次使用SAS都要将系统时间后调，否则没法启动SAS正常使用。确实很闹心，而且修改系统时间后很多软件会提示异常，比如浏览器等。 2、奇技淫巧 其实认真查找SID文件，还是可以找到不过期的。 比如下面的链接：http://downloads.npust.edu.tw/otherFile/20170703022854.txt 过期时间为：30APR2018 [ ] 如果用这个SID提示无效或报错，这时候我们需要调整一下这个SID文件。 - 在安装源文件目录中找到order.xml文件，我的目录为：~\SAS 9.4\order_data\99YYS5\order.xm。在文件中找到两个参数：setnumid=&quot;51200421&quot;、number=&quot;99YYS5&quot; - 将SID文件中下面两个参数调整和order文件中相同： 123#调整后Order=99YYS5Setnumid=51200421 [ ] 最后重新加载SID文件，顺利安装。 &gt; 另外如果还是报错可能是遇到别的坑了。。。。。 第三部分 分享一个坑作者部署平台是win7。所以涉及“C:\ Program Files”（存放64bit软件）和“C:\ Program Files(x86)”（存放32bit软件）。 而下载的SAS 9.4是32bit的，路径选择了“C:\ Program Files”，所以导入SID一直报错。 最后缓过神，调整了安装路径为“C:\ Program Files（x86）”（存放32bit软件的地方），最后顺利安装。 第四部分 一点提示SID文件中会有各个组件的授权，如果使用SAS软件部分组件无效，可能是SID文件中未有该组件的授权信息。下面是截取一个SID的授权组件的信息。例如有基础SAS 、SAS EG、还有SAS和oracle、Teradata连接的组件等。 123456Base SAS 31DEC2017SAS Enterprise Guide 31DEC2017SAS Enterprise Miner Personal Client 31DEC2017SAS/ACCESS Interface to Oracle 31DEC2017SAS/ACCESS Interface to PC Files 31DEC2017SAS/ACCESS Interface to Teradata 31DEC2017 例如：Interface to Oracle组件用来和oracle数据链接。连接后SAS EG客户端可以读取数据库中表，新建映射逻辑库就可以用SAS EG来做表操作。 参考文献及资料1、SAS安装介质网盘，链接：介质可以参考一个网盘地址：http://pan.baidu.com/s/1qYz7ZNA 密码：ulig]]></content>
      <categories>
        <category>SAS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习系列文章-性能评估度量总结]]></title>
    <url>%2F2018%2F01%2F25%2F2018-01-27-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E5%BA%A6%E9%87%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景​ 机器学习方法（监督学习、无监督学习、强化学习）通常认为由模型、策略和算法三个部分构成。例如在监督学习，模型需要从数据中学习条件概率分布（概率模型）或者决策函数（非概率模型）。而模型的假设空间（hypothesis space）就是所有可能的条件概率分布或者决策函数的总体（集合）。 ​ 假设空间通常是庞大的，例如深度神经网络结构能够表示一大类输入层到输出层的函数集合。这样就需要我们设定一个可以量化评测假设空间中函数优劣的度量标准，即选出最优解。 我们从数学角度去理解：这里定义在假设空间上的度量，其实是一个泛函（函数的函数）。 第一部分 几个概念的理解​ 另外我们在优化模型中，经常遇到三个概念：损失函数（loss function）、代价函数（cost function）、目标函数（object function）。 ​ 很多书对三个概念的定义范畴有差异。例如《统计学习方法（李航）》中认为损失函数和代价函数等同，而《深度学习（Yoshua）》书中认为三者等同。个人比较赞同下面的理解： loss function ，为定义在单个样本（点）上的函数。 cost function 为全体样本误差的期望，也就是损失函数（loss function）的平均。 object function ，（目标函数 ）定义为：cost function + 正则化项。 ​ 这样，代价函数和目标函数就可以理解为：定义在假设空间上的性能度量（泛函）。 注意：大家在阅读其他材料时，在概念没有达成一致情况下，需根据前后文，寻找确切的定义，避免混淆。 第二部分 性能度量介绍常用性能度量按照学习类型分类主要如下： 学习类型 性能度量 分类 accuracy、precision、recall、F1 Score、ROC Curve、auc等 回归 MAE、MSE等 2.1 回归问题给定一个训练数据集：$Train={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，这里 $x_i \in R^n$ 和 $y_i \in R$ 分别是输入和输出。学习系统基于训练数据集构建拟合函数 $f st. Y=f(X)$，对新的输入$x_{new}$，函数$f$ 给出预测输出 $y_{new}$。 记测试集：$Test={(x_1,y_1),(x_2,y_2),…,(x_M,y_M)}$ ，记其中观测值：$Y={y_1,y_2,…,y_M}$ ，预测值为:$\hat{Y}={\hat{y_1},\hat{y_2},…,\hat{y_M}}$，则有下面两种泛函： MAE（Mean Absolute Error 平均绝对误差） $$MAE(f)=MAE(Y,\hat{Y})=\dfrac{1}{M}\sum_{i=1}^M{|y_i-\hat{y_i}|}$$ 注：数学形式是向量的$L_1$范数。 MSE（Mean Squared Error 均方误差） $$MSE(f)=MSE(Y,\hat{Y})=\dfrac{1}{M}\sum_{i=1}^M{|y_i-\hat{y_i}|^2}$$ 注：数学形式是向量的$L_2$范数。 在sklearn包中已经封装相关性能函数供调用： 性能容量函数 metrics.explained_variance_score(y_true, y_pred) Explained variance regression score function metrics.mean_absolute_error(y_true, y_pred) Mean absolute error regression loss metrics.mean_squared_error(y_true, y_pred[, …]) Mean squared error regression loss metrics.mean_squared_log_error(y_true, y_pred) Mean squared logarithmic error regression loss metrics.median_absolute_error(y_true, y_pred) Median absolute error regression loss metrics.r2_score(y_true, y_pred[, …]) R^2 (coefficient of determination) regression score function. 123456from sklearn.metrics import mean_absolute_error as maefrom sklearn.metrics import mean_squared_error as msey_true = [3, -0.5, 2, 7]y_pred = [2.5, 0.0, 2, 8]mae(y_true, y_pred)mse(y_true, y_pred) 注：其他性能函数可以查看sklearn的介绍。 2.2 分类问题对于分类问题我们先介绍一个重要概念：混淆矩阵（Confusion matrix）。为了方便讨论，我们以二分类问题为例。 注意：混淆矩阵的定义很多材料也有差异，主要是记号上的区别。这里使用wiki百科的记号标准。 True Positive(TP)：将正类预测为正类数（预测正确） True Negative(TN)：将负类预测为负类数（预测正确） False Positive(FP)：将负类预测为正类数 （预测错误）（Type I error） False Negative(FN)：将正类预测为负类数（预测错误）（Type II error） 预测\真实 Positive Negative True True Positive（TP） False Positive（FP） （Type I error） False False Negative（FN）（Type II error） True negative（TN） ACC（accuracy）准确率 分类正确的样本数占样本总数的比例 。 $$ACC=\dfrac{正确预测的正类+正确预测的反类}{预测样本总量}=\dfrac{TP+TN}{TP+TN+FP+FN}$$ 缺点：在正负类样本分布不均衡的情况下，例如异常检测中如果使用该性能指标，由于样本中异常点（负类）占比很小，这样即使样本全部预测成正类，准确率也有很好的表现。该性能度量无法指导算法寻找最优解。 Precision（精确度） ​ 预测为True中，预测正确的样本占比。$$Precision=\dfrac{正确预测的正类}{预测为正类样本总量}=\dfrac{TP}{TP+FP}$$ 缺点：和ACC相同。 Recall（召回率） ​ 正确预测的正例占实际正例的比例。$$Recall=\dfrac{正确预测的正类}{实际正类样本总量}=\dfrac{TP}{TP+FN}$$ F1（F-score） 精确率和召回率的调和均值。 $$\dfrac{1}{F_1}=\dfrac{1}{Precision}+\dfrac{1}{recall},整理：F_1=\dfrac{2TP}{2TP+FP+FN}$$ 精确率和召回率都高的情况下，F1 Score也会很高 。 ROC（receiver operating characteristic curve）曲线 对于分类问题，模型通常输出的结果为样本点属于类别的概率大小，例如样本属于正类（1类）的概率为0.76，等等。通常我们会设定一个阀值（例如0.5），概率值大于阀值则认为样本属于正类，否则属于负类。 上面的性能指标都是基于通过阀值判断后的结果（0或1）进行度量。 这里引入两个度量：$$TPR=\dfrac{TP}{TP+FN}；FPR=\dfrac{FP}{FP+TN}$$ TPR：实际为正样本中，被正确地判断为正例的比率 。 FPR：实际为负样本中，被错误地判断为正例的比率。 根据不同的概率阀值$threshold \in [0,1] $，计算FPR和TPR的值，得到坐标值$(FPR,TPR)$，即ROC曲线上的一点。采样足够多的坐标点就得到了ROC曲线。 AUC 有了ROC曲线，可以计算曲线下的面积，这就是AUC（Area under the Curve of ROC ）。 AUC值越大的分类器，正确率越高。对于AUC有下面的讨论： AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。下图红线。 0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。 AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。 0&lt;0.5AUC&lt;0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在0&lt;0.5AUC&lt;0.5 的情况。 log_loss（对数损失 ） $$logloss(f)=\dfrac{1}{M}\sum_{i=1}^M \sum_{j=1}^Cy_{i,j}log(P_{i,j}(x_i))$$ ​ 其中$M$为样本数，$C$为类别数，$y_{i,j}$为第$x_i$属于$j$类的符号函数。$p_{i,j}$为$x_i$为$j$类的概率。 这个性能度量其实是在算数据真实分布和预测分布的交叉熵（cross entropy ）。交叉熵越小说明预测分布和真实分布越接近。表明模型具体有较强的泛化能力。 事实上，这里有个小的trick需要说明一下： 信息论中用相对熵用来度量同一个随机变量不同概率分布的“距离”，也称为KL散度。记随机变量$X$有两个分布$p和q$,数学形式为：$$KL(p||q)=\int_Xp(X)log(\dfrac{p(X)}{q(X)})dx=-\int_Xp(X)log(q(X))dx+\int_Xp(X)log(p(X))dx$$右边等式的第一项是交叉熵，第二项是p的信息熵（常量）。所以为了使得KL散度最小化，需要交叉熵越小。即极小化相对熵和交叉熵是一回事（等价）。 同样sklearn包中对上面的性能度量均有封装函数。 第三部分 性能度量使用和总结选择恰当的性能函数对于提升模型的泛化能力是一个重要因素。学习的目的是得到泛化误差小的模型，实际问题中即使得测试集上误差最小。通常会遇到下面两种情况： 当模型在训练集上表现良好、在测试集上误差较大，称为过拟合；而模型在训练集上误差就很大时，称为欠拟合。 欠拟合解决手段有：数据增强、增加训练次数等。 过拟合则有：减少模型参数，减少特征空间，交叉验证，正则化项，Dropout（神经网络中使用）等。 为了防止模型的过拟合通常性能度量会包含正则项。 另外也可以使用多个性能度量对模型进行度量，例如$F_1 score$为精确率和召回率两个性能度量的调和值。 学习的过程转化为最小化性能度量的最优化问题。接着我们得到性能度量函数的极大似然函数。问题转换log极大似然函数最大化问题。这时通常没有解析解（线性回归是有解析解的），只能通过数值方法来寻找最优参数。 这里不再展开说了。 当然对于其他学习任务同样是有性能度量的。比如无监督的聚类问题，K-Means算法中性能度量为：寻找最佳K个质心，使得所有点到所属质心的距离和最小。无监督的降维技术PCA算法的性能度量为：寻找坐标变化，使得新的坐标系下，使得主要坐标轴上的分量的方差最大化。强化学习中Q Learing的性能容量为：对于每个状态（state）选择最佳动作（action）使得奖赏（reword）最大化。 广义上理解，性能度量是假设空间上的泛函，用于量化（度量）假设空间，从而指导算法寻找最优解。 参考文献1、ROC和AUC介绍以及如何计算AUC，链接：http://alexkong.net/2013/06/introduction-to-auc-and-roc/ 2、sklearn，链接：http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics 3、wiki百科，链接：https://zh.wikipedia.org/wiki/ROC%E6%9B%B2%E7%BA%BF 4、《统计学习方法》 5、《深度学习》]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记（数据类型及常用命令）]]></title>
    <url>%2F2018%2F01%2F25%2F2018-01-25-Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%8A%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4)%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 Redis的数据类型 参考文献及资料 背景Redis作为内存数据库，具有高效的读取性能。首次安装完Redis和Python连接redis的包。尝试做一些hello world的练习。 第一部分 Redis的数据类型 Redis是一个键值数据库（Key-Value） 其中Value值支持5中数据类型。分别是string（字符串）、list（列表）、set（集合）、hash（散列）、zset（有序集合） redis-cli控制台的使用及各种数据类型的操作 进入控制台 12&gt;&gt;&gt;redis-cli127.0.0.1:6379&gt; 表示进入控制台，可以和redis进行互动。首先尝试字符串类型的三个命令（set、get、del）。 1、string类型的3个常用命令set命令 12127.0.0.1:6379&gt; set hello worldOK set命令用于给redis中指定键赋值。这里key为hello，而键值value为world。如果key不存在会自动新建该键。 get命令 12127.0.0.1:6379&gt; get hello"world" get命令用于从redis中获取指定键的值。例子获取了hello键的值，返回键值字符串“world”。 del命令 1234127.0.0.1:6379&gt; del hello(integer) 1127.0.0.1:6379&gt; get hello(nil) del命令用于从redis中删除指定键的键值。且我们用get命令检验，确实被删除了。 2、list类型的4个命令rpush、lpush命令 12345678127.0.0.1:6379&gt; rpush list-test item1(integer) 1127.0.0.1:6379&gt; rpush list-test item2(integer) 2127.0.0.1:6379&gt; rpush list-test item1(integer) 3127.0.0.1:6379&gt; lpush list-test item3(integer) 4 上面的命令我们从右边先后推入了item1，item2，item1；然后从左边推入了item3。所以最后的形式应该是[“item3”,”item1”,”item2”,”item1”] lrange命令 12345127.0.0.1:6379&gt; lrange list-test 0 -11) "item3"2) "item1"3) "item2"4) "item1" lrange用于获取list中指定范围的值。这里0是起始索引，-1是最后一个索引（类似python中的list索引）。 lindex 命令 12127.0.0.1:6379&gt; lindex list-test 2"item2" lindex命令从list中获得指定索引位置的值。这里2实际是第三个值，所以返回“item2”。 lpop、rpop命令 123456127.0.0.1:6379&gt; lpop list-test"item3"127.0.0.1:6379&gt; lrange list-test 0 -11) "item1"2) "item2"3) "item1" lpop命令将list-test中最左边的值删除（弹掉），我们用lrange命令查看，确实已经删除。rpop类似使用。 3、set集合类型的四个命令sadd命令 12345678910127.0.0.1:6379&gt; sadd set-test item(integer) 1127.0.0.1:6379&gt; sadd set-test item1(integer) 1127.0.0.1:6379&gt; sadd set-test item2(integer) 1127.0.0.1:6379&gt; sadd set-test item3(integer) 1127.0.0.1:6379&gt; sadd set-test item4(integer) 1 使用sadd命令向集合set-test加入了5个字符串值。注意set类型类似python中的set类型。无序值不重复。 sismember命令 12127.0.0.1:6379&gt; sismember set-test item3(integer) 1 sismember用来查看值是否在集合中。上面检查item3是否在set-test，返回1，表示在集合中。 srem命令 1234127.0.0.1:6379&gt; srem set-test item2(integer) 1127.0.0.1:6379&gt; srem set-test item6(integer) 0 srem命令查看值是否在集合中，如果在返回1且删除该值。否则返回0。 smembers命令 12345127.0.0.1:6379&gt; smembers set-test1) "item"2) "item4"3) "item1"4) "item3" smembers命令查看集合中所有值。上面的结果也验证了srem确实将item2删除了。 set类型还有集合之间的运算（数学），例如sinter、sunion、sdiff分别是集合的交集、并集、差集运算。 4、hash散列类型的命令散列的数据类型是存储多个键值对之间的映射。 hset命令 12345678127.0.0.1:6379&gt; hset hash-test sub-key1 value1(integer) 1127.0.0.1:6379&gt; hset hash-test sub-key2 value2(integer) 1127.0.0.1:6379&gt; hset hash-test sub-key3 value3(integer) 1127.0.0.1:6379&gt; hset hash-test sub-key2 value2(integer) 0 hset 向hash-test中插入键及键值。返回1表示原hash中无该值，0表示重复插入。 hgetall命令 1234567127.0.0.1:6379&gt; hgetall hash-test1) "sub-key1"2) "value1"3) "sub-key2"4) "value2"5) "sub-key3"6) "value3" hgetall命令从hash-test中获取所有的键值。 hget命令 12127.0.0.1:6379&gt; hget hash-test sub-key2"value2" hget命令从hash-test中获得指定键的键值。 hdel命令 1234567127.0.0.1:6379&gt; hdel hash-test sub-key2(integer) 1127.0.0.1:6379&gt; hgetall hash-test1) "sub-key1"2) "value1"3) "sub-key3"4) "value3" hdel命令删除指定键及键值。 5、有序集合的命令zadd命令 123456127.0.0.1:6379&gt; zadd zset-test 123 number1(integer) 1127.0.0.1:6379&gt; zadd zset-test 456 number2(integer) 1127.0.0.1:6379&gt; zadd zset-test 123 number1(integer) 0 zadd命令zset-test中插入分值（score）及成员名（member）。 zrange命令 12345678127.0.0.1:6379&gt; zrange zset-test 0 -1 withscores1) "number1"2) "123"3) "number2"4) "456"127.0.0.1:6379&gt; zrange zset-test 0 -11) "number1"2) "number2" zrange命令获取指定范围的分值和成员名。其中withscores参数用来控制是否同时获得score值。 zrangebyscores命令 123127.0.0.1:6379&gt; zrangebyscore zset-test 0 200 withscores1) "number1"2) "123" zrangebyscores命令获取scores指定范围的分值和成员名。 zrem命令 12345127.0.0.1:6379&gt; zrem zset-test number1(integer) 1127.0.0.1:6379&gt; zrange zset-test 0 -1 withscores1) "number2"2) "456" zrem命令检查zset-test中知否有该分值和成员名。如果有返回1，并且删除。 参考文献及资料1、redisbook，链接：https://github.com/huangz1990/redisbook]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python标准库timeit的使用简介]]></title>
    <url>%2F2018%2F01%2F25%2F2018-01-25-Python%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-Python%E6%A0%87%E5%87%86%E5%BA%93timeit%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[目录 背景 第一部分 模块介绍 参考文献及资料 背景Python调试代码时，经常需要测算一些代码模块或函数的执行效率（即耗时）。常用手段会在代码前后分别用time.time()记下开始和结束的时间，然后相减获得执行耗时。 本篇博客是一篇学习笔记，介绍Python一个内置模块实现代码执行计时。 第一部分 模块介绍 timeit属于Python的标准库。文件路径在~Lib/timeit.py。 timeit同时具有命令行接口和可调用的函数接口。 一、命令行接口1、案例12&gt;&gt;&gt;python -m timeit '"-".join(str(n) for n in range(100))&gt;&gt;&gt;100000 loops, best of 3: 14.1 usec per loop 回显内容：语句&quot;-&quot;.join(str(n) for n in range(100)执行了10w次，平均耗时14.1 usec。 2、接口参数说明 123456789python -m timeit [-n N] [-r N] [-s S] [-t] [-c] [-h] [statement ...]#[-n N] 表示测试语句（statement）执行的次数。如果不指定，会连续执行10，100，1000，...即10的倍数次，直到总时间至少0.2秒，结束。#[-r N] 计数器重复次数。默认是3。返回一个list，记录每次耗时。#[-s S] statement之前前的初始化语句。默认为pass。#[-t] 使用time.time()。#[-c] 使用time.clock()。#[-v] 会输出更多的执行过程信息。10次的耗时，1000次耗时，等等#[-h] 单独使用，反馈接口的使用信息。 注意：statement和[-s S]的参数按照字符串的形式传入。 二、函数接口1、类timeit.Timer 案例 123456789import timeit#定义一个类t=timeit.Timer('char in text',setup='text="sample string";char="g"')#timeit()函数t.timeit()#回显：0.019882534000089436#repeat()函数t.repeat()#回显：[0.01990252700011297, 0.01574616299990339, 0.015739961000008407] 参数说明 123456#1、初始化一个Timer类的参数：timeit.Timer(stmt='pass', setup='pass', timer=&lt;timer function&gt;)#2、timeit(number=1000000)# 默认number执行100w次。#3、repeat(repeat=3，number=1000000)# 默认执行100w次，重复3次（返回list） 2、两个函数 类似Timer的类，timeit也有两个函数。 1234567#1、timeit函数timeit.timeit(stmt="pass", setup="pass", timer=default_timer,number=default_number)#参数说明：stmt即statement，重复执行的语句。setup即执行前的初始化语句（执行一次）。#2、repeat函数timeit.repeat(stmt='pass', setup='pass', timer=&lt;default timer&gt;, repeat=3, number=1000000)#类似Timer类中函数。 案例 1234567import timeitdef test_example(): for i in range(100): "-".join(str(i))if __name__ == '__main__': print(timeit.timeit("test_example()", setup="from __main__ import test_example"))#25.844697911000367 例子中statement是个函数，重复执行前需要在setup中提前import。 参考文献及资料]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[重读本科毕业设计--那年我们一起做毕业论文]]></title>
    <url>%2F2012%2F03%2F31%2F2012-03-31-%E9%87%8D%E8%AF%BB%E6%9C%AC%E7%A7%91%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1--%E9%82%A3%E5%B9%B4%E6%88%91%E4%BB%AC%E4%B8%80%E8%B5%B7%E5%81%9A%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[前言文章从多年前网易博客迁移。 正文再过几个月就要硕士毕业了，现在正在忙于毕业论文的撰写。突然有种冲动，从电脑里翻出了本科的毕业论文，想重温一下当年的成果。打开文件，一看页数是27页，惊讶于当时自己很能写呀，和现在的硕士论文一样长。 记得当时是2008年的年末，院里通知要选毕业论文的题目和指导老师了。当时忙于考研，很晚才看到，已经有不少同学已经选好了。由于临近毕业，大家都很忙，所以记得当时大家的选择标准是：选最简单的题目和最容易通过的导师。等到我选的时候，显然可选性已经很低了。当时曹老师的一个论文题目一下子吸引了我：有限域上的立方幂等阵的个数。当时脑中就闪过了一个想法，这个问题应该用相似标准型来做。于是就选择了曹老师的这个题目。 选题的当天晚上，记得是从汇文楼下自习回来后，就开始试图解决这个问题。在计算了几个实例之后感觉没有这么简单。有个亟须解决的问题就是：有限域上的可逆矩阵的个数如何计算？晚上折腾了很长时间，还是没有突破这个问题。 第二天，准备就去曹老师的办公室去找他，记得当时是和老苗一起去的，但是老师不在，和老苗讨论了这个问题。后来约了一个时间才见到曹老师。这时才从曹老师那知道，华罗庚的典型群这本书上，已经得到有限域上可逆矩阵的计算公式了。于是我这个问题瞬间被秒杀了。同时也感叹一下，大师就是大师呀，我折腾一个晚上都没有解决。晚上回去，就开始演算。因为要利用相似标准型，所以显然要建立有限域上的或者一般域上的相似标准型的理论。很快这个工作就完成了（但是后来答辩时才知道这个工作已经是已知结果了），然后计算幂等阵的相似标准型（这地方要分特征是2和非2两种情况分析）。然后就是简单的组合数学的知识了，但是这地方有一个表示的唯一性问题，想到用等价关系瞬间秒杀。于是论文的初稿在凌晨2点多完成了。后来才发现，问题想简单了，其实这里的相似标准型有很多情况的，这是后话了。 之后，除了开题报告时讲了一下论文思路，就再也没有花时间润色论文了。 到了09年的4月份吧，复试结束后。就开始用TEX开始敲论文了。记得有天晚上回寝室，和阿珂聊起我的论文时，在陈述我的思路时，突然发现了论文中有些相似标准型的情况没有考虑。这还得感谢阿珂，没有他的讨论，估计要等到答辩才能发现这个重大错误，所以说做数学时讨论是很重要的学习方法。 第二天，重新撰写了论文，然后就算定稿了。交给曹老师审稿也通过了。这地方想说一个细节：曹老师的审稿很细致的，在英文摘要里指出了我的一个语法不规范。然后润色了一下文字，就等着答辩了。 答辩之前，学院有个安排是：每个同学可以申请毕业论文优秀组答辩，而且只有这个组的论文可以评优秀本科毕业论文。但是自己比较懒，没有特别在意这件事。但是结果出来时，自己分在了优秀组，有点意外，应该是曹老师的帮我申请的吧。真的很感谢老师的关心。但是后来还是没有评上优毕，估计是答辩时准备的不充分，没有讲得有条理。不过在这过程中还是收获颇多。特别是被曹老师的治学风范和关心学生的精神所感染吧。而且是第一次体会纯数学研究的方法。 现在重读这篇论文，对于有些自己写的定理都已经陌生了，现在如果让我重新证明，也许要花很长时间了…… 不管怎样这篇文章算是成长的经历吧 也许多少年后，我的数学知识估计会退化到连里面的定义都看不懂了……但是那年做论文的事却依然很清晰…… 记得当年那个喜欢数学的纯粹的自己……还有我那亲爱的老师和同学……]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于Whitney嵌入定理]]></title>
    <url>%2F2010%2F06%2F15%2F2010-06-15-%E5%85%B3%E4%BA%8EWhitney%E5%B5%8C%E5%85%A5%E5%AE%9A%E7%90%86%20%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文Whitney嵌入定理是微分拓扑中重要的定理，也许可以认为正是Whitney发现了这个定理，开创了微分拓扑。在这个定理之前，人们对于流形还是把握不定的。但是在这个定理后，由于流形可以嵌入到维数较大的欧氏空间中，所以有了一系列的关于流形的重要结果，形成了微分拓扑这个分支。 Whitney嵌入定理：设M是m维光滑流形，存在M到欧氏空间Rn的光滑嵌入映射f，其中n&gt;=2m+1。并且f的像是Rn中的闭集。 定理的证明要对流形的拓扑性质有好的认识。通常我们定义流形时都是首先假定它是一个Hausdorff和第二可数的拓扑空间，然后是空间中的任意一点都存在一个开领域同胚与欧氏空间中的某个开集。也就是流形局部和欧氏空间是一样的。当然这样一个特殊的拓扑空间一定具有某些良好的性质。首先它是一个局部紧空间，对于空间中的每一点，该点的每个开领域都含有一个闭包是紧的开领域。然后它还是一个仿紧空间。正是这个仿紧性，建立了流形局部性质和整体的关系，通过单位分解的技术，我们可以从局部到整体。 当然在证明中Sard定理起了重要的作用，Sard定理把测度和拓扑性质联系，在微分拓扑中起着基础性作用。 证明方法是纯分析的技术，首先考虑流形到欧氏空间的浸入，再是单浸入，而后引入常态映射，最后证明定理。 张筑生老师的微分拓扑新讲是不错的参考书。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于紧致的Hausdorff空间上的连续函数空间C(X)]]></title>
    <url>%2F2010%2F05%2F04%2F2010-05-04-%E5%85%B3%E4%BA%8E%E7%B4%A7%E8%87%B4%E7%9A%84Hausdorff%E7%A9%BA%E9%97%B4%E4%B8%8A%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%87%BD%E6%95%B0%E7%A9%BA%E9%97%B4C(X)%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文在泛函分析中，定义在紧致的Hausdorff空间（Compact Hausdorff Space）上的复值连续函数（Continuous Complex-valued Functions）全体C(X)是重要的空间。在通常的函数加法，数乘和函数乘法（pointwise multiplication）下是一个交换代数（communicative algebra）。如果赋予最大值范数，还构成交换Banach代数。而且有更深刻的定理表明：任何一个Banach 空间H都等距同构与某个紧致的Hausdorff空间X上的连续空间。某种意义下可以说这个定理对Banach空间给了一个很好的刻画。事实上这个紧致的Hausdorff空间X就是H的对偶空间中的单位球，其拓扑是弱拓扑（由Alaoglu定理容易知道它是弱紧的）。 关于紧致的Hausdorff空间上的连续函数空间有一个有趣的命题： X是紧致的Haosdorff空间，则C(X)是有限维空间当且仅当X是有限的。 充分性：要考虑到当X是有限时，由于X是Hausdorff空间，这时X的拓扑很简单就是离散拓扑，即X的任意子集都是开集。所以X上的任意映射都是连续的，这样不难验证所有的单点集的特征函数就是C(X)的一个基底，推出C(X)是有限维的。 必要性：反证法，如果X非有限，存在可列点集（xi）。注意到点集拓扑中有个结果：紧致的Hausdorff空间是T4空间。当然也是正规空间，而且X中的有限点集是闭集，由Urysohn Lemma知道，有连续函数fj（xi）=0；当i小于等于j时，fj（xj+1）=1。这样至少有可列个线性无关的连续函数fj。这与C(X)有限维矛盾。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于凸集分离定理]]></title>
    <url>%2F2010%2F05%2F03%2F2010-05-03-%E5%85%B3%E4%BA%8E%E5%87%B8%E9%9B%86%E5%88%86%E7%A6%BB%E5%AE%9A%E7%90%86%20%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文泛函分析中凸集分离定理是Hahn-Banach定理的几何形式。描述的是在一定条件下，赋范线性空间中的两个互不相交的凸集被连续线性泛函分离。通常的泛函书上有两种形式：1.赋范线性空间X上的两个互不相交的凸集E和F，其中有一个有内点，这时候存在连续线性泛函分离两凸集。（证明中需要这个内点来定义Minkowski泛函）2.赋范线性空间X上的两个互不相交的凸集E、F，当E和F的距离大于0时，也有连续线性泛函分离两个凸集。（证明中由于两个凸集距离大于零，所以可以常用技巧在其中一个凸集上“镶”一个适当的“开环”仍然是凸集而且有内点，利用情形1即可）。3.赋范线性空间X上的两个互不相交的凸闭集E和凸紧集F，这是同样有连续线性泛函隔离两个集合。（证明是由于紧集和闭集的距离大于0，利用2即可）。 以上是凸集分离定理通常的三种形式，但是对于有限维的赋范线性空间（即欧氏空间），这时对于任意的两个互不相交的凸集都有连续线性泛函隔离之。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数学系的恩师与他们的课]]></title>
    <url>%2F2009%2F08%2F01%2F2009-08-01-%E6%95%B0%E5%AD%A6%E7%B3%BB%E7%9A%84%E6%81%A9%E5%B8%88%E4%B8%8E%E4%BB%96%E4%BB%AC%E7%9A%84%E8%AF%BE%20%2F</url>
    <content type="text"><![CDATA[前言文章从多年前网易博客迁移。 悼念在迁移文章的时候，曹重光老师、吕万金老师已经永远离开了我们。在此深深怀念两位恩师。 正文曹重光老师和Linear Algebra &amp; Advanced Algebra &amp; Modern Algebra 大学里的第一节课在记忆中还是那样清晰。实验楼的一间的大教室，05级的信息与计算科学六十多人几乎坐满了。曹先自我介绍，他手里拿着我们的教材，指着书面上的作者名，说我就叫这名，接着便开始讲课（学数学的很简洁，没太多废话）。 他主讲Linear Algebra。助教是杨魏（曹的研究生），主持习题课和答疑。听说为了做好助教，她把曹老师的书后习题都挨个做完。所以我大一时不会的题问杨魏都会得到解决。 大一的上学期专业课有三门，还有李春明老师的Mathematical Analysis.和吴彦平的Analytical Geometry。这两门课的助教都很无为所以没记住名字。个人感觉三者中我比较喜欢听曹老师的Linear Algebra，在我的印象中他是没有讲课笔记的，上课时也很少翻书（除讲习题时），但是课讲得却很流畅，即使用自己的书，也从不照本宣科。让人感觉他的知识是自然流淌出来。他喜欢用一个数学问题作为引子，然后分析它，创造出新的更有力的数学工具来解决它。经常是根据我们已经了解的数学提出一些有意义的问题，然后接连几节课都是解决这些问题。听他的课能让人体会到数学知识是如何产生的，是一种莫名的享受。四年学习后，重新品味曹的课，深深感觉到一个老师如果讲课没能做到自然流畅，那是因为他对自己所讲的课根本没有很深的体会，甚至自己也是一知半解的。即使有体会也是局部的，没有高度。没有从宏观上，从数学的整体上把握这门课的结构和内涵。 曹喜欢课上提问，讲习题时还喜欢叫人上黑板做题，有时还有随堂考试。所以他的课上必须得认真听。这对于数学系的学生是很好的训练。计算专业的学生都得到良好的代数基础训练。 后来大三上学期我还旁听了他给研究生开的Modern Algebra。教材是刘绍学的An Introduction to Modern Algebra，这本书事实上是刘绍学为本科生写的教材，内容要比张禾瑞的小册子要丰富得多。每周五节课（安排在一个上午），中间只休息半个小时，作业是全部的课后习题。事实上，由于自己大二暑假留校自学了张禾瑞的小册子，所以是带着很多问题听的，受益匪浅呀。 李春明老师 和Mathematical Analysis李的Mathematical Analysis讲得也是很不错的，虽然讲得没有太多深度，也没有联系和自己的体会，但是随着课本讲得却是相当明白。他的课思路清晰，一听就知道认真备课了。听了他两年（四学期）的Mathematical Analysis，感觉李是位负责的老师。课上他喜欢跟我们提起分析中的先哲。印象深刻的是当他讲到Cauchy和 Weierstrass时总是深深的说，这些都是大师，大师了……然后用手推一推鼻梁上的眼镜。 李除了开Mathematical Analysis，也给数学班开实变函数。由于与我的课时也不冲突，差不多全听了。教材是黑大自己编的，与周民强的体系略有差异。比如关于Lebesgue积分概念的引入：强的书利用简单函数积分的引进方式，而黑大这本书采用了一种类似Riemann积分定义的方法。这样对比得听，对于一些概念的理解还是不无好处的。 肖相武老师与高级语言设计（C 语言）大一的下学期开了C语言程序设计，教材是清华大学谭浩强的。记得当时学习计算机的热情十分高涨，每次课都很积极得坐在第一排。老肖（大家喜欢这样称呼他，感觉亲切）的课讲得很自信，课上他喜欢时不时的停下来，然后转身问我们：“是不是这样呀……”，略作停顿后，自己补充道：“恩……是这样的”。 当时每周三节课，课时很紧，但老肖讲得还是蛮细致、蛮清晰的，后来当然没有讲完全书。最后考试形式是开卷，不过好象只能带课本。还记得最后一道题是个大程序题，自己写了很长，但后来发现其中有个错误。不过觉得这道题出的很好的，能检验C语言的功底。 郝翠霞老师和Real Analysis郝是在Australia拿的PhD，她的Real Analysis是双语教学。这可能是大学专业课唯一的一门课。教材是北大周民强的《实变函数》，参考书是Rudin的Real Analysis。每周还有一次习题课，内容是作业本上的习题和郝留的补充题，还是有一定难度的。班上20个人每个人都要上黑板讲几题，形式有点象讨论班，很锻炼人的。把自己的解题方法讲给别人听，使别人了解自己的思想，我个人感觉这是一件很有趣的事情。所以记得当时自己还是很积极的，有时会多讲几道的。 我觉得这门课对我的影响很大。郝老师的方法是板书和讲解是英文的，而后再加一遍中文讲解。开始不太适应，但后来也就慢慢跟上了。但这门课却有个“后裔症”：使我养成了阅读外文书籍的习惯，以后每门专业课我都会找几本外文书作为参考书。大一时是没有去西文图书馆的意识，但以后成了西文的常客。 还有在大二开学后，当时由于考虑到黑大数学系的现实状况和现实因素的影响（毕业就业等），有一种想弃数学学计算机的想法。当时甚至很快弄来一本《计算机组成原理》开始自学。但是开了这门课后，没有那么多的时间学计算机了，后来也就不了了之了。可以说是这门课使我又回到了数学。 任洪善老师 和Ordinary Differential EquationsOrdinary Differential Equations是大二下学期开的课程。由数学系方程方向领袖人物任洪善老师主讲。助教名字我忘了，但是一个很负责的老师。习题课和答疑都有的，任老师有时也会在。任的研究方向是Functional Differential Equations，科研很好的。但讲课我喜欢用Boring来形容，大家喜欢开玩笑的形容为：读报课（大家都在下面看报纸了）。有一次我坐在前面，发现他的讲课笔记可能还是他在电视广播大学时的讲义，纸张都有些发黄了。他的讲课内容完全是讲义上的，甚至是定理标号都不带变的。可能讲授时间太长了，讲义早已烂熟于心，通常在讲某个定理的证明时，“由刚才擦掉的定理3.5.4知定理4.2.3成立”，而对于定理3.5.4是什么内容从不重复。所以如果上课没有笔记，经常跟不上他，导致不知所云。 有时也会和同学们讲讲其他的东西。 （一） 做题与扫雷 印象深刻的是期末考试前期，在宣布考试标准时： 某女问：老师，如果答案写错了，但过程正确怎么给分呀？ 任：嗯……嗯……这……做题……题……就像扫雷一样，你……你一步走错了，那不就全……全错了…… 原来任老师钟情于扫雷游戏。 （二）数学老师VS英语老师 考研时旁听过任老师给研究生开的Stability Theory，课间休息时，和徒弟们聊天：“嗯……嗯……现在老师……也就……也就英语老师挣得多，在外面开个班，教室就能坐满……嗯……你看……看我们……这么大教室……就……就三个人（包括我，他只有两个研究生）……这……这没法比……”。 吕万金老师和Functional Analysis &amp; Methods of Mathematical Physics吕老师讲课很潇洒，他的风格是一切问题课堂上解决,所以讲课时通常挂黑板的。但他却不慌不忙，在黑板上擦擦写写，时不时的还翻翻书，不一会又推出结果了（不知道对不对，反正我是没用明白过）。也有时候，干脆挥一挥手中的粉笔，很轻松的说：“这疙瘩就这样了……你们自己回去看吧”。然后就开始讲下一个内容了。所以对于大三的Functional Analysis 和 Methods of Mathematical Physics是一场“灾难”。 Methods of Mathematical Physics学完后除了会几类特殊方程的求解公式，几乎没有任何物理和几何背景知识，俨然是工科的Methods of Mathematical Physics。Functional Analysis 还算幸运，但一学期讲得内容实在可怜得很。吕老师的课讲得实在不敢恭维，但为人平易近人，对待学生热情，没有老师的架子，和学生打成一片。课前课后，课上课间和学生唠家常，给学生出谋划策。这却是数学系任何一位老师都不能比的。有目共睹呀。 王士模老师和Probability Theory 王士模老师，北大数学系硕士研究生毕业，给数学班讲授Probability Theory。教材是黑大数学系自己编的，但是他却很少用，除了用习题（此书习题全部选自北大教材，其实我个人认为没有必要出这种垃圾书，还不如直接用北大的教材）。我旁听了大半学期，后来快期末考试了，害怕被自己班的老师给挂了，所以后面关于大数定理的内容便没有听了（其实以王士模老师深厚的实变功底，这部分讲得应该相当精彩，无论如何这是一个遗憾）。王老师是江苏扬州人氏，带有浓厚的扬州口音。大部分北方同学都抱怨听不懂，虽然我是安徽的，但安徽和扬州同属楚地方言区，跟上他的课还是没有问题的。王讲课最重要的特点就是充满激情，讲到高兴处通常是“手舞足蹈”，在讲台上180度的旋转，说“好……大家一看就知道”，然后再在黑板上推导，定理证明结束他还有个口头禅“这没什么……”。适当的时候还会讲一些生活中的实例，他喜欢运动，尤其是篮球，所以经常举投篮的例子。印象中王老师为人朴实，平易近人，有时甚至给人“可爱”的感觉；做学问求真、务实、执着；做老师认真负责，有真才实学。有时和大家聊起王老师，总感觉王老师怎么来黑大数学系，太屈才了，于是都喜欢猜测其中的缘由，但后来观点统一了，王老师带有浓重口音的普通话是最大的原因。 唐孝敏老师和Modern Algebra大三下学期开了Modern Algebra，教材用的是张禾瑞的那本黄色的小册子（个人感觉教材太老了，不过听说全国很多数学系还在用的）。唐讲课很有书卷气，娓娓道来，思路清晰。当时自己已经自学了张禾瑞的小册子，也旁听了曹老师的课。所以听起来要轻松得多，但是再听一遍，对于一些地方的理解还是很有收获的。当时正在准备考研，所以把张的小册上的后一章的习题都做了，并且仔细自学了姚慕生老师的Modern Algebra和群、环及域的大部分课后习题。感觉大学中Modern Algebra的确花了不少时间的，这也使得后来做毕业论文时，我的选题也是Modern Algebra 这一块的，有关有限域上的矩阵理论。当然在做论文时，对于一些概念又有了新的理解，特别是有关域论的。 秦家虎老师和Point Set TopologyPoint Set Topology是大三下学期开的课程，用的教材是熊金成的，很经典的一部教材。虽然当时的全部精力都投入在考研复习中，但是对于这门课还是投入一定精力了。作业是秦所选的部分课后习题，并且会定期交作业的，秦亲自批改，并且有时课上会选讲一些题目。虽然当时时间紧张，但每次作业都能交上。临近考试时感觉这门课自己把握不是太好，就用两个星期的时间，把前七章的余下课后习题都做了。但是感觉自己对于一些地方还是理解得不透彻。然而自己对于分析中的问题却有了一个全新的观点。记得当时正在复习分析，在做Euclidean Space的课后习题时，有些命题都是推广到Topological Space，用拓扑的语言给出证明，还是一件很有趣的事情。秦讲课语速甚快，所以写字也快。后来秦还组织同学上去讲一部分内容，个人认为这种方式很锻炼人的，非常好。但是由于当时没有太多时间去准备，没有上去锻炼一下，也是一个遗憾吧。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[离别哈尔滨]]></title>
    <url>%2F2009%2F08%2F01%2F2009-08-01-%E7%A6%BB%E5%88%AB%E5%93%88%E5%B0%94%E6%BB%A8%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文公元2009年7月3日。坐在64路公共汽车，看着夹在学生证中的火车票，我知道：我即将离开美丽的哈尔滨，离开我生活学习四年的黑大，告别我四年的同窗们。望着窗外的城市，忽然间有一种莫名的伤感，而这种感觉却和四年前当我离开安庆时心情是那样的相似……不知道何时能再回到这片留下我太多记忆和挂念的土地。 但依稀记得4年前，那个北上的火车载着我和一个高中生对大学的懵懂憧憬来到了这片黑土，来到了黑大数学系，开始了我大学生活。]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在FD数院面试时的self-introduction]]></title>
    <url>%2F2009%2F08%2F01%2F2009-08-01-%E5%9C%A8FD%E6%95%B0%E9%99%A2%E9%9D%A2%E8%AF%95%E6%97%B6%E7%9A%84self-introduction%20%2F</url>
    <content type="text"><![CDATA[前言文件从多年前网易博客迁移。 正文Good afternoon, my dear teachers. I am very glad to be informed to have this interview. Now I will introduce myself briefly. My name is ???. I was born in September,??th ,??? in ??? province, and I am now a senior undergraduate student from the school of Mathematics, ????? University. And I will graduate in July this year. My hometown is Anqing, a beautiful ancient city near the Changjiang river. It is famous for its Huangmei opera. In Anhui’s history, Anqing played a very important role. So it has a profound culture background. I graduated from Anqing No.1 Middle School in 2005. And then I left Anqing for Haerbin to begin my college life. Although my university is not well known, I still appreciate the education it gave me. Because I’ve acquired basic knowledge of elementary mathematics during the four years. In college, my major was information and computing science when I was a freshman, but I changed it for mathematics and applied mathematics next year. Because I wanted to learn more professional knowledge about pure mathematics. In my junior year, I begin to learn the courses about abstract algebra and general topology. It was an exciting experience. I have a new view of the concepts which I had learned before. I spent almost my spare time learning by myself during the four years. And of all the courses I studied, I like real analysis most. But I think at present, I still have many things to learn. So further study is still urgent for me. And I hope I can lay a solid foundation after two years study. That’s all. Thank you.]]></content>
      <categories>
        <category>math</category>
      </categories>
  </entry>
</search>
